"Conference Proceedings"	""	"2017"	"Can you Poison a Machine Learning Algorithm ?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Can you Poison a Machine Learning Algorithm ?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Glossary"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Glossary"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Privacy-Preserving Mechanisms for SVM Learning"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Privacy-Preserving Mechanisms for SVM Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Appendix A: Background for Learning and Hyper-Geometry"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Appendix A: Background for Learning and Hyper-Geometry"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Appendix B: Full Proofs for Hypersphere Attacks"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Appendix B: Full Proofs for Hypersphere Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Appendix D: Full Proofs for Near-Optimal Evasion"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Appendix D: Full Proofs for Near-Optimal Evasion"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Attacking a Hypersphere Learner"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Attacking a Hypersphere Learner"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"References"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"References"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Integrity Attack Case Study: PCA Detector"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Integrity Attack Case Study: PCA Detector"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Appendix C: Analysis of SpamBayes"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Appendix C: Analysis of SpamBayes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Availability Attack Case Study: SpamBayes"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Availability Attack Case Study: SpamBayes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Index"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Index"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	""	"2019"	"Introduction"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Introduction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	""	"2020"	"UNDERSTANDING ADVERSARIAL ATTACKS ON AU"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"UNDERSTANDING ADVERSARIAL ATTACKS ON AU"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	""	"2020"	"The standard approach to estimating the derivative of the log-likelihood function is using the Markov chain"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"The standard approach to estimating the derivative of the log-likelihood function is using the Markov chain"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	""	"2021"	"A Recipe for Disaster: Neural Architecture Search with Search Space Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Recipe for Disaster: Neural Architecture Search with Search Space Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	""	"2021"	"Membership Inference Attacks on Machine Learning Models: Analysis and Mitigation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Membership Inference Attacks on Machine Learning Models: Analysis and Mitigation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	""	"2021"	"A More experiments and technical details"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A More experiments and technical details"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Abad; S. Paguada; S. Picek; V. i. J. Ram'irez-Dur'an; A. Urbieta"	"2022"	"Client-Wise Targeted Backdoor in Federated Learning"	""	"ArXiv"	""	""	"abs/2203.08689"	""	""	""	""	""	""	""	""	""	""	"Client-Wise Targeted Backdoor in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"G. Abad; S. Picek; V. i. J. Ram'irez-Dur'an; A. Urbieta"	"2021"	"On the Security&Privacy in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"On the Security&Privacy in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Abad; S. Picek; A. Urbieta"	"2021"	"SoK: On the Security & Privacy in Federated Learning"	""	"ArXiv"	""	""	"abs/2112.05423"	""	""	""	""	""	""	""	""	""	""	"SoK: On the Security & Privacy in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Adesina; C.-C. G. Hsieh; Y. E. Sagduyu; L. Qian"	"2020"	"Adversarial Machine Learning in Wireless Communications using RF Data: A Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning in Wireless Communications using RF Data: A Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Afzal-Houshmand; S. Homayoun; T. Giannetsos"	"2021"	"A Perfect Match: Deep Learning Towards Enhanced Data Trustworthiness in Crowd-Sensing Systems"	""	"2021 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)"	""	""	""	""	""	"258-264"	""	""	""	""	""	""	""	"A Perfect Match: Deep Learning Towards Enhanced Data Trustworthiness in Crowd-Sensing Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Agarwal; N. K. Ratha"	"2021"	"Black-Box Adversarial Entry in Finance through Credit Card Fraud Detection"	""	"CIKM Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Black-Box Adversarial Entry in Finance through Credit Card Fraud Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Agrawal; S. Sarkar; O. Aouedi; G. Yenduri; K. Piamrat; S. Bhattacharya; P. K. R. Maddikunta; T. R. Gadekallu"	"2021"	"Federated Learning for Intrusion Detection System: Concepts, Challenges and Future Directions"	""	"ArXiv"	""	""	"abs/2106.09527"	""	""	""	""	""	""	""	""	""	""	"Federated Learning for Intrusion Detection System: Concepts, Challenges and Future Directions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. A. Alatwi; A. Aldweesh"	"2021"	"Adversarial Black-Box Attacks Against Network Intrusion Detection Systems: A Survey"	""	"2021 IEEE World AI IoT Congress (AIIoT)"	""	""	""	""	""	"0034-0040"	""	""	""	""	""	""	""	"Adversarial Black-Box Attacks Against Network Intrusion Detection Systems: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. A. Alatwi; C. Morisset"	"2021"	"Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review"	""	"ArXiv"	""	""	"abs/2112.03315"	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Albaseer; B. S. Ciftler; M. M. Abdallah"	"2020"	"Performance Evaluation of Physical Attacks against E2E Autoencoder over Rayleigh Fading Channel"	""	"2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)"	""	""	""	""	""	"177-182"	""	""	""	""	""	""	""	"Performance Evaluation of Physical Attacks against E2E Autoencoder over Rayleigh Fading Channel"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Alberti; V. Pondenkandath; M. Würsch; M. Bouillon; M. Seuret; R. Ingold; M. Liwicki"	"2018"	"Are You Tampering With My Data?"	""	"ECCV Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Are You Tampering With My Data?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Alfeld; A. Vartanian; L. Newman-Johnson; B. I. P. Rubinstein"	"2019"	"Attacking Data Transforming Learners at Training Time"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Attacking Data Transforming Learners at Training Time"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"I. R. Alkhouri; A. Velasquez; G. K. Atia"	"2021"	"BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples"	""	"ArXiv"	""	""	"abs/2108.02756"	""	""	""	""	""	""	""	""	""	""	"BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Z. Alom; T. M. Taha; C. Yakopcic; S. Westberg; P. Sidike; M. S. Nasrin; B. C. V. Essen; A. A. S. Awwal; V. K. Asari"	"2018"	"The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches"	""	"ArXiv"	""	""	"abs/1803.01164"	""	""	""	""	""	""	""	""	""	""	"The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Aloufi; H. Haddadi; D. Boyle"	"2021"	"A Tandem Framework Balancing Privacy and Security for Voice User Interfaces"	""	"ArXiv"	""	""	"abs/2107.10045"	""	""	""	""	""	""	""	""	""	""	"A Tandem Framework Balancing Privacy and Security for Voice User Interfaces"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Aloufi; A. Nautsch; H. Haddadi; D. Boyle"	"2022"	"Locally Authenticated Privacy-preserving Voice Input"	""	"ArXiv"	""	""	"abs/2205.14026"	""	""	""	""	""	""	""	""	""	""	"Locally Authenticated Privacy-preserving Voice Input"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. Alsuwat"	"2019"	"Challenges in Large-Scale Machine Learning Systems: Security and Correctness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Challenges in Large-Scale Machine Learning Systems: Security and Correctness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. Alsuwat; H. Alsuwat; J. R. Rose; M. Valtorta; C. Farkas"	"2019"	"Detecting Adversarial Attacks in the Context of Bayesian Networks"	""	"DBSec"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Detecting Adversarial Attacks in the Context of Bayesian Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. R. Alvar; L. Wang; J. Pei; Y. Zhang"	"2022"	"Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation"	""	"ArXiv"	""	""	"abs/2203.05212"	""	""	""	""	""	""	""	""	""	""	"Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Amsaleg"	"2014"	"A Database Perspective on Large Scale High-Dimensional Indexing"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Database Perspective on Large Scale High-Dimensional Indexing"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. An; M. J. Lee; J. So"	"2020"	"Improving Robustness Against Adversarial Example Attacks Using Non-Parametric Models on MNIST"	""	"2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)"	""	""	""	""	""	"443-447"	""	""	""	""	""	""	""	"Improving Robustness Against Adversarial Example Attacks Using Non-Parametric Models on MNIST"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. An; G. Tao; Q. Xu; Y. Liu; G. Shen; Y. Yao; J. Xu; X. Zhang"	"2022"	"MIRROR: Model Inversion for Deep Learning Network with High Fidelity"	""	"Proceedings 2022 Network and Distributed System Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"MIRROR: Model Inversion for Deep Learning Network with High Fidelity"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"G. Andresini; A. Appice; D. Malerba"	"2020"	"Dealing with Class Imbalance in Android Malware Detection by Cascading Clustering and Classification"	""	"Complex Pattern Mining"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Dealing with Class Imbalance in Android Malware Detection by Cascading Clustering and Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Anshumaan; S. Balasubramanian; S. Tiwari; N. Natarajan; S. Sellamanickam; V. N. Padmanabhan"	"2022"	"Simulating Network Paths with Recurrent Buffering Units"	""	"ArXiv"	""	""	"abs/2202.13870"	""	""	""	""	""	""	""	""	""	""	"Simulating Network Paths with Recurrent Buffering Units"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Argento; F. Angiulli; A. Furfaro"	"2018"	"Detection, Prevention and Simulation Approaches to Address Anomalies in Cyber Security"	""	"SEBD"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Detection, Prevention and Simulation Approaches to Address Anomalies in Cyber Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Ö. Arik"	"2019"	"Database Prototypes 0 . 6 Classifier “ bird ” 0 . 3 Weights Confidence 0"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Database Prototypes 0 . 6 Classifier “ bird ” 0 . 3 Weights Confidence 0"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Aryal; M. Gupta; M. Abdelsalam"	"2021"	"A Survey on Adversarial Attacks for Malware Analysis"	""	"ArXiv"	""	""	"abs/2111.08223"	""	""	""	""	""	""	""	""	""	""	"A Survey on Adversarial Attacks for Malware Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Ashok; S. Tiwari; N. Natarajan; V. N. Padmanabhan; S. Sellamanickam"	"2022"	"Data-Driven Network Path Simulation with iBox"	""	"Proceedings of the ACM on Measurement and Analysis of Computing Systems"	""	""	"6"	""	""	"1 - 26"	""	""	""	""	""	""	""	"Data-Driven Network Path Simulation with iBox"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Ashraf; N. F. Areed; H. Salem; E. H. Abdelhay; A. Farouk"	"2022"	"FIDChain: Federated Intrusion Detection System for Blockchain-Enabled IoT Healthcare Applications"	""	"Healthcare"	""	""	""	""	""	""	""	""	""	""	""	""	""	"FIDChain: Federated Intrusion Detection System for Blockchain-Enabled IoT Healthcare Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"K. Auernhammer; R. T. Kolagari; M. Zoppelt"	"2019"	"Attacks on Machine Learning: Lurking Danger for Accountability"	""	"SafeAI@AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Attacks on Machine Learning: Lurking Danger for Accountability"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Azadi"	"2021"	"Generative Models as a Robust Alternative for Image Classification: Progress and Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generative Models as a Robust Alternative for Image Classification: Progress and Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. M. Azari; S. Solanki; S. Chatzinotas; O. Kodheli; H. Sallouha; A. Colpaert; J. F. M. Montoya; S. Pollin; A. R. Haqiqatnejad; A. Mostaani; E. Lagunas; B. E. Ottersten"	"2021"	"Evolution of Non-Terrestrial Networks From 5G to 6G: A Survey"	""	"ArXiv"	""	""	"abs/2107.06881"	""	""	""	""	""	""	""	""	""	""	"Evolution of Non-Terrestrial Networks From 5G to 6G: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Baek; H. Shim"	"2022"	"Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data"	""	"ArXiv"	""	""	"abs/2204.04950"	""	""	""	""	""	""	""	""	""	""	"Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Bai; J. Zhao; J. Zhu; S. Han; J. Chen; B. Li"	"2021"	"AI-GAN: Attack-Inspired Generation of Adversarial Examples"	""	"2021 IEEE International Conference on Image Processing (ICIP)"	""	""	""	""	""	"2543-2547"	""	""	""	""	""	""	""	"AI-GAN: Attack-Inspired Generation of Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. M. Bakker; M. Lew"	"2021"	"Adversarial Detection and Defense in Deep learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Detection and Defense in Deep learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. R. Balda; A. Behboodi; R. Mathar"	"2018"	"Perturbation Analysis of Learning Algorithms: A Unifying Perspective on Generation of Adversarial Examples"	""	"ArXiv"	""	""	"abs/1812.07385"	""	""	""	""	""	""	""	""	""	""	"Perturbation Analysis of Learning Algorithms: A Unifying Perspective on Generation of Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. R. Balda; A. Behboodi; R. Mathar"	"2019"	"Adversarial Examples in Deep Neural Networks: An Overview"	""	"Deep Learning: Algorithms and Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples in Deep Neural Networks: An Overview"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Baluja; I. S. Fischer"	"2017"	"Adversarial Transformation Networks: Learning to Generate Adversarial Examples"	""	"ArXiv"	""	""	"abs/1703.09387"	""	""	""	""	""	""	""	""	""	""	"Adversarial Transformation Networks: Learning to Generate Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Banihashem; A. K. Singla; G. Radanovic"	"2021"	"Defense Against Reward Poisoning Attacks in Reinforcement Learning"	""	"ArXiv"	""	""	"abs/2102.05776"	""	""	""	""	""	""	""	""	""	""	"Defense Against Reward Poisoning Attacks in Reinforcement Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Barrett; A. Camuto; M. Willetts; T. Rainforth"	"2022"	"Certifiably Robust Variational Autoencoders"	""	"AISTATS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Certifiably Robust Variational Autoencoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Basu"	"2021"	"Privacy-preserving recommendation system using federated learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Privacy-preserving recommendation system using federated learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. A. Bauer; V. Bindschaedler"	"2021"	"Generative Models for Security: Attacks, Defenses, and Opportunities"	""	"ArXiv"	""	""	"abs/2107.10139"	""	""	""	""	""	""	""	""	""	""	"Generative Models for Security: Attacks, Defenses, and Opportunities"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. V. Bergen; J.-F. Rajotte; F. Yousefirizi; A. Rahmim; R. T. Ng"	"2022"	"Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Bergsma; T. J. Zeyl; A. Senderovich; J. C. Beck"	"2021"	"Generating Complex, Realistic Cloud Workloads using Recurrent Neural Networks"	""	"Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generating Complex, Realistic Cloud Workloads using Recurrent Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Bernau; J. Robl; F. Kerschbaum"	"2022"	"Assessing Differentially Private Variational Autoencoders under Membership Inference"	""	"ArXiv"	""	""	"abs/2204.07877"	""	""	""	""	""	""	""	""	""	""	"Assessing Differentially Private Variational Autoencoders under Membership Inference"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. N. Bhagoji; S. Chakraborty; P. Mittal; S. B. Calo"	"2019"	"Analyzing Federated Learning through an Adversarial Lens"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Analyzing Federated Learning through an Adversarial Lens"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. N. Bhagoji; D. Cullina; P. Mittal"	"2019"	"Lower Bounds on Adversarial Robustness from Optimal Transport"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Lower Bounds on Adversarial Robustness from Optimal Transport"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Bhargabjyoti; M. Sudipta"	"2021"	"Analysis of performance vulnerability of MAC scheduling algorithms due to SYN flood attack in 5G NR mmWave"	""	"International Journal of Advanced Technology and Engineering Exploration"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Analysis of performance vulnerability of MAC scheduling algorithms due to SYN flood attack in 5G NR mmWave"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Bhargava"	"2018"	"Anomaly Detection under Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Anomaly Detection under Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Bhattacharjee; K. Chaudhuri"	"2020"	"When are Non-Parametric Methods Robust?"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"When are Non-Parametric Methods Robust?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Bhattacharjee; K. Chaudhuri"	"2021"	"Consistent Non-Parametric Methods for Adaptive Robustness"	""	"ArXiv"	""	""	"abs/2102.09086"	""	""	""	""	""	""	""	""	""	""	"Consistent Non-Parametric Methods for Adaptive Robustness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Bhattacharjee; K. Chaudhuri"	"2021"	"Consistent Non-Parametric Methods for Maximizing Robustness"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Consistent Non-Parametric Methods for Maximizing Robustness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Biehler; Z. Zhong; J. Shi"	"2021"	"SAGE: Stealthy Attack GEneration for Cyber-Physical Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"SAGE: Stealthy Attack GEneration for Cyber-Physical Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Biggio"	"2014"	"On learning and recognition of secure patterns"	""	"AISec '14"	""	""	""	""	""	""	""	""	""	""	""	""	""	"On learning and recognition of secure patterns"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Biggio"	"2016"	"Machine Learning under Attack: Vulnerability Exploitation and Security Measures"	""	"Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Machine Learning under Attack: Vulnerability Exploitation and Security Measures"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Biggio; S. R. Bulò; I. Pillai; M. Mura; E. Z. Mequanint; M. Pelillo; F. Roli"	"2014"	"Poisoning Complete-Linkage Hierarchical Clustering"	""	"S+SSPR"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning Complete-Linkage Hierarchical Clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Biggio; G. Fumera; P. Russu; L. Didaci; F. Roli"	"2015"	"Adversarial Biometric Recognition [A review on biometric system security from the adversarial machine-learning perspective]"	""	"Ieee Signal Processing Magazine"	""	""	"32"	""	"5"	"31-41"	""	""	""	""	"Sep"	""	""	"Adversarial Biometric Recognition [A review on biometric system security from the adversarial machine-learning perspective]"	"Ieee Signal Proc Mag"	"1053-5888"	"10.1109/Msp.2015.2426728"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000359585200008"	""	""	""	""	"Cp0sh
Times Cited:46
Cited References Count:29"	""	"<Go to ISI>://WOS:000359585200008"	""	"Univ Cagliari, Dept Elect & Elect Engn, I-09124 Cagliari, Italy
Univ Cagliari, I-09124 Cagliari, Italy
Univ Cagliari, Dept Elect & Elect Engn, Comp Engn, I-09124 Cagliari, Italy
Univ Trento, Trento, Italy"	""	""	""	""	""	""	""	"English"
"Conference Proceedings"	"B. Biggio; K. Rieck; D. Ariu; C. Wressnegger; I. Corona; G. Giacinto; F. Roli"	"2014"	"Poisoning behavioral malware clustering"	""	"AISec '14"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning behavioral malware clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Biggio; F. Roli"	"2018"	"Wild patterns: Ten years after the rise of adversarial machine learning"	""	"Pattern Recognit."	""	""	"84"	""	""	"317-331"	""	""	""	""	""	""	""	"Wild patterns: Ten years after the rise of adversarial machine learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Bitton; N. Maman; I. Singh; S. Momiyama; Y. Elovici; A. Shabtai"	"2021"	"A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems"	""	"ArXiv"	""	""	"abs/2107.01806"	""	""	""	""	""	""	""	""	""	""	"A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Borgnia; J. Geiping; V. Cherepanova; L. Fowl; A. Gupta; A. Ghiasi; F. Huang; M. Goldblum; T. Goldstein"	"2021"	"DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations"	""	"ArXiv"	""	""	"abs/2103.02079"	""	""	""	""	""	""	""	""	""	""	"DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. S. Borkar; F. Heide; L. Karam"	"2019"	"Defending against Adversarial Attacks through Resilient Feature Regeneration"	""	"ArXiv"	""	""	"abs/1906.03444"	""	""	""	""	""	""	""	""	""	""	"Defending against Adversarial Attacks through Resilient Feature Regeneration"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Breier; X. Hou; M. Ochoa; J. Solano"	"2022"	"FooBaR: Fault Fooling Backdoor Attack on Neural Network Training"	""	"ArXiv"	""	""	"abs/2109.11249"	""	""	""	""	""	""	""	""	""	""	"FooBaR: Fault Fooling Backdoor Attack on Neural Network Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Bu; Y. Duan; F. Song; Z. Zhao"	"2021"	"Taking Care of The Discretization Problem: A Comprehensive Study of the Discretization Problem and A Black-Box Adversarial Attack in Discrete Integer Domain"	""	"IEEE Transactions on Dependable and Secure Computing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Taking Care of The Discretization Problem: A Comprehensive Study of the Discretization Problem and A Black-Box Adversarial Attack in Discrete Integer Domain"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Burkard; B. Lagesse"	"2017"	"Analysis of Causative Attacks against SVMs Learning from Data Streams"	""	"Proceedings of the 3rd ACM on International Workshop on Security And Privacy Analytics"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Analysis of Causative Attacks against SVMs Learning from Data Streams"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Cai; I. Z. Li; Y. Wu"	"2019"	"Graph the Invisible : Reasoning for Adversarial Attacks with GNNs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Graph the Invisible : Reasoning for Adversarial Attacks with GNNs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Cai; Z. Xiong; H. Xu; P. Wang; W. Li; Y.-L. Pan"	"2021"	"Generative Adversarial Networks: A Survey Towards Private and Secure Applications"	""	"ArXiv"	""	""	"abs/2106.03785"	""	""	""	""	""	""	""	""	""	""	"Generative Adversarial Networks: A Survey Towards Private and Secure Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Calivá; K. Cheng; R. Shah; V. Pedoia"	"2020"	"Adversarial Robust Training of Deep Learning MRI Reconstruction Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Robust Training of Deep Learning MRI Reconstruction Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Camuto; M. Willetts; S. J. Roberts; C. C. Holmes; T. Rainforth"	"2021"	"Towards a Theoretical Understanding of the Robustness of Variational Autoencoders"	""	"AISTATS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Towards a Theoretical Understanding of the Robustness of Variational Autoencoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. R. B. Cañizares"	"2019"	"Robustness analysis of deep neural networks in the presence of adversarial perturbations and noisy labels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robustness analysis of deep neural networks in the presence of adversarial perturbations and noisy labels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Cao; L. Zhang; B. Cao"	"2021"	"Towards On-Device Federated Learning: A Direct Acyclic Graph-based Blockchain Approach"	""	"IEEE transactions on neural networks and learning systems"	""	""	"PP"	""	""	""	""	""	""	""	""	""	""	"Towards On-Device Federated Learning: A Direct Acyclic Graph-based Blockchain Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Cao; J. Jia; N. Z. Gong"	"2021"	"Data Poisoning Attacks to Local Differential Privacy Protocols"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks to Local Differential Privacy Protocols"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Carlini"	"2018"	"Evaluation and Design of Robust Neural Network Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evaluation and Design of Robust Neural Network Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Carlini; A. Terzis"	"2021"	"Poisoning and Backdooring Contrastive Learning"	""	"ArXiv"	""	""	"abs/2106.09667"	""	""	""	""	""	""	""	""	""	""	"Poisoning and Backdooring Contrastive Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Carnerero-Cano; L. Muñoz-González; P. Spencer; E. C. Lupu"	"2020"	"Regularisation Can Mitigate Poisoning Attacks: A Novel Analysis Based on Multiobjective Bilevel Optimisation"	""	"ArXiv"	""	""	"abs/2003.00040"	""	""	""	""	""	""	""	""	""	""	"Regularisation Can Mitigate Poisoning Attacks: A Novel Analysis Based on Multiobjective Bilevel Optimisation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Carnerero-Cano; L. Muñoz-González; P. Spencer; E. C. Lupu"	"2021"	"Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters"	""	"ArXiv"	""	""	"abs/2105.10948"	""	""	""	""	""	""	""	""	""	""	"Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Carrara; G. Amato; F. Falchi"	"2020"	"etection of Face Recognition Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"etection of Face Recognition Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. T. Cemgil"	"2019"	"Learning Robust Representations with Smooth Encoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Learning Robust Representations with Smooth Encoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. T. Cemgil; S. Ghaisas; K. Dvijotham; P. Kohli"	"2020"	"Adversarially Robust Representations with Smooth Encoders"	""	"ICLR"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarially Robust Representations with Smooth Encoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Chan; L. Ma; F. Juefei-Xu; X. Xie; Y. Liu; Y. S. Ong"	"2018"	"Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer"	""	"ArXiv"	""	""	"abs/1809.02444"	""	""	""	""	""	""	""	""	""	""	"Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. P. K. Chan; Z.-M. He; X. Hu; E. C. C. Tsang; D. S. Yeung; W. W. Y. Ng"	"2021"	"Causative label flip attack detection with data complexity measures"	""	"International Journal of Machine Learning and Cybernetics"	""	""	"12"	""	""	"103-116"	""	""	""	""	""	""	""	"Causative label flip attack detection with data complexity measures"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"V. Chandrasekaran; D. Edge; S. Jha; A. Sharma; C. Zhang; S. Tople"	"2021"	"Causally Constrained Data Synthesis for Private Data Release"	""	"ArXiv"	""	""	"abs/2105.13144"	""	""	""	""	""	""	""	""	""	""	"Causally Constrained Data Synthesis for Private Data Release"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"V. Chandrasekaran; H. Jia; A. Thudi; A. Travers; M. Yaghini; N. Papernot"	"2021"	"SoK: Machine Learning Governance"	""	"ArXiv"	""	""	"abs/2109.10870"	""	""	""	""	""	""	""	""	""	""	"SoK: Machine Learning Governance"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Chang; S. Laridi; Z. Ren; G. Palmer; B. W. Schuller; M. Fisichella"	"2022"	"Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition"	""	"ArXiv"	""	""	"abs/2203.04696"	""	""	""	""	""	""	""	""	""	""	"Robust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Chen; M. A. Babar"	"2022"	"Security for Machine Learning-based Software Systems: a survey of threats, practices and challenges"	""	"ArXiv"	""	""	"abs/2201.04736"	""	""	""	""	""	""	""	""	""	""	"Security for Machine Learning-based Software Systems: a survey of threats, practices and challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Chen; H. Zhang; D. S. Boning; C.-J. Hsieh"	"2019"	"Robust Decision Trees Against Adversarial Examples"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Decision Trees Against Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. S. Chen; A. S. Coyner; R. V. P. Chan; M. E. Hartnett; D. M. Moshfeghi; L. A. Owen; J. Kalpathy-Cramer; M. F. Chiang; J. Campbell"	"2021"	"Deepfakes in Ophthalmology: Applications and Realism of Synthetic Retinal Images from Generative Adversarial Networks"	""	"Ophthalmology Science"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Deepfakes in Ophthalmology: Applications and Realism of Synthetic Retinal Images from Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Chen; Y. Min; M. Zhang; A. Karbasi"	"2020"	"More Data Can Expand the Generalization Gap Between Adversarially Robust and Standard Models"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"More Data Can Expand the Generalization Gap Between Adversarially Robust and Standard Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Chen; K. Zheng; Y. Li; X. Yang; H. Zhang; Y. Liang; J. Huang; Y. Zhang; P. Xie; Y. Zhao"	"2021"	"Data Augmentation Algorithm Based on Generative Antagonism Networks (GAN) Model for Optical Transmission Networks (OTN)"	""	"Proceedings of CECNet 2021"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Augmentation Algorithm Based on Generative Antagonism Networks (GAN) Model for Optical Transmission Networks (OTN)"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Chen; J. Li; C. Wu; B. Sheng; P. Li"	"2020"	"A Framework of Randomized Selection Based Certified Defenses Against Data Poisoning Attacks"	""	"ArXiv"	""	""	"abs/2009.08739"	""	""	""	""	""	""	""	""	""	""	"A Framework of Randomized Selection Based Certified Defenses Against Data Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Chen; Z. Li; J. Li; C. Wu; J. Yan"	"2022"	"On Collective Robustness of Bagging Against Data Poisoning"	""	"ArXiv"	""	""	"abs/2205.13176"	""	""	""	""	""	""	""	""	""	""	"On Collective Robustness of Bagging Against Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Chen; Z. Ma"	"2021"	"Towards Robust Neural Image Compression: Adversarial Attack and Model Finetuning"	""	"ArXiv"	""	""	"abs/2112.08691"	""	""	""	""	""	""	""	""	""	""	"Towards Robust Neural Image Compression: Adversarial Attack and Model Finetuning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Chen; C.-J. Hsieh"	"2020"	"Stabilizing Differentiable Architecture Search via Perturbation-based Regularization"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Stabilizing Differentiable Architecture Search via Perturbation-based Regularization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Chen; X. Huang; J. Zhang"	"2019"	"Machine Learning for Cyber Security: Second International Conference, ML4CS 2019, Xi’an, China, September 19-21, 2019, Proceedings"	""	"Machine Learning for Cyber Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Machine Learning for Cyber Security: Second International Conference, ML4CS 2019, Xi’an, China, September 19-21, 2019, Proceedings"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Chen; C. Liu; B. Li; K. Lu; D. X. Song"	"2017"	"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"	""	"ArXiv"	""	""	"abs/1712.05526"	""	""	""	""	""	""	""	""	""	""	"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Chen; C. Shen; C. Wang; Y. Zhang"	"2021"	"Teacher Model Fingerprinting Attacks Against Transfer Learning"	""	"ArXiv"	""	""	"abs/2106.12478"	""	""	""	""	""	""	""	""	""	""	"Teacher Model Fingerprinting Attacks Against Transfer Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Chen; P. Tian; W. Liao; W. Yu"	"2021"	"Towards multi-party targeted model poisoning attacks against federated learning systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Towards multi-party targeted model poisoning attacks against federated learning systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Cheng; K. Xu; S. Liu; P.-Y. Chen; P. Zhao; X. Lin"	"2020"	"Defending against Backdoor Attack on Deep Neural Networks"	""	"ArXiv"	""	""	"abs/2002.12162"	""	""	""	""	""	""	""	""	""	""	"Defending against Backdoor Attack on Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"K. Cheng; F. Calivá; R. Shah; M. Han; S. Majumdar; V. Pedoia"	"2020"	"Addressing The False Negative Problem of Deep Learning MRI Reconstruction Models by Adversarial Attacks and Robust Training"	""	"MIDL"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Addressing The False Negative Problem of Deep Learning MRI Reconstruction Models by Adversarial Attacks and Robust Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Cheng; C. Healey; T. Wu"	"2021"	"Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones"	""	"ArXiv"	""	""	"abs/2108.00335"	""	""	""	""	""	""	""	""	""	""	"Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Cheng; P.-Y. Chen; S. Liu; S. Chang; C.-J. Hsieh; P. Das"	"2021"	"Self-Progressing Robust Training"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Self-Progressing Robust Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Cheng; W. Wei; C.-J. Hsieh"	"2019"	"Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent"	""	"NAACL"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evaluating and Enhancing the Robustness of Dialogue Systems: A Case Study on a Negotiation Agent"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Cheng; H. Zhang; Z. Li"	"2021"	"Label Noise Detection System Against Label Flipping Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Label Noise Detection System Against Label Flipping Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Q. Cheng; S. Zhou; Y. Shen; D. Kong; C. Wu"	"2021"	"Packet-Level Adversarial Network Traffic Crafting using Sequence Generative Adversarial Networks"	""	"ArXiv"	""	""	"abs/2103.04794"	""	""	""	""	""	""	""	""	""	""	"Packet-Level Adversarial Network Traffic Crafting using Sequence Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Cheng; C.-K. Chu; H.-Y. Lin; M. Lombard-Platet; D. Naccache"	"2019"	"Keyed Non-parametric Hypothesis Tests"	""	"NSS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Keyed Non-parametric Hypothesis Tests"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Cheng; X. Zhu; Q. Zhang; L. Gao; J. Song"	"2021"	"Fast Gradient Non-sign Methods"	""	"ArXiv"	""	""	"abs/2110.12734"	""	""	""	""	""	""	""	""	""	""	"Fast Gradient Non-sign Methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Chernikova; A. Oprea"	"2019"	"FENCE: Feasible Evasion Attacks on Neural Networks in Constrained Environments"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"FENCE: Feasible Evasion Attacks on Neural Networks in Constrained Environments"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Chernikova; A. Oprea"	"2019"	"Adversarial Examples for Deep Learning Cyber Security Analytics"	""	"ArXiv"	""	""	"abs/1909.10480"	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples for Deep Learning Cyber Security Analytics"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Chhabra; A. Roy; P. Mohapatra"	"2019"	"Strong Black-box Adversarial Attacks on Unsupervised Machine Learning Models"	""	"ArXiv"	""	""	"abs/1901.09493"	""	""	""	""	""	""	""	""	""	""	"Strong Black-box Adversarial Attacks on Unsupervised Machine Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Chhabra; A. Roy; P. Mohapatra"	"2020"	"Suspicion-Free Adversarial Attacks on Clustering Algorithms"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Suspicion-Free Adversarial Attacks on Clustering Algorithms"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Chhabra; A. K. Singla; P. Mohapatra"	"2021"	"Fairness Degrading Adversarial Attacks Against Clustering Algorithms"	""	"ArXiv"	""	""	"abs/2110.12020"	""	""	""	""	""	""	""	""	""	""	"Fairness Degrading Adversarial Attacks Against Clustering Algorithms"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H.-J. Choi; E. Jang"	"2018"	"Generative Ensembles for Robust Anomaly Detection"	""	"ArXiv"	""	""	"abs/1810.01392"	""	""	""	""	""	""	""	""	""	""	"Generative Ensembles for Robust Anomaly Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H.-J. Choi; E. Jang; A. A. Alemi"	"2018"	"WAIC, but Why? Generative Ensembles for Robust Anomaly Detection"	""	"arXiv: Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"WAIC, but Why? Generative Ensembles for Robust Anomaly Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J.-H. Choi; H. Zhang; J.-H. Kim; C.-J. Hsieh; J.-S. Lee"	"2020"	"Adversarially Robust Deep Image Super-Resolution Using Entropy Regularization"	""	"ACCV"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarially Robust Deep Image Super-Resolution Using Entropy Regularization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Choraś; M. Pawlicki; D. Puchalski; R. Kozik"	"2020"	"Machine Learning – The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?"	""	"Computational Science – ICCS 2020"	""	""	"12140"	""	""	"615 - 628"	""	""	""	""	""	""	""	"Machine Learning – The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Chourasia; B. Enkhtaivan; K. Ito; J. Mori; I. Teranishi; H. Tsuchida"	"2022"	"Knowledge Cross-Distillation for Membership Privacy"	""	"Proceedings on Privacy Enhancing Technologies"	""	""	"2022"	""	""	"362 - 377"	""	""	""	""	""	""	""	"Knowledge Cross-Distillation for Membership Privacy"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y.-W. Chow; W. Susilo; J. Wang; R. Buckland; J. Baek; J. Kim; N. Li"	"2019"	"Protecting the Visual Fidelity of Machine Learning Datasets Using QR Codes"	""	"ML4CS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Protecting the Visual Fidelity of Machine Learning Datasets Using QR Codes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Chu; A. Zhmoginov; M. Sandler"	"2017"	"CycleGAN, a Master of Steganography"	""	"ArXiv"	""	""	"abs/1712.02950"	""	""	""	""	""	""	""	""	""	""	"CycleGAN, a Master of Steganography"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. P. Chukhnov; Y. S. Ivanov"	"2021"	"Algorithms for Detecting and Preventing Attacks on Machine Learning Models in Cyber-Security Problems"	""	"Journal of Physics: Conference Series"	""	""	"2096"	""	""	""	""	""	""	""	""	""	""	"Algorithms for Detecting and Preventing Attacks on Machine Learning Models in Cyber-Security Problems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. E. Cina; K. Grosse; A. Demontis; S. Vascon; W. Zellinger; B. A. Moser; A. Oprea; B. Biggio; M. Pelillo; F. Roli"	"2022"	"Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning"	""	"ArXiv"	""	""	"abs/2205.01992"	""	""	""	""	""	""	""	""	""	""	"Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. E. Cina; A. Torcinovich; M. Pelillo"	"2022"	"A Black-box Adversarial Attack for Poisoning Clustering"	""	"Pattern Recognit."	""	""	"122"	""	""	"108306"	""	""	""	""	""	""	""	"A Black-box Adversarial Attack for Poisoning Clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. E. Cina; S. Vascon; A. Demontis; B. Biggio; F. Roli; M. Pelillo"	"2021"	"The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers?"	""	"2021 International Joint Conference on Neural Networks (IJCNN)"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Clements; Y. Yang; A. A. Sharma; H. Hu; Y. Lao"	"2021"	"Rallying Adversarial Techniques against Deep Learning for Network Security"	""	"2021 IEEE Symposium Series on Computational Intelligence (SSCI)"	""	""	""	""	""	"01-08"	""	""	""	""	""	""	""	"Rallying Adversarial Techniques against Deep Learning for Network Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. N. Cohen; D. Snow; L. Szpruch"	"2021"	"Black-Box Model Risk in Finance"	""	"Risk Management eJournal"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Black-Box Model Risk in Finance"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. Condessa; J. Z. Kolter"	"2020"	"Provably robust deep generative models"	""	"ArXiv"	""	""	"abs/2004.10608"	""	""	""	""	""	""	""	""	""	""	"Provably robust deep generative models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Consortium"	"2020"	"D 5 . 3 Final Report on Privacy Metrics , Risks , and Utility"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"D 5 . 3 Final Report on Privacy Metrics , Risks , and Utility"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Costa; F. Pinelli; S. Soderi; G. Tolomei"	"2021"	"Covert Channel Attack to Federated Learning Systems"	""	"ArXiv"	""	""	"abs/2104.10561"	""	""	""	""	""	""	""	""	""	""	"Covert Channel Attack to Federated Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Creswell; A. A. Bharath; B. Sengupta"	"2017"	"LatentPoison - Adversarial Attacks On The Latent Space"	""	"ArXiv"	""	""	"abs/1711.02879"	""	""	""	""	""	""	""	""	""	""	"LatentPoison - Adversarial Attacks On The Latent Space"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Crussell; W. P. Kegelmeyer"	"2015"	"Attacking DBSCAN for Fun and Profit"	""	"SDM"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Attacking DBSCAN for Fun and Profit"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. K. Dang; P. T. T. Truong; P. Tran"	"2020"	"Data Poisoning Attack on Deep Neural Network and Some Defense Methods"	""	"2020 International Conference on Advanced Computing and Applications (ACOMP)"	""	""	""	""	""	"15-22"	""	""	""	""	""	""	""	"Data Poisoning Attack on Deep Neural Network and Some Defense Methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Dasgupta; A. Piplai; P. Ranade; A. Joshi"	"2021"	"Cybersecurity Knowledge Graph Improvement with Graph Neural Networks"	""	"2021 IEEE International Conference on Big Data (Big Data)"	""	""	""	""	""	"3290-3297"	""	""	""	""	""	""	""	"Cybersecurity Knowledge Graph Improvement with Graph Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. De Gaspari; D. Hitaj; G. Pagnotta; L. De Carli; L. V. Mancini"	"2022"	"Evading behavioral classifiers: a comprehensive analysis on evading ransomware detection techniques"	""	"Neural Computing and Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evading behavioral classifiers: a comprehensive analysis on evading ransomware detection techniques"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Deb; X. Liu; A. K. Jain"	"2020"	"FaceGuard: A Self-Supervised Defense Against Adversarial Face Images"	""	"ArXiv"	""	""	"abs/2011.14218"	""	""	""	""	""	""	""	""	""	""	"FaceGuard: A Self-Supervised Defense Against Adversarial Face Images"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Deb; X. Liu; A. K. Jain"	"2021"	"Unified Detection of Digital and Physical Face Attacks"	""	"ArXiv"	""	""	"abs/2104.02156"	""	""	""	""	""	""	""	""	""	""	"Unified Detection of Digital and Physical Face Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Debnath; N. Gupta; G. V. Waghmare; H. Wadhwa; S. Asthana; A. Arora"	"2021"	"Adversarial Generation of Temporal Data: A Critique on Fidelity of Synthetic Data"	""	"PKDD/ECML Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Generation of Temporal Data: A Critique on Fidelity of Synthetic Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Debnath; G. V. Waghmare; H. Wadhwa; S. Asthana; A. Arora"	""	"Exploring Generative Data Augmentation in Multivariate Time Series Forecasting : Opportunities and Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Exploring Generative Data Augmentation in Multivariate Time Series Forecasting : Opportunities and Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Dekang; D. P. Guralnik; W. Xuezhi; L. Xiang; B. Moran"	"2015"	"Statistical estimation for Single Linkage Hierarchical Clustering"	""	"2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)"	""	""	""	""	""	"745-750"	""	""	""	""	""	""	""	"Statistical estimation for Single Linkage Hierarchical Clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Deldjoo; T. D. Noia; F. A. Merra"	"2020"	"A survey on Adversarial Recommender Systems: from Attack/Defense strategies to Generative Adversarial Networks"	""	"arXiv: Information Retrieval"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A survey on Adversarial Recommender Systems: from Attack/Defense strategies to Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Demeter"	"2019"	"Background and Notation"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Background and Notation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Demontis; M. Melis; M. Pintor; M. Jagielski; B. Biggio; A. Oprea; C. Nita-Rotaru; F. Roli"	"2018"	"On the Intriguing Connections of Regularization, Input Gradients and Transferability of Evasion and Poisoning Attacks"	""	"ArXiv"	""	""	"abs/1809.02861"	""	""	""	""	""	""	""	""	""	""	"On the Intriguing Connections of Regularization, Input Gradients and Transferability of Evasion and Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Demontis; M. Melis; M. Pintor; M. Jagielski; B. Biggio; A. Oprea; C. Nita-Rotaru; F. Roli"	"2019"	"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Demontis; P. Russu; B. Biggio; G. Fumera; F. Roli"	"2016"	"On Security and Sparsity of Linear Classifiers for Adversarial Settings"	""	"S+SSPR"	""	""	""	""	""	""	""	""	""	""	""	""	""	"On Security and Sparsity of Linear Classifiers for Adversarial Settings"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Deng; S. Garg; S. Jha; S. Mahloujifar; M. Mahmoody; A. Thakurta"	"2020"	"On the Power of Oblivious Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"On the Power of Oblivious Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Deng; S. Garg; S. Jha; S. Mahloujifar; M. Mahmoody; A. Thakurta"	"2021"	"A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Deng; C. Dwork; J. Wang; L. Zhang"	"2020"	"Interpreting Robust Optimization via Adversarial Influence Functions"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Interpreting Robust Optimization via Adversarial Influence Functions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Deng; H. He; J. Huang; W. J. Su"	"2020"	"Towards Understanding the Dynamics of the First-Order Adversaries"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Towards Understanding the Dynamics of the First-Order Adversaries"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. M. P. Dinakarrao; S. Amberkar; S. Rafatirad; H. Homayoun"	"2018"	"Efficient Utilization of Adversarial Training towards Robust Machine Learners and its Analysis"	""	"2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Efficient Utilization of Adversarial Training towards Robust Machine Learners and its Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. W. Ding; L. Wang; X. Jin"	"2019"	"advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch"	""	"ArXiv"	""	""	"abs/1902.07623"	""	""	""	""	""	""	""	""	""	""	"advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Ding; F. Yang; J. Huang"	"2020"	"Defending Support Vector Machines against Poisoning Attacks: the Hardness and Algorithm"	""	"ArXiv"	""	""	"abs/2006.07757"	""	""	""	""	""	""	""	""	""	""	"Defending Support Vector Machines against Poisoning Attacks: the Hardness and Algorithm"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Ding; F. Yang; J. Huang"	"2021"	"Defending SVMs against poisoning attacks: the hardness and DBSCAN approach"	""	"UAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Defending SVMs against poisoning attacks: the hardness and DBSCAN approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Ding; Y. Tian; F. Xu; Q. A. Li; S. Zhong"	"2019"	"Poisoning Attack on Deep Generative Models in Autonomous Driving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning Attack on Deep Generative Models in Autonomous Driving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Ding; Y. Tian; F. Xu; Q. A. Li; S. Zhong"	"2019"	"Trojan Attack on Deep Generative Models in Autonomous Driving"	""	"SecureComm"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Trojan Attack on Deep Generative Models in Autonomous Driving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. N. Docena"	"2020"	"Systematic Analysis Of Deep Neural Networks: Retrieving Sensitive Samples Via Smt Solving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Systematic Analysis Of Deep Neural Networks: Retrieving Sensitive Samples Via Smt Solving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Domingo-Enrich; Y. Mroueh"	"2022"	"Auditing Differential Privacy in High Dimensions with the Kernel Quantum Rényi Divergence"	""	"ArXiv"	""	""	"abs/2205.13941"	""	""	""	""	""	""	""	""	""	""	"Auditing Differential Privacy in High Dimensions with the Kernel Quantum Rényi Divergence"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Dong; Y. Wang; J. Lai; X. Xie"	"2022"	"Restricted Black-box Adversarial Attack Against DeepFake Face Swapping"	""	"ArXiv"	""	""	"abs/2204.12347"	""	""	""	""	""	""	""	""	""	""	"Restricted Black-box Adversarial Attack Against DeepFake Face Swapping"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Dong; D. Yan; R. Wang"	"2022"	"Adversarial Privacy Protection on Speech Enhancement"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Privacy Protection on Speech Enhancement"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Dong; B. Zhao; L. Lyu"	"2022"	"Privacy for Free: How does Dataset Condensation Help Privacy?"	""	"ArXiv"	""	""	"abs/2206.00240"	""	""	""	""	""	""	""	""	""	""	"Privacy for Free: How does Dataset Condensation Help Privacy?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Dong; H. Wang; Y.-d. Yao"	"2021"	"A Robust Adversarial Network-Based End-to-End Communications System With Strong Generalization Ability Against Adversarial Attacks"	""	"ArXiv"	""	""	"abs/2103.02654"	""	""	""	""	""	""	""	""	""	""	"A Robust Adversarial Network-Based End-to-End Communications System With Strong Generalization Ability Against Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Dong; P. Zhu; Q. Liu; Y. Chen; P. Xun"	"2018"	"Degrading Detection Performance of Wireless IDSs Through Poisoning Feature Selection"	""	"WASA"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Degrading Detection Performance of Wireless IDSs Through Poisoning Feature Selection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. Downing; Y. Mirsky; K. Park; W. Lee"	"2021"	"DeepReflect: Discovering Malicious Functionality through Binary Reconstruction"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"DeepReflect: Discovering Malicious Functionality through Binary Reconstruction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Du; C.-M. Pun"	"2020"	"Adversarial Image Attacks Using Multi-Sample and Most-Likely Ensemble Methods"	""	"Proceedings of the 28th ACM International Conference on Multimedia"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Image Attacks Using Multi-Sample and Most-Likely Ensemble Methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Duan; Z. Zhao; L. Bu; F. Song"	"2019"	"Things You May Not Know About Adversarial Example: A Black-box Adversarial Image Attack"	""	"ArXiv"	""	""	"abs/1905.07672"	""	""	""	""	""	""	""	""	""	""	"Things You May Not Know About Adversarial Example: A Black-box Adversarial Image Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. B. E. Becirovic; E. G. Larsson; J. Sefyrin; E. Wihlborg"	"2020"	"Publications 2019"	""	"Méthodos"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Publications 2019"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. J. L. Eggen"	"2019"	"Sobolev GANs and rates of convergence for a simple manifold learning problem"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Sobolev GANs and rates of convergence for a simple manifold learning problem"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E.-M. El-Mhamdi; R. Guerraoui; A. Guirguis; L. N. Hoang; S. Rouault"	"2020"	"Genuinely Distributed Byzantine Machine Learning"	""	"Proceedings of the 39th Symposium on Principles of Distributed Computing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Genuinely Distributed Byzantine Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. F. Elsayed; I. J. Goodfellow; J. Sohl-Dickstein"	"2019"	"Adversarial Reprogramming of Neural Networks"	""	"ArXiv"	""	""	"abs/1806.11146"	""	""	""	""	""	""	""	""	""	""	"Adversarial Reprogramming of Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Emami; R. Taheri"	"2019"	"Deep Learning Based Communication: an Adversarial Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Deep Learning Based Communication: an Adversarial Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Erkinay Ozdemir; Z. Ali; B. Subeshan; E. Asmatulu"	"2021"	"Applying machine learning approach in recycling"	""	"Journal of Material Cycles and Waste Management"	""	""	""	""	""	"1-17"	""	""	""	""	""	""	""	"Applying machine learning approach in recycling"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"I. Evtimov; K. Eykholt; E. Fernandes; T. Kohno; B. Li; A. Prakash; A. Rahmati; D. X. Song"	"2017"	"Robust Physical-World Attacks on Deep Learning Models"	""	"arXiv: Cryptography and Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Physical-World Attacks on Deep Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"I. Evtimov; K. Eykholt; E. Fernandes; T. Kohno; B. Li; A. Prakash; A. Rahmati; D. X. Song"	"2017"	"Robust Physical-World Attacks on Machine Learning Models"	""	"ArXiv"	""	""	"abs/1707.08945"	""	""	""	""	""	""	""	""	""	""	"Robust Physical-World Attacks on Machine Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Eykholt; I. Evtimov; E. Fernandes; B. Li; A. Rahmati; F. Tramèr; A. Prakash; T. Kohno; D. X. Song"	"2018"	"Physical Adversarial Examples for Object Detectors"	""	"ArXiv"	""	""	"abs/1807.07769"	""	""	""	""	""	""	""	""	""	""	"Physical Adversarial Examples for Object Detectors"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Eykholt; I. Evtimov; E. Fernandes; B. Li; D. X. Song; T. Kohno; A. Rahmati; A. Prakash; F. Tramèr"	"2017"	"Note on Attacking Object Detectors with Adversarial Stickers"	""	"ArXiv"	""	""	"abs/1712.08062"	""	""	""	""	""	""	""	""	""	""	"Note on Attacking Object Detectors with Adversarial Stickers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Fang; X. Cao; J. Jia; N. Z. Gong"	"2020"	"Local Model Poisoning Attacks to Byzantine-Robust Federated Learning"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Local Model Poisoning Attacks to Byzantine-Robust Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Fang; J. Liu; M. Momma; Y. Sun"	"2022"	"FairRoad: Achieving Fairness for Recommender Systems with Optimized Antidote Data"	""	"Proceedings of the 27th ACM on Symposium on Access Control Models and Technologies"	""	""	""	""	""	""	""	""	""	""	""	""	""	"FairRoad: Achieving Fairness for Recommender Systems with Optimized Antidote Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Fang; A. Choromańska"	"2020"	"Backdoor Attacks on the DNN Interpretation System"	""	"ArXiv"	""	""	"abs/2011.10698"	""	""	""	""	""	""	""	""	""	""	"Backdoor Attacks on the DNN Interpretation System"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Fang; Z. Li; G. Yang"	"2022"	"A novel approach to generating high-resolution adversarial examples"	""	"Appl. Intell."	""	""	"52"	""	""	"1289-1305"	""	""	""	""	""	""	""	"A novel approach to generating high-resolution adversarial examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. Farnia; A. Aghazadeh; J. Zou; D. Tse"	"2021"	"Group-Structured Adversarial Training"	""	"ArXiv"	""	""	"abs/2106.10324"	""	""	""	""	""	""	""	""	""	""	"Group-Structured Adversarial Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. Farokhi; M. A. Kâafar"	"2020"	"Modelling and Quantifying Membership Information Leakage in Machine Learning"	""	"ArXiv"	""	""	"abs/2001.10648"	""	""	""	""	""	""	""	""	""	""	"Modelling and Quantifying Membership Information Leakage in Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. F. Fladby"	"2018"	"Adaptive Network Flow Parameters for Stealthy Botnet Behavior"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adaptive Network Flow Parameters for Stealthy Botnet Behavior"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. F. Fladby; H. Haugerud; S. Nichele; K. M. Begnum; A. Yazidi"	"2020"	"Evading a Machine Learning-based Intrusion Detection System through Adversarial Perturbations"	""	"Proceedings of the International Conference on Research in Adaptive and Convergent Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evading a Machine Learning-based Intrusion Detection System through Adversarial Perturbations"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Fletcher"	"2022"	"Artificial Neural Network Based Approach for Malware Detection Artificial Neural Network Based Approach for Malware Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Artificial Neural Network Based Approach for Malware Detection Artificial Neural Network Based Approach for Malware Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Flores-Velazco"	"2022"	"Improved Search of Relevant Points for Nearest-Neighbor Classification"	""	"ArXiv"	""	""	"abs/2203.03567"	""	""	""	""	""	""	""	""	""	""	"Improved Search of Relevant Points for Nearest-Neighbor Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Foresti; J. López"	"2016"	"Information Security Theory and Practice"	""	"Lecture Notes in Computer Science"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Information Security Theory and Practice"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Fowl; P.-Y. Chiang; M. Goldblum; J. Geiping; A. Bansal; W. Czaja; T. Goldstein"	"2021"	"Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release"	""	"ArXiv"	""	""	"abs/2103.02683"	""	""	""	""	""	""	""	""	""	""	"Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Fowl; M. Goldblum; P.-Y. Chiang; J. Geiping; W. Czaja; T. Goldstein"	"2021"	"Adversarial Examples Make Strong Poisons"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples Make Strong Poisons"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Franci; M. Cordy; M. Gubri; M. Papadakis; Y. L. Traon"	"2020"	"Effective and Efficient Data Poisoning in Semi-Supervised Learning"	""	"ArXiv"	""	""	"abs/2012.07381"	""	""	""	""	""	""	""	""	""	""	"Effective and Efficient Data Poisoning in Semi-Supervised Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Franci; M. Cordy; M. Gubri; M. Papadakis; Y. L. Traon"	"2022"	"Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers"	""	"2022 IEEE/ACM 1st International Conference on AI Engineering – Software Engineering for AI (CAIN)"	""	""	""	""	""	"77-87"	""	""	""	""	""	""	""	"Influence-Driven Data Poisoning in Graph-Based Semi-Supervised Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. M. Freeman; S. Jain; M. Dürmuth; B. Biggio; G. Giacinto"	"2016"	"Who Are You? A Statistical Approach to Measuring User Authenticity"	""	"NDSS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Who Are You? A Statistical Approach to Measuring User Authenticity"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. Furon"	"2020"	"An alternative proof of the vulnerability of retrieval in high intrinsic dimensionality neighborhood"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"An alternative proof of the vulnerability of retrieval in high intrinsic dimensionality neighborhood"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Gálvez; V. Moonsamy; C. Díaz"	"2021"	"Less is More: A privacy-respecting Android malware classifier using federated learning"	""	"Proceedings on Privacy Enhancing Technologies"	""	""	"2021"	""	""	"96 - 116"	""	""	""	""	""	""	""	"Less is More: A privacy-respecting Android malware classifier using federated learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"G. Ganev"	"2021"	"DP-SGD vs PATE: Which Has Less Disparate Impact on GANs?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"DP-SGD vs PATE: Which Has Less Disparate Impact on GANs?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Ganev; B. Oprisanu; E. De Cristofaro"	"2021"	"Robin Hood and Matthew Effects - Differential Privacy Has Disparate Impact on Synthetic Data"	""	"ArXiv"	""	""	"abs/2109.11429"	""	""	""	""	""	""	""	""	""	""	"Robin Hood and Matthew Effects - Differential Privacy Has Disparate Impact on Synthetic Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Gao; T. Cai; H. Li; C.-J. Hsieh; L. Wang; J. Lee"	"2019"	"Convergence of Adversarial Training in Overparametrized Neural Networks"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Convergence of Adversarial Training in Overparametrized Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Gao; T. Cai; H. Li; L. Wang; C.-J. Hsieh; J. Lee"	"2019"	"L G ] 1 9 Ju n 20 19 Convergence of Adversarial Training in Overparametrized Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"L G ] 1 9 Ju n 20 19 Convergence of Adversarial Training in Overparametrized Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Gao; B. G. Doan; Z. Zhang; S. Ma; J. Zhang; A. Fu; S. Nepal; H. Kim"	"2020"	"Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"	""	"ArXiv"	""	""	"abs/2007.10760"	""	""	""	""	""	""	""	""	""	""	"Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. D. Gaspari; D. Hitaj; G. Pagnotta; L. D. Carli; L. V. Mancini"	"2020"	"The Naked Sun: Malicious Cooperation Between Benign-Looking Processes"	""	"ACNS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Naked Sun: Malicious Cooperation Between Benign-Looking Processes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Gaßner"	"2019"	"Maschinelles Lernen für die IT-Sicherheit"	""	"Künstliche Intelligenz"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Maschinelles Lernen für die IT-Sicherheit"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Ge; Q. Wang; J. Zhang; J. Zhou; Y. Zhang; C. Shen"	"2022"	"WaveFuzz: A Clean-Label Poisoning Attack to Protect Your Voice"	""	"ArXiv"	""	""	"abs/2203.13497"	""	""	""	""	""	""	""	""	""	""	"WaveFuzz: A Clean-Label Poisoning Attack to Protect Your Voice"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Geigel"	"2014"	"Unsupervised Learning Trojan"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Unsupervised Learning Trojan"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Geiping; L. Fowl; W. R. Huang; W. Czaja; G. Taylor; M. Moeller; T. Goldstein"	"2021"	"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching"	""	"ArXiv"	""	""	"abs/2009.02276"	""	""	""	""	""	""	""	""	""	""	"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Geldmacher; C. R. Yerkes; Y. Zhao"	"2021"	"Convolutional Neural Networks for Feature Extraction and Automated Target Recognition in Synthetic Aperture Radar Images"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Convolutional Neural Networks for Feature Extraction and Automated Target Recognition in Synthetic Aperture Radar Images"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Georges-Filteau; E. Cirillo"	"2020"	"Synthetic Observational Health Data with GANs: from slow adoption to a boom in medical research and ultimately digital twins?"	""	"arXiv: Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Synthetic Observational Health Data with GANs: from slow adoption to a boom in medical research and ultimately digital twins?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"d. N. Georgia; H. Ronan; J. Henrik; N. Rossen; M. Apostolos; S. M. J. Ignacio"	"2021"	"Cybersecurity challenges in the uptake of Artificial Intelligence in Autonomous Driving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Cybersecurity challenges in the uptake of Artificial Intelligence in Autonomous Driving"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Ghosal; A. Nandy; A. K. Das; S. Goswami; M. Panday"	"2019"	"A Short Review on Different Clustering Techniques and Their Applications"	""	"Advances in Intelligent Systems and Computing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Short Review on Different Clustering Techniques and Their Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Goldblum; D. Tsipras; C. Xie; X. Chen; A. Schwarzschild; D. X. Song; A. Madry; B. Li; T. Goldstein"	"2020"	"Data Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Gomrokchi; S. Amin; H. Aboutalebi; A. Wong; D. Precup"	"2021"	"Where Did You Learn That From? Surprising Effectiveness of Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning"	""	"ArXiv"	""	""	"abs/2109.03975"	""	""	""	""	""	""	""	""	""	""	"Where Did You Learn That From? Surprising Effectiveness of Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"G. Gondim-Ribeiro; P. Tabacof"	"2018"	"Input Encoder Latent Representation Decoder Output"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Input Encoder Latent Representation Decoder Output"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Gondim-Ribeiro; P. Tabacof; E. Valle"	"2018"	"Adversarial Attacks on Variational Autoencoders"	""	"ArXiv"	""	""	"abs/1806.04646"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks on Variational Autoencoders"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Gowal; S.-A. Rebuffi; O. Wiles; F. Stimberg; D. A. Calian; T. Mann"	"2021"	"Improving Robustness using Generated Data"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Improving Robustness using Generated Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. D. Grosso; H. Jalalzai; G. Pichler; C. Palamidessi; P. Piantanida"	"2022"	"Leveraging Adversarial Examples to Quantify Membership Information Leakage"	""	"ArXiv"	""	""	"abs/2203.09566"	""	""	""	""	""	""	""	""	""	""	"Leveraging Adversarial Examples to Quantify Membership Information Leakage"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Guan"	"2019"	"Neural Backdoors in NLP"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Neural Backdoors in NLP"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Gui; Z. Sun; Y. Wen; D. Tao; J. Ye"	"2021"	"A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications"	""	"ArXiv"	""	""	"abs/2001.06937"	""	""	""	""	""	""	""	""	""	""	"A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Gulshad; J. H. Metzen; A. W. M. Smeulders; Z. Akata"	"2019"	"Interpreting Adversarial Examples with Attributes"	""	"ArXiv"	""	""	"abs/1904.08279"	""	""	""	""	""	""	""	""	""	""	"Interpreting Adversarial Examples with Attributes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Guo; S. Ma; S. Guo"	"2019"	"MAANet: Multi-view Aware Attention Networks for Image Super-Resolution"	""	"ArXiv"	""	""	"abs/1904.06252"	""	""	""	""	""	""	""	""	""	""	"MAANet: Multi-view Aware Attention Networks for Image Super-Resolution"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Guo; S. Ma; J. Zhang; Q. Zhou; S. Guo"	"2020"	"Dual-view Attention Networks for Single Image Super-Resolution"	""	"Proceedings of the 28th ACM International Conference on Multimedia"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Dual-view Attention Networks for Single Image Super-Resolution"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L.-Z. Guo; Z. Zhou; Y.-F. Li"	"2022"	"Robust Deep Semi-Supervised Learning: A Brief Introduction"	""	"ArXiv"	""	""	"abs/2202.05975"	""	""	""	""	""	""	""	""	""	""	"Robust Deep Semi-Supervised Learning: A Brief Introduction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. P. Guralnik; B. Moran; A. Pezeshki; O. Arslan"	"2017"	"Detecting poisoning attacks on hierarchical malware classification systems"	""	"Defense + Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Detecting poisoning attacks on hierarchical malware classification systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Gür"	"2020"	"Expansive networks: Exploiting spectrum sharing for capacity boost and 6G vision"	""	"J. Commun. Networks"	""	""	"22"	""	""	"444-454"	""	""	""	""	""	""	""	"Expansive networks: Exploiting spectrum sharing for capacity boost and 6G vision"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Ha; J. Jang; Y. Jeong; S. Yoon"	"2022"	"Membership Feature Disentanglement Network"	""	"Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Membership Feature Disentanglement Network"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. M. Hafiz; G. M. Bhat"	"2020"	"Reinforcement Learning Based Handwritten Digit Recognition with Two-State Q-Learning"	""	"ArXiv"	""	""	"abs/2007.01193"	""	""	""	""	""	""	""	""	""	""	"Reinforcement Learning Based Handwritten Digit Recognition with Two-State Q-Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. M. Hafiz; G. M. Bhat"	"2021"	"Fast Training of Deep Networks with One-Class CNNs"	""	"ArXiv"	""	""	"abs/2007.00046"	""	""	""	""	""	""	""	""	""	""	"Fast Training of Deep Networks with One-Class CNNs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"O. Hajihassani"	"2021"	"Learning Representations for Anonymizing Sensor Data in IoT Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Learning Representations for Anonymizing Sensor Data in IoT Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"O. Hajihassani; O. Ardakanian; H. Khazaei"	"2021"	"ObscureNet: Learning Attribute-invariant Latent Representation for Anonymizing Sensor Data"	""	"Proceedings of the International Conference on Internet-of-Things Design and Implementation"	""	""	""	""	""	""	""	""	""	""	""	""	""	"ObscureNet: Learning Attribute-invariant Latent Representation for Anonymizing Sensor Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Hammoudeh; D. Lowd"	"2022"	"Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation"	""	"ArXiv"	""	""	"abs/2201.10055"	""	""	""	""	""	""	""	""	""	""	"Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Han; M. R. Min; A. Stathopoulos; Y. Tian; R. Gao; A. Kadav; D. N. Metaxas"	"2021"	"Dual Projection Generative Adversarial Networks for Conditional Image Generation"	""	"2021 IEEE/CVF International Conference on Computer Vision (ICCV)"	""	""	""	""	""	"14418-14427"	""	""	""	""	""	""	""	"Dual Projection Generative Adversarial Networks for Conditional Image Generation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Hanisch; P. A. Cabarcos; J. Parra-Arnau; T. Strufe"	"2021"	"Privacy-Protecting Techniques for Behavioral Data: A Survey"	""	"ArXiv"	""	""	"abs/2109.04120"	""	""	""	""	""	""	""	""	""	""	"Privacy-Protecting Techniques for Behavioral Data: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. Hassanzadeh; R. E. Tillman"	"2022"	"Generative Models with Information-Theoretic Protection Against Membership Inference Attacks"	""	"ArXiv"	""	""	"abs/2206.00071"	""	""	""	""	""	""	""	""	""	""	"Generative Models with Information-Theoretic Protection Against Membership Inference Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Hayase; W. Kong; R. Somani; S. Oh"	"2021"	"SPECTRE: Defending Against Backdoor Attacks Using Robust Covariance Estimation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"SPECTRE: Defending Against Backdoor Attacks Using Robust Covariance Estimation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Hayes; O. Ohrimenko"	"2018"	"Contamination Attacks and Mitigation in Multi-Party Machine Learning"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Contamination Attacks and Mitigation in Multi-Party Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. He; J. Wei; X. Chen; N. Carlini; D. X. Song"	"2017"	"Adversarial Example Defense: Ensembles of Weak Defenses are not Strong"	""	"ArXiv"	""	""	"abs/1706.04701"	""	""	""	""	""	""	""	""	""	""	"Adversarial Example Defense: Ensembles of Weak Defenses are not Strong"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. He; R. Wen; Y. Wu; M. Backes; Y. Shen; Y. Zhang"	"2021"	"Node-Level Membership Inference Attacks Against Graph Neural Networks"	""	"ArXiv"	""	""	"abs/2102.05429"	""	""	""	""	""	""	""	""	""	""	"Node-Level Membership Inference Attacks Against Graph Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. He; G. Meng; K. Chen; X. Hu; J. He"	"2019"	"Towards Privacy and Security of Deep Learning Systems: A Survey"	""	"ArXiv"	""	""	"abs/1911.12562"	""	""	""	""	""	""	""	""	""	""	"Towards Privacy and Security of Deep Learning Systems: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. He; Z. Shen; C. Xia; W. Tong; J. Hua; S. Zhong"	"2021"	"SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. He; T. Zhang; R. B. Lee"	"2018"	"VerIDeep: Verifying Integrity of Deep Neural Networks through Sensitive-Sample Fingerprinting"	""	"ArXiv"	""	""	"abs/1808.03277"	""	""	""	""	""	""	""	""	""	""	"VerIDeep: Verifying Integrity of Deep Neural Networks through Sensitive-Sample Fingerprinting"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Hedayat"	"2016"	"The Devil ’ s Right Hand : An Investigation on Malware-oriented Obfuscation Techniques"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Devil ’ s Right Hand : An Investigation on Malware-oriented Obfuscation Techniques"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Hein; M. Andriushchenko"	"2017"	"Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation"	""	"NIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Hidano; T. Murakami; S. Katsumata; S. Kiyomoto; G. Hanaoka"	"2018"	"A Framework for Model Inversion Attacks on Collaborative Filtering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Framework for Model Inversion Attacks on Collaborative Filtering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Hlihor; R. Volpi; L. Malagò"	"2020"	"Evaluating the Robustness of Defense Mechanisms based on AutoEncoder Reconstructions against Carlini-Wagner Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evaluating the Robustness of Defense Mechanisms based on AutoEncoder Reconstructions against Carlini-Wagner Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Hofmann; T. Rückel; N. Urbach"	"2021"	"Innovating with Artificial Intelligence: Capturing the Constructive Functional Capabilities of Deep Generative Learning"	""	"HICSS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Innovating with Artificial Intelligence: Capturing the Constructive Functional Capabilities of Deep Generative Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Hong; V. Chandrasekaran; Y. Kaya; T. Dumitras; N. Papernot"	"2020"	"On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping"	""	"ArXiv"	""	""	"abs/2002.11497"	""	""	""	""	""	""	""	""	""	""	"On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Hosseini; Y. Chen; S. Kannan; B. Zhang; R. Poovendran"	"2017"	"Blocking Transferability of Adversarial Examples in Black-Box Learning Systems"	""	"ArXiv"	""	""	"abs/1703.04318"	""	""	""	""	""	""	""	""	""	""	"Blocking Transferability of Adversarial Examples in Black-Box Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Hou; J. Zhang; K. L. Man; J. Ma; Z. Peng"	"2021"	"A Systematic Literature Review of Blockchain-based Federated Learning: Architectures, Applications and Issues"	""	"2021 2nd Information Communication Technologies Conference (ICTC)"	""	""	""	""	""	"302-307"	""	""	""	""	""	""	""	"A Systematic Literature Review of Blockchain-based Federated Learning: Architectures, Applications and Issues"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Hu; J. Pang"	"2021"	"Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks"	""	"Annual Computer Security Applications Conference"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Hu; J. Pang"	"2021"	"Model Extraction and Defenses on Generative Adversarial Networks"	""	"ArXiv"	""	""	"abs/2101.02069"	""	""	""	""	""	""	""	""	""	""	"Model Extraction and Defenses on Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Hu; Z. A. Salcic; G. Dobbie; X. Zhang"	"2022"	"Membership Inference Attacks on Machine Learning: A Survey"	""	"ACM Computing Surveys (CSUR)"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Membership Inference Attacks on Machine Learning: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Hu; W. Kuang; Z. Qin; K. Li; J. Zhang; Y. Gao; W. Li; K. Li"	"2021"	"Artificial Intelligence Security: Threats and Countermeasures"	""	"ACM Computing Surveys"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Artificial Intelligence Security: Threats and Countermeasures"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Huang"	"2021"	"Defense against Membership Inference Attack Applying Domain Adaptation with Addictive Noise"	""	"Journal of Computer and Communications"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Defense against Membership Inference Attack Applying Domain Adaptation with Addictive Noise"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Huang; W. Luo; G. Zeng; J. Weng; Y. Zhang; A. Yang"	"2020"	"DAMIA: Leveraging Domain Adaptation as a Defense against Membership Inference Attacks"	""	"ArXiv"	""	""	"abs/2005.08016"	""	""	""	""	""	""	""	""	""	""	"DAMIA: Leveraging Domain Adaptation as a Defense against Membership Inference Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Huang; Y. Wang; Z. Chen; Y. Li; Z. Tang; W. Chu; J. Chen; W. Lin; K.-K. Ma"	"2021"	"CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes"	""	"ArXiv"	""	""	"abs/2105.10872"	""	""	""	""	""	""	""	""	""	""	"CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Huang"	"2021"	"Designing adversarial signals against three deep learning and non-deep learning methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Designing adversarial signals against three deep learning and non-deep learning methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Hui; H. Wang; Z. Wang; X. Yang; Z. Liu; D. Jin; Y. Li"	"2022"	"Knowledge Enhanced GAN for IoT Traffic Generation"	""	"Proceedings of the ACM Web Conference 2022"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Knowledge Enhanced GAN for IoT Traffic Generation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Hutchings; S. Pastrana; R. Clayton"	"2019"	"Displacing big data : How criminals cheat the system"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Displacing big data : How criminals cheat the system"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"O. Ibitoye; R. A. Khamis; A. Matrawy; M. O. Shafiq"	"2019"	"The Threat of Adversarial Attacks on Machine Learning in Network Security - A Survey"	""	"ArXiv"	""	""	"abs/1911.02621"	""	""	""	""	""	""	""	""	""	""	"The Threat of Adversarial Attacks on Machine Learning in Network Security - A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Ilyas; A. Jalal; E. Asteri; C. Daskalakis; A. G. Dimakis"	"2017"	"The Robust Manifold Defense: Adversarial Training using Generative Models"	""	"ArXiv"	""	""	"abs/1712.09196"	""	""	""	""	""	""	""	""	""	""	"The Robust Manifold Defense: Adversarial Training using Generative Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. R. Insua; R. Naveiro; V. Gallego; J. Poulos"	"2020"	"Adversarial Machine Learning: Perspectives from Adversarial Risk Analysis"	""	"ArXiv"	""	""	"abs/2003.03546"	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning: Perspectives from Adversarial Risk Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Izmailov; S. Venkatesan; A. d. S. Reddy; R. Chadha; M. J. de Lucia; A. Oprea"	"2022"	"Poisoning attacks on machine learning models in cyber systems and mitigation strategies"	""	"Defense + Commercial Sensing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning attacks on machine learning models in cyber systems and mitigation strategies"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Jahanian; X. Puig; Y. Tian; P. Isola"	"2021"	"Generative Models as a Data Source for Multiview Representation Learning"	""	"ArXiv"	""	""	"abs/2106.05258"	""	""	""	""	""	""	""	""	""	""	"Generative Models as a Data Source for Multiview Representation Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Jatain; V. Singh; N. Dahiya"	"2021"	"A Contemplative Perspective on Federated Machine Learning: Taxonomy, Threats & Vulnerability Assessment and Challenges"	""	"Journal of King Saud University - Computer and Information Sciences"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Contemplative Perspective on Federated Machine Learning: Taxonomy, Threats & Vulnerability Assessment and Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Jha"	"2019"	"Trust, Resilience and Interpretability of AI Models"	""	"NSV@CAV"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Trust, Resilience and Interpretability of AI Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Jia; X. Cao; N. Z. Gong"	"2020"	"Certified Robustness of Nearest Neighbors against Data Poisoning Attacks"	""	"ArXiv"	""	""	"abs/2012.03765"	""	""	""	""	""	""	""	""	""	""	"Certified Robustness of Nearest Neighbors against Data Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Jia; X. Cao; N. Z. Gong"	"2021"	"Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Jia; H. Liu; N. Z. Gong"	"2021"	"10 Security and Privacy Problems in Self-Supervised Learning"	""	"ArXiv"	""	""	"abs/2110.15444"	""	""	""	""	""	""	""	""	""	""	"10 Security and Privacy Problems in Self-Supervised Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Jia; Y. Liu; X. Cao; N. Z. Gong"	"2020"	"Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Jia; B. Wang; N. Z. Gong"	"2021"	"Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes"	""	"Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"W. Jia"	"2022"	"Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Trafﬁc Sign Recognition Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Trafﬁc Sign Recognition Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Jia; Z. Lu; H. Zhang; Z. Liu; J. Wang; G. Qu"	"2022"	"Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems"	""	"ArXiv"	""	""	"abs/2201.06192"	""	""	""	""	""	""	""	""	""	""	"Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Jiang; J. Nagra; P. Ahammad"	"2016"	"SoK: Applying Machine Learning in Security - A Survey"	""	"ArXiv"	""	""	"abs/1611.03186"	""	""	""	""	""	""	""	""	""	""	"SoK: Applying Machine Learning in Security - A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Jin; Y. Li; H. Xu; Y. Wang; S. Ji; C. C. Aggarwal; J. Tang"	"2020"	"Adversarial Attacks and Defenses on Graphs"	""	"ACM SIGKDD Explorations Newsletter"	""	""	"22"	""	""	"19 - 34"	""	""	""	""	""	""	""	"Adversarial Attacks and Defenses on Graphs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Jin; Y. Li; H. Xu; Y. Wang; J. Tang"	"2020"	"Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study"	""	"ArXiv"	""	""	"abs/2003.00653"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Jordon; L. Szpruch; F. Houssiau; M. Bottarelli; G. Cherubin; C. Maple; S. N. Cohen; A. Weller"	"2022"	"Synthetic Data - what, why and how?"	""	"ArXiv"	""	""	"abs/2205.03257"	""	""	""	""	""	""	""	""	""	""	"Synthetic Data - what, why and how?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. D. Joseph; B. Nelson; B. I. P. Rubinstein; J. D. Tygar"	"2019"	"Background and Notation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Background and Notation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. D. Joseph; B. Nelson; B. I. P. Rubinstein; J. D. Tygar"	"2019"	"Adversarial Machine Learning Challenges"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning Challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. D. Joseph; B. Nelson; B. I. P. Rubinstein; J. D. Tygar"	"2019"	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. D. Joseph; B. Nelson; B. I. P. Rubinstein; J. D. Tygar"	"2019"	"A Framework for Secure Learning"	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Framework for Secure Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. M. R. Junior; C. A. Kamienski"	"2021"	"A Survey on Trustworthiness for the Internet of Things"	""	"IEEE Access"	""	""	"9"	""	""	"42493-42514"	""	""	""	""	""	""	""	"A Survey on Trustworthiness for the Internet of Things"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Karlavs; P. Li; R. Wu; N. M. Gurel; X. Chu; W. Wu; C. Zhang"	"2020"	"Nearest Neighbor Classifiers over Incomplete Information: From Certain Answers to Certain Predictions"	""	"Proc. VLDB Endow."	""	""	"14"	""	""	"255-267"	""	""	""	""	""	""	""	"Nearest Neighbor Classifiers over Incomplete Information: From Certain Answers to Certain Predictions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Khare; R. Aralikatte; S. Mani"	"2018"	"Adversarial Black-Box Attacks for Automatic Speech Recognition Systems Using Multi-Objective Genetic Optimization"	""	"ArXiv"	""	""	"abs/1811.01312"	""	""	""	""	""	""	""	""	""	""	"Adversarial Black-Box Attacks for Automatic Speech Recognition Systems Using Multi-Objective Genetic Optimization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Kilbertus; G. Parascandolo; B. Schölkopf"	"2018"	"Generalization in anti-causal learning"	""	"ArXiv"	""	""	"abs/1812.00524"	""	""	""	""	""	""	""	""	""	""	"Generalization in anti-causal learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Kim; Y. E. Sagduyu; K. Davaslioglu; T. Erpek; S. Ulukus"	"2022"	"Channel-Aware Adversarial Attacks Against Deep Learning-Based Wireless Signal Classifiers"	""	"IEEE Transactions on Wireless Communications"	""	""	"21"	""	""	"3868-3880"	""	""	""	""	""	""	""	"Channel-Aware Adversarial Attacks Against Deep Learning-Based Wireless Signal Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Kim; Y. Shi; Y. E. Sagduyu; T. Erpek; S. Ulukus"	"2021"	"Adversarial Attacks against Deep Learning Based Power Control in Wireless Communications"	""	"2021 IEEE Globecom Workshops (GC Wkshps)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Adversarial Attacks against Deep Learning Based Power Control in Wireless Communications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Kim; S. Tariq; S. S. Woo"	"2022"	"PTD: privacy-preserving human face processing framework using tensor decomposition"	""	"Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"PTD: privacy-preserving human face processing framework using tensor decomposition"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Y. Kim; S. Tariq; S. S. Woo"	"2022"	"PTD"	""	"Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"PTD"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Kong; Y. Li; J. Wang; A. Rezaei; H. Zhou"	"2020"	"KNN-enhanced Deep Learning Against Noisy Labels"	""	"ArXiv"	""	""	"abs/2012.04224"	""	""	""	""	""	""	""	""	""	""	"KNN-enhanced Deep Learning Against Noisy Labels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Kong; C. Liu"	"2019"	"Generating Adversarial Fragments with Adversarial Networks for Physical-world Implementation"	""	"ArXiv"	""	""	"abs/1907.04449"	""	""	""	""	""	""	""	""	""	""	"Generating Adversarial Fragments with Adversarial Networks for Physical-world Implementation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"I. C. Konstantakopoulos"	"2018"	"Statistical Learning Towards Gamification in Human-Centric Cyber-Physical Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Statistical Learning Towards Gamification in Human-Centric Cyber-Physical Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. M. Kornaropoulos; S. Ren; R. Tamassia"	"2022"	"The Price of Tailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures"	""	"Proceedings of the 2022 International Conference on Management of Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Price of Tailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Kossen; M. A. Hirzel; V. I. Madai; F. Boenisch; A. Hennemuth; K. Hildebrand; S. Pokutta; K. Sharma; A. Hilbert; J. Sobesky; I. Galinovic; A. A. Khalil; J. B. Fiebach; D. Frey"	"2022"	"Toward Sharing Brain Images: Differentially Private TOF-MRA Images With Segmentation Labels Using Generative Adversarial Networks"	""	"Frontiers in Artificial Intelligence"	""	""	"5"	""	""	""	""	""	""	""	""	""	""	"Toward Sharing Brain Images: Differentially Private TOF-MRA Images With Segmentation Labels Using Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Kostyuchenko; B. Lodonova; Y. Usoltsev; A. Konovalov"	"2021"	"Poisoning attack of the training dataset for on-line signature authentication using the perceptron"	""	"IOP Conference Series: Materials Science and Engineering"	""	""	"1069"	""	""	""	""	""	""	""	""	""	""	"Poisoning attack of the training dataset for on-line signature authentication using the perceptron"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Krishnan; J. Wang; E. Wu; M. J. Franklin; K. Goldberg"	"2016"	"ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models"	""	"ArXiv"	""	""	"abs/1601.03797"	""	""	""	""	""	""	""	""	""	""	"ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Krishnan; J. Wang; E. Wu; M. J. Franklin; K. Goldberg"	"2016"	"ActiveClean: Interactive Data Cleaning For Statistical Modeling"	""	"Proc. VLDB Endow."	""	""	"9"	""	""	"948-959"	""	""	""	""	""	""	""	"ActiveClean: Interactive Data Cleaning For Statistical Modeling"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Kumar"	"2021"	"Center Smoothing: Certified Robustness for Networks with Structured Outputs"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Center Smoothing: Certified Robustness for Networks with Structured Outputs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Kumar; T. Goldstein"	"2021"	"Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Kumar; T. A. Goldstein"	"2021"	"Center Smoothing for Certifiably Robust Vector-Valued Functions"	""	"ArXiv"	""	""	"abs/2102.09701"	""	""	""	""	""	""	""	""	""	""	"Center Smoothing for Certifiably Robust Vector-Valued Functions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Kumar; S. Mehta"	"2017"	"A Survey on Resilient Machine Learning"	""	"ArXiv"	""	""	"abs/1707.03184"	""	""	""	""	""	""	""	""	""	""	"A Survey on Resilient Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Kunar"	"2021"	"Effective and Privacy preserving Tabular Data Synthesizing"	""	"ArXiv"	""	""	"abs/2108.10064"	""	""	""	""	""	""	""	""	""	""	"Effective and Privacy preserving Tabular Data Synthesizing"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Kunar; R. Birke; Z. Zhao; L. Y. Chen"	"2021"	"DTGAN: Differential Private Training for Tabular GANs"	""	"ArXiv"	""	""	"abs/2107.02521"	""	""	""	""	""	""	""	""	""	""	"DTGAN: Differential Private Training for Tabular GANs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Kuo"	"2019"	"Generative Synthesis of Insurance Datasets"	""	"ArXiv"	""	""	"abs/1912.02423"	""	""	""	""	""	""	""	""	""	""	"Generative Synthesis of Insurance Datasets"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Kuppa; N.-A. Le-Khac"	"2022"	"Learn to Adapt: Robust Drift Detection in Security Domain"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Learn to Adapt: Robust Drift Detection in Security Domain"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P.-A. Laharotte"	"2016"	"Contributions à la prévision court-terme, multi-échelle et multi-variée, par apprentissage statistique du trafic routier"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Contributions à la prévision court-terme, multi-échelle et multi-variée, par apprentissage statistique du trafic routier"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Landauer; M. Wurzenberger; F. Skopik; G. Settanni; P. Filzmoser"	"2018"	"Time Series Analysis: Unsupervised Anomaly Detection Beyond Outlier Detection"	""	"ISPEC"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Time Series Analysis: Unsupervised Anomaly Detection Beyond Outlier Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Leal-Taixé; S. Roth"	"2018"	"Computer Vision – ECCV 2018 Workshops"	""	"Lecture Notes in Computer Science"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Computer Vision – ECCV 2018 Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Leclerc; H. Salman; A. Ilyas; S. Vemprala; L. Engstrom; V. Vineet; K. Y. Xiao; P. Zhang; S. Santurkar; G. Yang; A. Kapoor; A. Madry"	"2021"	"3DB: A Framework for Debugging Computer Vision Models"	""	"ArXiv"	""	""	"abs/2106.03805"	""	""	""	""	""	""	""	""	""	""	"3DB: A Framework for Debugging Computer Vision Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Lécuyer; V. Atlidakis; R. Geambasu; D. J. Hsu; S. S. Jana"	"2018"	"On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning"	""	"ArXiv"	""	""	"abs/1802.03471"	""	""	""	""	""	""	""	""	""	""	"On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Lécuyer; V. Atlidakis; R. Geambasu; D. J. Hsu; S. S. Jana"	"2019"	"Certified Robustness to Adversarial Examples with Differential Privacy"	""	"2019 IEEE Symposium on Security and Privacy (SP)"	""	""	""	""	""	"656-672"	""	""	""	""	""	""	""	"Certified Robustness to Adversarial Examples with Differential Privacy"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Lee"	"2021"	"Invertible Tabular GANs: Killing Two Birds with OneStone for Tabular Data Synthesis"	""	"ArXiv"	""	""	"abs/2202.03636"	""	""	""	""	""	""	""	""	""	""	"Invertible Tabular GANs: Killing Two Birds with OneStone for Tabular Data Synthesis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"K. Lee; C. Suh; K. Ramchandran"	"2020"	"Reprogramming GANs via Input Noise Design"	""	"ECML/PKDD"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Reprogramming GANs via Input Noise Design"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Lesnikowski; G. d. S. P. Moreira; S. Rabhi; K. Byleen-Higley"	"2021"	"Synthetic Data and Simulators for Recommendation Systems: Current State and Future Directions"	""	"ArXiv"	""	""	"abs/2112.11022"	""	""	""	""	""	""	""	""	""	""	"Synthetic Data and Simulators for Recommendation Systems: Current State and Future Directions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Li; Y. Wang; A. Singh; Y. Vorobeychik"	"2016"	"Data Poisoning Attacks on Factorization-Based Collaborative Filtering"	""	"NIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks on Factorization-Based Collaborative Filtering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Li; Q. Li; Y. Ye; S. Xu"	"2020"	"SoK: Arms Race in Adversarial Malware Detection"	""	"ArXiv"	""	""	"abs/2005.11671"	""	""	""	""	""	""	""	""	""	""	"SoK: Arms Race in Adversarial Malware Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Li; Q. Li; Y. Ye; S. Xu"	"2020"	"Arms Race in Adversarial Malware Detection: A Survey"	""	"ACM Computing Surveys"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Arms Race in Adversarial Malware Detection: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Li; P. Zhu; J. Li; Z. Yang; N. Cao; Z. Chen"	"2018"	"Security Matters: A Survey on Adversarial Machine Learning"	""	"ArXiv"	""	""	"abs/1810.07339"	""	""	""	""	""	""	""	""	""	""	"Security Matters: A Survey on Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Li; B. J. Cairns; J. Li; T. Zhu"	"2021"	"Generating Synthetic Mixed-type Longitudinal Electronic Health Records for Artificial Intelligent Applications"	""	"ArXiv"	""	""	"abs/2112.12047"	""	""	""	""	""	""	""	""	""	""	"Generating Synthetic Mixed-type Longitudinal Electronic Health Records for Artificial Intelligent Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Li; J. Cao; Y. Zhang; J. Chen; M. Tan"	"2021"	"Learning Defense Transformers for Counterattacking Adversarial Examples"	""	"ArXiv"	""	""	"abs/2103.07595"	""	""	""	""	""	""	""	""	""	""	"Learning Defense Transformers for Counterattacking Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Li; H. Chen"	"2022"	"Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline"	""	"ArXiv"	""	""	"abs/2206.01733"	""	""	""	""	""	""	""	""	""	""	"Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. Li; Q. Liu; W. Zhao; D. Wang; S. Wang"	"2018"	"BEBP: An Poisoning Method Against Machine Learning Based IDSs"	""	"ArXiv"	""	""	"abs/1803.03965"	""	""	""	""	""	""	""	""	""	""	"BEBP: An Poisoning Method Against Machine Learning Based IDSs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Q. Li; Y. Guo; H. Chen"	"2020"	"Practical No-box Adversarial Attacks against DNNs"	""	"ArXiv"	""	""	"abs/2012.02525"	""	""	""	""	""	""	""	""	""	""	"Practical No-box Adversarial Attacks against DNNs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Li; B. Z. H. Zhao; J. Yu; M. Xue; D. Kaafar; H. Zhu"	"2019"	"Invisible Backdoor Attacks Against Deep Neural Networks"	""	"ArXiv"	""	""	"abs/1909.02742"	""	""	""	""	""	""	""	""	""	""	"Invisible Backdoor Attacks Against Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Li; Y. Chen; Y. He; H. Xue"	"2019"	"AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With Approximate Gradients"	""	"ArXiv"	""	""	"abs/1911.06591"	""	""	""	""	""	""	""	""	""	""	"AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With Approximate Gradients"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Li; Y. Li; B. Wu; L. Li; R. He; S. Lyu"	"2020"	"Backdoor Attack with Sample-Specific Triggers"	""	"ArXiv"	""	""	"abs/2012.03816"	""	""	""	""	""	""	""	""	""	""	"Backdoor Attack with Sample-Specific Triggers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Li; Y. Sharma"	"2019"	"Are Generative Classifiers More Robust to Adversarial Attacks?"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Are Generative Classifiers More Robust to Adversarial Attacks?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Li; T. Zhai; B. Wu; Y. Jiang; Z. Li; S. Xia"	"2020"	"Rethinking the Trigger of Backdoor Attack"	""	"ArXiv"	""	""	"abs/2004.04692"	""	""	""	""	""	""	""	""	""	""	"Rethinking the Trigger of Backdoor Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Li; W. Zhang; J. Liu; X. Kou; H. Li; J. Cui"	"2021"	"Enhanced countering adversarial attacks via input denoising and feature restoring"	""	"ArXiv"	""	""	"abs/2111.10075"	""	""	""	""	""	""	""	""	""	""	"Enhanced countering adversarial attacks via input denoising and feature restoring"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Li; J. Shao; Y. Mao; J. H. Wang; J. Zhang"	"2022"	"Federated Learning with GAN-based Data Synthesis for Non-IID Clients"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Federated Learning with GAN-based Data Synthesis for Non-IID Clients"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Li; P. Xia; M. Yue; B. Li"	"2020"	"Direct Adversarial Training for GANs"	""	"ArXiv"	""	""	"abs/2008.09041"	""	""	""	""	""	""	""	""	""	""	"Direct Adversarial Training for GANs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Li; J. Zhang; L. Liu; J. Liu"	"2022"	"Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage"	""	"ArXiv"	""	""	"abs/2203.15696"	""	""	""	""	""	""	""	""	""	""	"Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Lin; K. Xiong"	"2021"	"Applying the Mahalanobis Distance to Develop Robust Approaches Against False Data Injection Attacks on Dynamic Power State Estimation"	""	"ArXiv"	""	""	"abs/2105.08873"	""	""	""	""	""	""	""	""	""	""	"Applying the Mahalanobis Distance to Develop Robust Approaches Against False Data Injection Attacks on Dynamic Power State Estimation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y.-C. Lin; M.-Y. Liu; M. Sun; J.-B. Huang"	"2017"	"Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight"	""	"ArXiv"	""	""	"abs/1710.00814"	""	""	""	""	""	""	""	""	""	""	"Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Lindqvist; S. Sugrim; R. Izmailov"	"2018"	"AutoGAN: Robust Classifier Against Adversarial Attacks"	""	"ArXiv"	""	""	"abs/1812.03405"	""	""	""	""	""	""	""	""	""	""	"AutoGAN: Robust Classifier Against Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Liu"	"2020"	"Anonymized GCN: A Novel Robust Graph Embedding Method via Hiding Node Position in Noise"	""	"ArXiv"	""	""	"abs/2005.03482"	""	""	""	""	""	""	""	""	""	""	"Anonymized GCN: A Novel Robust Graph Embedding Method via Hiding Node Position in Noise"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Liu; B. Li; T. Li; P. Zhou; R. Wang"	"2020"	"AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Liu; B. Li; Y. Vorobeychik; A. Oprea"	"2016"	"Robust High-Dimensional Linear Regression"	""	"ArXiv"	""	""	"abs/1608.02257"	""	""	""	""	""	""	""	""	""	""	"Robust High-Dimensional Linear Regression"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Liu; B. Li; Y. Vorobeychik; A. Oprea"	"2017"	"Robust Linear Regression Against Training Data Poisoning"	""	"Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Linear Regression Against Training Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Liu; T. Derr; Z. Liu; J. Tang"	"2019"	"Say What I Want: Towards the Dark Side of Neural Dialogue Models"	""	"ArXiv"	""	""	"abs/1909.06044"	""	""	""	""	""	""	""	""	""	""	"Say What I Want: Towards the Dark Side of Neural Dialogue Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Liu"	"2017"	"Deficient data classification with fuzzy learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Deficient data classification with fuzzy learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Liu; W. Kong; S. M. Kakade; S. Oh"	"2021"	"Robust and Differentially Private Mean Estimation"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust and Differentially Private Mean Estimation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Liu; Y. Li; C. Wu; C.-J. Hsieh"	"2019"	"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network"	""	"ArXiv"	""	""	"abs/1810.01279"	""	""	""	""	""	""	""	""	""	""	"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Liu; Y. Xu; S. Mukherjee; J. M. L. Ferres"	"2020"	"MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models"	""	"ArXiv"	""	""	"abs/2009.05683"	""	""	""	""	""	""	""	""	""	""	"MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Liu; S. Ma; Y. Aafer; W.-C. Lee; J. Zhai; W. Wang; X. Zhang"	"2018"	"Trojaning Attack on Neural Networks"	""	"NDSS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Trojaning Attack on Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Liu; R. Wen; X. He; A. Salem; Z. Zhang; M. Backes; E. De Cristofaro; M. Fritz; Y. Zhang"	"2021"	"ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models"	""	"ArXiv"	""	""	"abs/2102.02551"	""	""	""	""	""	""	""	""	""	""	"ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Liu; Z. Xiang; E. J. Seong; A. Kapadia; D. S. Williamson"	"2021"	"Defending Against Microphone-Based Attacks with Personalized Noise"	""	"Proceedings on Privacy Enhancing Technologies"	""	""	"2021"	""	""	"130 - 150"	""	""	""	""	""	""	""	"Defending Against Microphone-Based Attacks with Personalized Noise"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Liu; Z. Zhao; A. Kolmus; T. Berns; T. v. Laarhoven; T. M. Heskes; M. Larson"	"2021"	"Going Grayscale: The Road to Understanding and Improving Unlearnable Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Going Grayscale: The Road to Understanding and Improving Unlearnable Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Long; V. Bindschaedler; L. Wang; D. Bu; X. Wang; H. Tang; C. A. Gunter; K. Chen"	"2018"	"Understanding Membership Inferences on Well-Generalized Learning Models"	""	"ArXiv"	""	""	"abs/1802.04889"	""	""	""	""	""	""	""	""	""	""	"Understanding Membership Inferences on Well-Generalized Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Luo; X. Zhu"	"2020"	"Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning"	""	"ArXiv"	""	""	"abs/2004.12571"	""	""	""	""	""	""	""	""	""	""	"Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Ma; J. Wei; B. Liu; M. Ding; L. Yuan; Z. Han; H. V. Poor"	"2022"	"Trusted AI in Multi-agent Systems: An Overview of Privacy and Security for Distributed Learning"	""	"ArXiv"	""	""	"abs/2202.09027"	""	""	""	""	""	""	""	""	""	""	"Trusted AI in Multi-agent Systems: An Overview of Privacy and Security for Distributed Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Ma; T. Chen; T.-K. Hu; C. You; X. Xie; Z. Wang"	"2021"	"Undistillable: Making A Nasty Teacher That CANNOT teach students"	""	"ArXiv"	""	""	"abs/2105.07381"	""	""	""	""	""	""	""	""	""	""	"Undistillable: Making A Nasty Teacher That CANNOT teach students"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. R. Machado; E. Silva; R. R. Goldschmidt"	"2020"	"Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"	""	"ArXiv"	""	""	"abs/2009.03728"	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Madhusudhanan; R. R. Nair"	"2019"	"Converging Security Threats and Attacks Insinuation in Multidisciplinary Machine Learning Applications: A Survey"	""	"2019 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)"	""	""	""	""	""	"217-222"	""	""	""	""	""	""	""	"Converging Security Threats and Attacks Insinuation in Multidisciplinary Machine Learning Applications: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. Mahlangu; S. January; C. T. Mashiane; T. M. Dlamini; S. Ngobeni; L. N. Ruxwana"	"2019"	"‘Data Poisoning’ – Achilles heel of cyber threat intelligence systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"‘Data Poisoning’ – Achilles heel of cyber threat intelligence systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Mahloujifar; D. I. Diochnos; M. Mahmoody"	"2018"	"Learning under p-Tampering Attacks"	""	"ArXiv"	""	""	"abs/1711.03707"	""	""	""	""	""	""	""	""	""	""	"Learning under p-Tampering Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Mahloujifar; D. I. Diochnos; M. Mahmoody"	"2019"	"The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Mahloujifar; H. A. Inan; M. Chase; E. Ghosh; M. Hasegawa"	"2021"	"Membership Inference on Word Embedding and Beyond"	""	"ArXiv"	""	""	"abs/2106.11384"	""	""	""	""	""	""	""	""	""	""	"Membership Inference on Word Embedding and Beyond"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Mahloujifar; M. Mahmoody"	"2017"	"Blockwise p-Tampering Attacks on Cryptographic Primitives, Extractors, and Learners"	""	"TCC"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Blockwise p-Tampering Attacks on Cryptographic Primitives, Extractors, and Learners"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Mahloujifar; M. Mahmoody"	"2019"	"Can Adversarially Robust Learning Leverage Computational Hardness?"	""	"ArXiv"	""	""	"abs/1810.01407"	""	""	""	""	""	""	""	""	""	""	"Can Adversarially Robust Learning Leverage Computational Hardness?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Mahloujifar; M. Mahmoody; A. Mohammed"	"2018"	"Multi-party Poisoning through Generalized p-Tampering"	""	"IACR Cryptol. ePrint Arch."	""	""	"2018"	""	""	"854"	""	""	""	""	""	""	""	"Multi-party Poisoning through Generalized p-Tampering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Mahmoud; N. Aggarwal; A. Nobbe; J. R. S. Vicarte; S. V. Adve; C. W. Fletcher; I. Frosio; S. K. S. Hari"	"2020"	"PyTorchFI: A Runtime Perturbation Tool for DNNs"	""	"2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)"	""	""	""	""	""	"25-31"	""	""	""	""	""	""	""	"PyTorchFI: A Runtime Perturbation Tool for DNNs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Maiorca"	"2016"	"Design and implementation of robust systems for secure malware detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Design and implementation of robust systems for secure malware detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Maiorca; D. Ariu; I. Corona; G. Giacinto"	"2015"	"An Evasion Resilient Approach to the Detection of Malicious PDF Files"	""	"ICISSP"	""	""	""	""	""	""	""	""	""	""	""	""	""	"An Evasion Resilient Approach to the Detection of Malicious PDF Files"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Marulli; E. Bellini; S. Marrone"	"2020"	"A Security-Oriented Architecture for Federated Learning in Cloud Environments"	""	"AINA Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Security-Oriented Architecture for Federated Learning in Cloud Environments"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. S. Master; N. Galinkin; L. Aroyo"	"2017"	"Catch Them If You Can A Simulation Study on Malicious Behavior in a Cultural Heritage Question Answering System"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Catch Them If You Can A Simulation Study on Malicious Behavior in a Cultural Heritage Question Answering System"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Mathur; C. P. Gupta"	"2019"	"Big Data Challenges and Issues: A Review"	""	"Lecture Notes on Data Engineering and Communications Technologies"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Big Data Challenges and Issues: A Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Meenakshi; G. Maragatham"	"2019"	"A Review on Security Attacks and Protective Strategies of Machine Learning"	""	"Emerging Trends in Computing and Expert Technology"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Review on Security Attacks and Protective Strategies of Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. E. Meester; J. J. Molenaar; M. Nuyens; Y. Rozenholc; K. v. Winden"	"2016"	"Catch them if you can"	""	"Nature"	""	""	"535"	""	""	"S68-S76"	""	""	""	""	""	""	""	"Catch them if you can"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Meinke"	"2019"	"T OWARDS NEURAL NETWORKS THAT PROVABLY KNOW WHEN THEY DON ’ T KNOW"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"T OWARDS NEURAL NETWORKS THAT PROVABLY KNOW WHEN THEY DON ’ T KNOW"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Meinke; M. Hein"	"2020"	"Towards neural networks that provably know when they don't know"	""	"ArXiv"	""	""	"abs/1909.12180"	""	""	""	""	""	""	""	""	""	""	"Towards neural networks that provably know when they don't know"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Meng; J. Huang; Z. Zeng; X. Jiang; S. Yu; T.-P. Jung; Chin-Teng; Lin; R. Chavarriaga; D. Wu"	"2020"	"EEG-Based Brain-Computer Interfaces Are Vulnerable to Back- door Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"EEG-Based Brain-Computer Interfaces Are Vulnerable to Back- door Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Meng; J. Huang; Z. Zeng; X. Jiang; S. Yu; T.-P. Jung; C.-T. Lin; R. Chavarriaga; D. Wu"	"2020"	"EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks"	""	"ArXiv"	""	""	"abs/2011.00101"	""	""	""	""	""	""	""	""	""	""	"EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. M. E. Mhamdi; E. Mahdi"	"2020"	"Robust Distributed Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Distributed Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Mi; J. Guan; S. Zhou"	"2022"	"ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning"	""	"ArXiv"	""	""	"abs/2202.04311"	""	""	""	""	""	""	""	""	""	""	"ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Miettinen; T. D. Nguyen; A.-R. Sadeghi; N. Asokan"	"2019"	"IoT Cloud Service Device-type Identification Policy Database IoT deviceIoT deviceIoT deviceIoT device IoT device Device Fingerprinting Policy Application WiFi Ethernet IoT deviceIoT device IoT device Local SOHO network IoT Gateway Internet"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"IoT Cloud Service Device-type Identification Policy Database IoT deviceIoT deviceIoT deviceIoT device IoT device Device Fingerprinting Policy Application WiFi Ethernet IoT deviceIoT device IoT device Local SOHO network IoT Gateway Internet"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. J. Miller; Z. Xiang; G. Kesidis"	"2019"	"Adversarial Learning in Statistical Classification: A Comprehensive Review of Defenses Against Attacks"	""	"ArXiv"	""	""	"abs/1904.06292"	""	""	""	""	""	""	""	""	""	""	"Adversarial Learning in Statistical Classification: A Comprehensive Review of Defenses Against Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Min; L. Chen; A. Karbasi"	"2021"	"The Curious Case of Adversarially Robust Models: More Data Can Help, Double Descend, or Hurt Generalization"	""	"UAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Curious Case of Adversarially Robust Models: More Data Can Help, Double Descend, or Hurt Generalization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Ming; Z. Xin; P. Lan; D. Wu; P. Liu; B. Mao"	"2015"	"Replacement Attacks: Automatically Impeding Behavior-Based Malware Specifications"	""	"ACNS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Replacement Attacks: Automatically Impeding Behavior-Based Malware Specifications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Ming; Z. Xin; P. Lan; D. Wu; P. Liu; B. Mao"	"2016"	"Impeding behavior-based malware analysis via replacement attacks to malware specifications"	""	"Journal of Computer Virology and Hacking Techniques"	""	""	"13"	""	""	"193-207"	""	""	""	""	""	""	""	"Impeding behavior-based malware analysis via replacement attacks to malware specifications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Mohapatra; X. He; G. Kamath; O. Thakkar"	"2020"	"DIFFINDO! Differentially Private Learning with Noisy Labels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"DIFFINDO! Differentially Private Learning with Noisy Labels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"I. Moisejevs"	"2019"	"Adversarial Attacks and Defenses in Intrusion Detection Systems: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks and Defenses in Intrusion Detection Systems: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Montanari; Y. Wu"	"2022"	"Adversarial Examples in Random Neural Networks with General Activations"	""	"ArXiv"	""	""	"abs/2203.17209"	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples in Random Neural Networks with General Activations"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Mukherjee; R. Muthukumar"	"2020"	"Guarantees on learning depth-2 neural networks under a data-poisoning attack"	""	"ArXiv"	""	""	"abs/2005.01699"	""	""	""	""	""	""	""	""	""	""	"Guarantees on learning depth-2 neural networks under a data-poisoning attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Mukherjee; Y. Xu; A. Trivedi; N. Patowary; J. M. L. Ferres"	"2021"	"privGAN: Protecting GANs from membership inference attacks at low cost to utility"	""	"Proceedings on Privacy Enhancing Technologies"	""	""	"2021"	""	""	"142 - 163"	""	""	""	""	""	""	""	"privGAN: Protecting GANs from membership inference attacks at low cost to utility"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Muñoz-González; B. Biggio; A. Demontis; A. Paudice; V. Wongrassamee; E. C. Lupu; F. Roli"	"2017"	"Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization"	""	"Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Muñoz-González; E. C. Lupu"	"2018"	"The Security of Machine Learning Systems"	""	"AI in Cybersecurity"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Security of Machine Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Muñoz-González; B. Pfitzner; M. Russo; J. Carnerero-Cano; E. C. Lupu"	"2019"	"Poisoning Attacks with Generative Adversarial Nets"	""	"ArXiv"	""	""	"abs/1906.07773"	""	""	""	""	""	""	""	""	""	""	"Poisoning Attacks with Generative Adversarial Nets"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. M. Müller"	"2021"	"DEFENDING AGAINST ADVERSARIAL DENIAL-OF-SERVICE DATA POISONING ATTACKS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"DEFENDING AGAINST ADVERSARIAL DENIAL-OF-SERVICE DATA POISONING ATTACKS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. M. Müller; D. Kowatsch; K. Böttinger"	"2020"	"Data Poisoning Attacks on Regression Learning and Corresponding Defenses"	""	"2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC)"	""	""	""	""	""	"80-89"	""	""	""	""	""	""	""	"Data Poisoning Attacks on Regression Learning and Corresponding Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. M. Müller; S. Roschmann; K. Böttinger"	"2021"	"Defending against Adversarial Denial-of-Service Attacks"	""	"ArXiv"	""	""	"abs/2104.06744"	""	""	""	""	""	""	""	""	""	""	"Defending against Adversarial Denial-of-Service Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. T. Nalisnick; A. Matsukawa; Y. W. Teh; D. Görür; B. Lakshminarayanan"	"2019"	"Do Deep Generative Models Know What They Don't Know?"	""	"ArXiv"	""	""	"abs/1810.09136"	""	""	""	""	""	""	""	""	""	""	"Do Deep Generative Models Know What They Don't Know?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Narayanan; M. Chandramohan; L. Chen; Y. Liu"	"2017"	"A multi-view context-aware approach to Android malware detection and malicious code localization"	""	"Empirical Software Engineering"	""	""	"23"	""	""	"1222-1274"	""	""	""	""	""	""	""	"A multi-view context-aware approach to Android malware detection and malicious code localization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Naseri; J. Hayes; E. De Cristofaro"	"2020"	"Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy"	""	"ArXiv"	""	""	"abs/2009.03561"	""	""	""	""	""	""	""	""	""	""	"Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Naseri; J. Hayes; E. De Cristofaro"	"2022"	"Local and Central Differential Privacy for Robustness and Privacy in Federated Learning"	""	"Proceedings 2022 Network and Distributed System Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Local and Central Differential Privacy for Robustness and Privacy in Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Nellutla; M. Mohammed"	"2020"	"Survey: A Comparative Study of Different Security Issues in Big Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Survey: A Comparative Study of Different Security Issues in Big Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. T. P. Nguyen"	"2021"	"Feature Engineering and Health Indicator Construction for Fault Detection and Diagnostic"	""	"Springer Series in Reliability Engineering"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Feature Engineering and Health Indicator Construction for Fault Detection and Diagnostic"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. D. Nguyen; S. Marchal; M. Miettinen; H. Fereidooni; N. Asokan; A.-R. Sadeghi"	"2018"	"IoT Security Service Device-Type Identification Anomaly Detection Models IoT deviceIoT device IoT deviceIoT device IoT device Device Fingerprinting Anomaly Detection WiFi Ethernet IoT deviceIoT device IoT device Local SOHO network Botnet server Security Gateway Infected IoT device Infected IoT devic"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"IoT Security Service Device-Type Identification Anomaly Detection Models IoT deviceIoT device IoT deviceIoT device IoT device Device Fingerprinting Anomaly Detection WiFi Ethernet IoT deviceIoT device IoT device Local SOHO network Botnet server Security Gateway Infected IoT device Infected IoT devic"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Nie; N. Wang; J. Li; Y. Wang; K. Wang"	"2022"	"Prediction of Liquid Magnetization Series Data in Agriculture Based on Enhanced CGAN"	""	"Frontiers in Plant Science"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Prediction of Liquid Magnetization Series Data in Agriculture Based on Enhanced CGAN"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Odena"	"2019"	"Open Questions about Generative Adversarial Networks"	""	"Distill"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Open Questions about Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Oleszkiewicz; T. Wlodarczyk; K. J. Piczak; T. Trzciński; P. Kairouz; R. Rajagopal"	"2018"	"Siamese Generative Adversarial Privatizer for Biometric Data"	""	"ArXiv"	""	""	"abs/1804.08757"	""	""	""	""	""	""	""	""	""	""	"Siamese Generative Adversarial Privatizer for Biometric Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Ortiz-Jiménez; A. Modas; S.-M. Moosavi-Dezfooli; P. Frossard"	"2021"	"Optimism in the Face of Adversity: Understanding and Improving Deep Learning Through Adversarial Robustness"	""	"Proceedings of the IEEE"	""	""	"109"	""	""	"635-659"	""	""	""	""	""	""	""	"Optimism in the Face of Adversity: Understanding and Improving Deep Learning Through Adversarial Robustness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"O. A. Osoba; W. Welser"	"2017"	"The Risks of Artificial Intelligence to Security and the Future of Work"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"The Risks of Artificial Intelligence to Security and the Future of Work"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Otation"	"2022"	"P OISONING AND B ACKDOORING C ONTRASTIVE L EARNING"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"P OISONING AND B ACKDOORING C ONTRASTIVE L EARNING"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. P'erez; P. Arroba; J. M. Moya"	"2022"	"Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks"	""	"ArXiv"	""	""	"abs/2201.06147"	""	""	""	""	""	""	""	""	""	""	"Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Pal; C. Chelmis; M. Frîncu; V. K. Prasanna"	"2014"	"Clustering for Demand Response An Online Algorithmic Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Clustering for Demand Response An Online Algorithmic Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. Pang; X. Yang; Y. Dong; H. Su; J. Zhu"	"2021"	"Accumulative Poisoning Attacks on Real-time Data"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Accumulative Poisoning Attacks on Real-time Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Papernot; P. Mcdaniel; A. Sinha; M. P. Wellman"	"2016"	"Towards the Science of Security and Privacy in Machine Learning"	""	"ArXiv"	""	""	"abs/1611.03814"	""	""	""	""	""	""	""	""	""	""	"Towards the Science of Security and Privacy in Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S.-w. Park; Y.-J. Yeo; Y.-G. Shin"	"2021"	"Generative Adversarial Network using Perturbed-Convolutions"	""	"ArXiv"	""	""	"abs/2101.10841"	""	""	""	""	""	""	""	""	""	""	"Generative Adversarial Network using Perturbed-Convolutions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Patil; X. Wang; X. Wang; S. Mao"	"2021"	"Adversarial Attacks on Deep Learning-based Floor Classification and Indoor Localization"	""	"Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks on Deep Learning-based Floor Classification and Indoor Localization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Paudice; L. Muñoz-González; A. György; E. C. Lupu"	"2018"	"Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection"	""	"ArXiv"	""	""	"abs/1802.03041"	""	""	""	""	""	""	""	""	""	""	"Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Paudice; L. Muñoz-González; E. C. Lupu"	"2018"	"Label Sanitization against Label Flipping Poisoning Attacks"	""	"ArXiv"	""	""	"abs/1803.00992"	""	""	""	""	""	""	""	""	""	""	"Label Sanitization against Label Flipping Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Paxton; I. S. Moskowitz; S. Russell; P. D. Hyden"	"2014"	"Developing a Network Science Based Approach to Cyber Incident Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Developing a Network Science Based Approach to Cyber Incident Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Pedersen; R. Munoz-G'omez; J. Huang; H. Sun; W.-W. Tu; I. Guyon"	"2022"	"LTU Attacker for Membership Inference"	""	"ArXiv"	""	""	"abs/2202.02278"	""	""	""	""	""	""	""	""	""	""	"LTU Attacker for Membership Inference"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Pérez; S. Nesmachnow; J. Toutouh; E. Hemberg; U.-M. O’Reilly"	"2020"	"Parallel/distributed implementation of cellular training for generative adversarial neural networks"	""	"2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)"	""	""	""	""	""	"512-518"	""	""	""	""	""	""	""	"Parallel/distributed implementation of cellular training for generative adversarial neural networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Pestana; N. Akhtar; W. Liu; D. G. Glance; A. S. Mian"	"2020"	"Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space"	""	"ArXiv"	""	""	"abs/2003.00883"	""	""	""	""	""	""	""	""	""	""	"Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"PierazziFabio; MezzourGhita; Hanqian; ColajanniMichele; S. SubrahmanianV."	"2020"	"A Data-driven Characterization of Modern Android Spyware"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Data-driven Characterization of Modern Android Spyware"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Pitropakis; E. A. Panaousis; A. Giannakoulias; G. Kalpakis; R. D. Rodriguez; P. Sarigiannidis"	"2018"	"An Enhanced Cyber Attack Attribution Framework"	""	"TrustBus"	""	""	""	""	""	""	""	""	""	""	""	""	""	"An Enhanced Cyber Attack Attribution Framework"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Pizarro; D. Kolossa; A. Fischer"	"2021"	"Robustifying automatic speech recognition by extracting slowly varying features"	""	"ArXiv"	""	""	"abs/2112.07400"	""	""	""	""	""	""	""	""	""	""	"Robustifying automatic speech recognition by extracting slowly varying features"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. D. Platzer; T. Reutterer"	"2021"	"Holdout-Based Fidelity and Privacy Assessment of Mixed-Type Synthetic Data"	""	"ArXiv"	""	""	"abs/2104.00635"	""	""	""	""	""	""	""	""	""	""	"Holdout-Based Fidelity and Privacy Assessment of Mixed-Type Synthetic Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Polyzotis; M. A. Zaharia"	"2021"	"What can Data-Centric AI Learn from Data and ML Engineering?"	""	"ArXiv"	""	""	"abs/2112.06439"	""	""	""	""	""	""	""	""	""	""	"What can Data-Centric AI Learn from Data and ML Engineering?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. E. Pope; Y. Balaji; S. Feizi"	"2019"	"FLOW-BASED GENERATIVE MODELS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"FLOW-BASED GENERATIVE MODELS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Qian; J. Su; Z. Wen; D. N. Jha; Y. Li; Y. Guan; D. Puthal; P. James; R. Yang; A. Y. Zomaya; O. F. Rana; L. Wang; R. Ranjan"	"2019"	"Orchestrating Development Lifecycle of Machine Learning Based IoT Applications: A Survey"	""	"ArXiv"	""	""	"abs/1910.05433"	""	""	""	""	""	""	""	""	""	""	"Orchestrating Development Lifecycle of Machine Learning Based IoT Applications: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Qian; K. Huang; Q. Wang; X.-Y. Zhang"	"2022"	"A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies"	""	"ArXiv"	""	""	"abs/2203.14046"	""	""	""	""	""	""	""	""	""	""	"A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"QianBin; Sujie; WenZhenyu; J. Nandan; LiYinhao; Guanyu; PuthalDeepak; JamesPhilip; YangRenyu; Y. ZomayaAlbert; RanaOmer; WangLizhe; KoutnyMaciej; RanjanRajiv"	"2020"	"Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey"	""	"ACM Computing Surveys"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Qiu; Y. Du; T. Lu"	"2022"	"The Framework of Cross-Domain and Model Adversarial Attack against Deepfake"	""	"Future Internet"	""	""	"14"	""	""	"46"	""	""	""	""	""	""	""	"The Framework of Cross-Domain and Model Adversarial Attack against Deepfake"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. J. Rad; A. Szabó"	"2021"	"Lookahead adversarial learning for near real-time semantic segmentation"	""	"Comput. Vis. Image Underst."	""	""	"212"	""	""	"103271"	""	""	""	""	""	""	""	"Lookahead adversarial learning for near real-time semantic segmentation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. J. Rad; A. Szabó; M. Presutto"	"2020"	"Lookahead Adversarial Semantic Segmentation"	""	"ArXiv"	""	""	"abs/2006.11227"	""	""	""	""	""	""	""	""	""	""	"Lookahead Adversarial Semantic Segmentation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Raghunathan; J. Steinhardt; P. Liang"	"2018"	"Certified Defenses against Adversarial Examples"	""	"ArXiv"	""	""	"abs/1801.09344"	""	""	""	""	""	""	""	""	""	""	"Certified Defenses against Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Rakhsha; G. Radanovic; R. Devidze; X. Zhu; A. K. Singla"	"2020"	"Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning"	""	"ArXiv"	""	""	"abs/2003.12909"	""	""	""	""	""	""	""	""	""	""	"Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. S. Rakin; Z. He; B. Gong; D. Fan"	"2018"	"Robust Pre-Processing: A Robust Defense Method Against Adversary Attack"	""	"arXiv: Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Pre-Processing: A Robust Defense Method Against Adversary Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. S. Rakin; Z. He; B. Gong; D. Fan"	"2018"	"Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples"	""	"ArXiv"	""	""	"abs/1802.01549"	""	""	""	""	""	""	""	""	""	""	"Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. S. Rakin; Y. Luo; X. Xu; D. Fan"	"2021"	"Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Ramasinghe; K. Fernando; S. H. Khan; N. Barnes"	"2021"	"Robust normalizing flows using Bernstein-type polynomials"	""	"ArXiv"	""	""	"abs/2102.03509"	""	""	""	""	""	""	""	""	""	""	"Robust normalizing flows using Bernstein-type polynomials"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. A. Ramírez; S.-K. Kim; H. A. Hamadi; E. Damiani; Y.-J. Byon; T.-Y. Kim; C.-S. Cho; C. Y. Yeun"	"2022"	"Poisoning Attacks and Defenses on Artificial Intelligence: A Survey"	""	"ArXiv"	""	""	"abs/2202.10276"	""	""	""	""	""	""	""	""	""	""	"Poisoning Attacks and Defenses on Artificial Intelligence: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Rawal; D. B. Rawat; B. M. Sadler"	"2021"	"Recent advances in adversarial machine learning: status, challenges and perspectives"	""	"Defense + Commercial Sensing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Recent advances in adversarial machine learning: status, challenges and perspectives"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Rawat; K. Levacher; M. Sinn"	"2021"	"The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks"	""	"ArXiv"	""	""	"abs/2108.01644"	""	""	""	""	""	""	""	""	""	""	"The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. Razmi; L. Xiong"	"2021"	"Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks"	""	"ArXiv"	""	""	"abs/2108.04206"	""	""	""	""	""	""	""	""	""	""	"Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. S. Reddy; S. M. Lakshmi"	"2021"	"Exploring adversarial attacks against malware classifiers in the backdoor poisoning attack"	""	"IOP Conference Series: Materials Science and Engineering"	""	""	"1022"	""	""	""	""	""	""	""	""	""	""	"Exploring adversarial attacks against malware classifiers in the backdoor poisoning attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. R. Reibman; F. M. Zhu; E. Cai; A. Chen; D. Chen; Q. Chen; Y.-H. Chen; J. Guo; M. Gupta; S. Han"	"2017"	"THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF DISSERTATION APPROVAL"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF DISSERTATION APPROVAL"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Ren; Z. Zhang; J. Jin; X. Zhao; S. Wu; Y. Zhou; Y. Shen; T. Che; R. Jin; D. Dou"	"2021"	"Integrated Defense for Resilient Graph Matching"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Integrated Defense for Resilient Graph Matching"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Rigaki"	"2017"	"Adversarial deep learning against intrusion detection classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial deep learning against intrusion detection classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Robiah; M. M. Zaki; S. S. Rahayu; A. M. Faizal; S. Shahrin; M. D. R. Fadhlee"	"2016"	"Formulating Generalize Malware Attack Pattern Using Features Selection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Formulating Generalize Malware Attack Pattern Using Features Selection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Robl"	"2022"	"A SSESSING D IFFERENTIALLY P RIVATE V ARIATIONAL A UTOENCODERS UNDER M EMBERSHIP I NFERENCE"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A SSESSING D IFFERENTIALLY P RIVATE V ARIATIONAL A UTOENCODERS UNDER M EMBERSHIP I NFERENCE"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Rodr'iguez-Barroso; D. J. e. L'opez; M. V. Luz'on; F. Herrera; E. Martínez-Cámara"	"2022"	"Survey on Federated Learning Threats: concepts, taxonomy on attacks and defences, experimental study and challenges"	""	"ArXiv"	""	""	"abs/2201.08135"	""	""	""	""	""	""	""	""	""	""	"Survey on Federated Learning Threats: concepts, taxonomy on attacks and defences, experimental study and challenges"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Roli; B. Biggio; G. Fumera"	"2013"	"Pattern Recognition Systems under Attack"	""	"CIARP"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Pattern Recognition Systems under Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Ronan; J. Henrik; S. M. J. Ignacio"	"2020"	"Robustness and Explainability of Artificial Intelligence"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robustness and Explainability of Artificial Intelligence"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Rosca; B. Lakshminarayanan; S. Mohamed"	"2018"	"Distribution Matching in Variational Inference"	""	"ArXiv"	""	""	"abs/1802.06847"	""	""	""	""	""	""	""	""	""	""	"Distribution Matching in Variational Inference"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"I. Rosenberg; E. Gudes"	"2016"	"Attacking and Defending Dynamic Analysis System-Calls Based IDS"	""	"WISTP"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Attacking and Defending Dynamic Analysis System-Calls Based IDS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"I. Rosenberg; E. Gudes"	"2016"	"Evading System-Calls Based Intrusion Detection Systems"	""	"NSS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Evading System-Calls Based Intrusion Detection Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Rosenblatt; X.-Y. Liu; S. Pouyanfar; E. d. Leon; A. M. Desai; J. Allen"	"2020"	"Differentially Private Synthetic Data: Applied Evaluations and Enhancements"	""	"ArXiv"	""	""	"abs/2011.05537"	""	""	""	""	""	""	""	""	""	""	"Differentially Private Synthetic Data: Applied Evaluations and Enhancements"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"E. Rosenfeld; E. Winston; P. Ravikumar; J. Z. Kolter"	"2020"	"Certified Robustness to Label-Flipping Attacks via Randomized Smoothing"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Certified Robustness to Label-Flipping Attacks via Randomized Smoothing"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Ruiz; S. A. Bargal; S. Sclaroff"	"2020"	"Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems"	""	"ECCV Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Ruiz; S. A. Bargal; S. Sclaroff"	"2020"	"Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks"	""	"ArXiv"	""	""	"abs/2006.06493"	""	""	""	""	""	""	""	""	""	""	"Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Ruiz; S. A. Bargal; S. Sclaroff"	"2021"	"BLACK-BOX IMAGE TRANSLATION DEEPFAKE GENERATION SYSTEMS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"BLACK-BOX IMAGE TRANSLATION DEEPFAKE GENERATION SYSTEMS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Ruiz-Shulcloper; G. S. d. Baja"	"2013"	"Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications"	""	"Lecture Notes in Computer Science"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. E. Sagduyu; T. Erpek; Y. Shi"	"2021"	"Adversarial Machine Learning for 5G Communications Security"	""	"ArXiv"	""	""	"abs/2101.02656"	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning for 5G Communications Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. E. Sagduyu; Y. Shi; T. Erpek; W. C. Headley; B. Flowers; G. Stantchev; Z. Lu"	"2020"	"When Wireless Security Meets Machine Learning: Motivation, Challenges, and Research Directions"	""	"ArXiv"	""	""	"abs/2001.08883"	""	""	""	""	""	""	""	""	""	""	"When Wireless Security Meets Machine Learning: Motivation, Challenges, and Research Directions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Sahay; C. G. Brinton; D. J. Love"	"2021"	"A Deep Ensemble-based Wireless Receiver Architecture for Mitigating Adversarial Interference in Automatic Modulation Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Deep Ensemble-based Wireless Receiver Architecture for Mitigating Adversarial Interference in Automatic Modulation Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Sajeeda; B. M. M. Hossain"	"2022"	"Exploring Generative Adversarial Networks and Adversarial Training"	""	"International Journal of Cognitive Computing in Engineering"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Exploring Generative Adversarial Networks and Adversarial Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Salem; R. Wen; M. Backes; S. Ma; Y. Zhang"	"2020"	"Dynamic Backdoor Attacks Against Machine Learning Models"	""	"ArXiv"	""	""	"abs/2003.03675"	""	""	""	""	""	""	""	""	""	""	"Dynamic Backdoor Attacks Against Machine Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Salman; A. Ilyas; L. Engstrom; S. Vemprala; A. Madry; A. Kapoor"	"2021"	"Unadversarial Examples: Designing Objects for Robust Vision"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Unadversarial Examples: Designing Objects for Robust Vision"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Samanta; S. Mehta"	"2017"	"L G ] 1 0 Ju l 2 01 7 Towards Crafting Text Adversarial Samples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"L G ] 1 0 Ju l 2 01 7 Towards Crafting Text Adversarial Samples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Samanta; S. Mehta"	"2017"	"Towards Crafting Text Adversarial Samples"	""	"ArXiv"	""	""	"abs/1707.02812"	""	""	""	""	""	""	""	""	""	""	"Towards Crafting Text Adversarial Samples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Samanta; S. Mehta"	"2018"	"Generating Adversarial Text Samples"	""	"ECIR"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generating Adversarial Text Samples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Saralajew; L. Holdijk; T. Villmann"	"2020"	"Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Fast Adversarial Robustness Certification of Nearest Prototype Classifiers for Arbitrary Seminorms"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Sarkar; A. Bansal; U. Mahbub; R. Chellappa"	"2017"	"UPSET and ANGRI : Breaking High Performance Image Classifiers"	""	"ArXiv"	""	""	"abs/1707.01159"	""	""	""	""	""	""	""	""	""	""	"UPSET and ANGRI : Breaking High Performance Image Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Schmidt; S. Santurkar; D. Tsipras; K. Talwar; A. Madry"	"2018"	"Adversarially Robust Generalization Requires More Data"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarially Robust Generalization Requires More Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Seetharaman; S. Malaviya; K. Rosni; M. Shukla; S. P. Lodha"	"2022"	"Influence Based Defense Against Data Poisoning Attacks in Online Learning"	""	"2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Influence Based Defense Against Data Poisoning Attacks in Online Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Segalis"	"2020"	"Disrupting Deepfakes with an Adversarial Attack that Survives Training"	""	"ArXiv"	""	""	"abs/2006.12247"	""	""	""	""	""	""	""	""	""	""	"Disrupting Deepfakes with an Adversarial Attack that Survives Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"V. Sehwag; A. N. Bhagoji; L. Song; C. Sitawarin; D. Cullina; M. Chiang; P. Mittal"	"2019"	"Better the Devil you Know: An Analysis of Evasion Attacks using Out-of-Distribution Adversarial Examples"	""	"ArXiv"	""	""	"abs/1905.01726"	""	""	""	""	""	""	""	""	""	""	"Better the Devil you Know: An Analysis of Evasion Attacks using Out-of-Distribution Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. G. Selvaganapathy; G. S. Sadasivam; V. Ravi"	"2021"	"A Review on Android Malware: Attacks, Countermeasures and Challenges Ahead"	""	"J. Cyber Secur. Mobil."	""	""	"10"	""	""	"177-230"	""	""	""	""	""	""	""	"A Review on Android Malware: Attacks, Countermeasures and Challenges Ahead"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J.-y. Seo; S. Park; J. Kang"	"2021"	"Secure wireless communication via adversarial machine learning: A Priori vs. A Posteriori"	""	"ICT Express"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Secure wireless communication via adversarial machine learning: A Priori vs. A Posteriori"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. C. Serban; E. Poll"	"2018"	"Adversarial Examples - A Complete Characterisation of the Phenomenon"	""	"ArXiv"	""	""	"abs/1810.01185"	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples - A Complete Characterisation of the Phenomenon"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. S. Sethi"	"2017"	"Dynamic adversarial mining - effectively applying machine learning in adversarial non-stationary environments"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Dynamic adversarial mining - effectively applying machine learning in adversarial non-stationary environments"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"G. Severi; J. Meyer; S. E. Coull"	"2021"	"Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. Severi; J. Meyer; S. E. Coull; A. Oprea"	"2020"	"Exploring Backdoor Poisoning Attacks Against Malware Classifiers"	""	"ArXiv"	""	""	"abs/2003.01031"	""	""	""	""	""	""	""	""	""	""	"Exploring Backdoor Poisoning Attacks Against Malware Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Shapira; D. Berend; I. Rosenberg; Y. Liu; A. Shabtai; Y. Elovici"	"2020"	"Being Single Has Benefits. Instance Poisoning to Deceive Malware Classifiers"	""	"ArXiv"	""	""	"abs/2010.16323"	""	""	""	""	""	""	""	""	""	""	"Being Single Has Benefits. Instance Poisoning to Deceive Malware Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Sharma; D. R. Mangrulkar"	"2019"	"DEEP LEARNING APPLICATIONS IN CYBER SECURITY: A COMPREHENSIVE REVIEW, CHALLENGES AND PROSPECTS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"DEEP LEARNING APPLICATIONS IN CYBER SECURITY: A COMPREHENSIVE REVIEW, CHALLENGES AND PROSPECTS"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Sharma; S. Verma; S. Medya; S. Ranu; A. Bhattacharya"	"2021"	"Task and Model Agnostic Adversarial Attack on Graph Neural Networks"	""	"ArXiv"	""	""	"abs/2112.13267"	""	""	""	""	""	""	""	""	""	""	"Task and Model Agnostic Adversarial Attack on Graph Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Shen; X. He; Y. Han; Y. Zhang"	"2021"	"Model Stealing Attacks Against Inductive Graph Neural Networks"	""	"ArXiv"	""	""	"abs/2112.08331"	""	""	""	""	""	""	""	""	""	""	"Model Stealing Attacks Against Inductive Graph Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Shi; Y. E. Sagduyu"	"2022"	"Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning"	""	"ArXiv"	""	""	"abs/2107.12173"	""	""	""	""	""	""	""	""	""	""	"Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Shi; Y. E. Sagduyu"	"2022"	"Jamming Attacks on Federated Learning in Wireless Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Jamming Attacks on Federated Learning in Wireless Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Shirazi; B. Bezawada; I. Ray; C. Anderson"	"2019"	"Adversarial Sampling Attacks Against Phishing Detection"	""	"DBSec"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Sampling Attacks Against Phishing Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. Shrestha; A. Omidkar; S. A. Roudi; R. Abbas; S. Kim"	"2021"	"Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Machine-Learning-Enabled Intrusion Detection System for Cellular Connected UAV Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Shukla"	"2014"	"A Review ON K-means DATA Clustering APPROACH"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Review ON K-means DATA Clustering APPROACH"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"W. Shuwei; W. Baosheng; Y. Tang; Y. Bo"	"2015"	"Malware Clustering Based on SNN Density Using System Calls"	""	"IEEE CLOUD 2015"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Malware Clustering Based on SNN Density Using System Calls"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Siddiqi"	"2019"	"Adversarial Security Attacks and Perturbations on Machine Learning and Deep Learning Methods"	""	"ArXiv"	""	""	"abs/1907.07291"	""	""	""	""	""	""	""	""	""	""	"Adversarial Security Attacks and Perturbations on Machine Learning and Deep Learning Methods"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. D. Silva; J. Kim; R. Raich"	"2020"	"Cost Aware Adversarial Learning"	""	"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"	""	""	""	""	""	"3587-3591"	""	""	""	""	""	""	""	"Cost Aware Adversarial Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Simonetto; S. Dyrmishi; S. Ghamizi; M. Cordy; Y. L. Traon"	"2021"	"A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space"	""	"ArXiv"	""	""	"abs/2112.01156"	""	""	""	""	""	""	""	""	""	""	"A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Sisaat; S. Kittitornkun; H. Kikuchi; C. Yukonhiatou; M. Terada; H. Ishii"	"2016"	"A Spatio-Temporal malware and country clustering algorithm: 2012 IIJ MITF case study"	""	"International Journal of Information Security"	""	""	"16"	""	""	"459-473"	""	""	""	""	""	""	""	"A Spatio-Temporal malware and country clustering algorithm: 2012 IIJ MITF case study"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Sitawarin; A. N. Bhagoji; A. Mosenia; M. Chiang; P. Mittal"	"2018"	"DARTS: Deceiving Autonomous Cars with Toxic Signs"	""	"ArXiv"	""	""	"abs/1802.06430"	""	""	""	""	""	""	""	""	""	""	"DARTS: Deceiving Autonomous Cars with Toxic Signs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"C. Sitawarin; E. M. Kornaropoulos; D. X. Song; D. A. Wagner"	"2021"	"Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Sivan; R. Bitton; A. Shabtai"	"2019"	"Analysis of Location Data Leakage in the Internet Traffic of Android-based Mobile Devices"	""	"RAID"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Analysis of Location Data Leakage in the Internet Traffic of Android-based Mobile Devices"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Solans; B. Biggio; C. Castillo"	"2020"	"Poisoning Attacks on Algorithmic Fairness"	""	"ECML/PKDD"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning Attacks on Algorithmic Fairness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Song; Z. Ou"	"2018"	"Generative Modeling by Inclusive Neural Random Fields with Applications in Image Generation and Anomaly Detection"	""	"arXiv: Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generative Modeling by Inclusive Neural Random Fields with Applications in Image Generation and Anomaly Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Song; Z. Ou"	"2018"	"Learning Neural Random Fields with Inclusive Auxiliary Generators"	""	"ArXiv"	""	""	"abs/1806.00271"	""	""	""	""	""	""	""	""	""	""	"Learning Neural Random Fields with Inclusive Auxiliary Generators"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Sorvisto"	"2018"	"THREATS AND CHALLENGES IN MACHINE LEARNING"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"THREATS AND CHALLENGES IN MACHINE LEARNING"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Stadler; B. Oprisanu; C. Troncoso"	"2020"	"Synthetic Data - A Privacy Mirage"	""	"ArXiv"	""	""	"abs/2011.07018"	""	""	""	""	""	""	""	""	""	""	"Synthetic Data - A Privacy Mirage"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Stevens; O. Suciu; A. Ruef; S. Hong; M. W. Hicks; T. Dumitras"	"2017"	"Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning"	""	"ArXiv"	""	""	"abs/1701.04739"	""	""	""	""	""	""	""	""	""	""	"Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"I. Stoica; D. X. Song; R. A. Popa; D. A. Patterson; M. W. Mahoney; R. H. Katz; A. D. Joseph; M. I. Jordan; J. M. Hellerstein; J. E. Gonzalez; K. Goldberg; A. Ghodsi; D. Culler; P. Abbeel"	"2017"	"A Berkeley View of Systems Challenges for AI"	""	"ArXiv"	""	""	"abs/1712.05855"	""	""	""	""	""	""	""	""	""	""	"A Berkeley View of Systems Challenges for AI"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Subedar; N. A. Ahuja; R. Krishnan; I. J. Ndiour; O. Tickoo"	"2019"	"Deep Probabilistic Models to Detect Data Poisoning Attacks"	""	"ArXiv"	""	""	"abs/1912.01206"	""	""	""	""	""	""	""	""	""	""	"Deep Probabilistic Models to Detect Data Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. Subramaniam; T. Kossen; K. Ritter; A. Hennemuth; K. Hildebrand; A. Hilbert; J. Sobesky; M. Livne; I. Galinovic; A. A. Khalil; J. B. Fiebach; D. Frey; V. I. Madai"	"2022"	"Generating 3D TOF-MRA volumes and segmentation labels using generative adversarial networks"	""	"Medical image analysis"	""	""	"78"	""	""	"102396"	""	""	""	""	""	""	""	"Generating 3D TOF-MRA volumes and segmentation labels using generative adversarial networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Sun; J. Plawinski; S. Subramaniam; A. Jamaludin; T. Kadir; A. Readie; G. Ligozio; D. I. Ohlssen; M. Baillie; T. Coroller"	"2021"	"A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs"	""	"ArXiv"	""	""	"abs/2106.13199"	""	""	""	""	""	""	""	""	""	""	"A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Sun; T. Zhu; Z. Zhang; D. Jin; P. Xiong; W. Zhou"	"2021"	"Adversarial Attacks Against Deep Generative Models on Data: A Survey"	""	"ArXiv"	""	""	"abs/2112.00247"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks Against Deep Generative Models on Data: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Sun; S. Wang; X. Tang; T.-Y. Hsieh"	"2020"	"Non-target-specific Node Injection Attacks on Graph Neural Networks: A Hierarchical Reinforcement Learning Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Non-target-specific Node Injection Attacks on Graph Neural Networks: A Hierarchical Reinforcement Learning Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Sun; S. Wang; X. Tang; T.-Y. Hsieh; V. G. Honavar"	"2019"	"Node Injection Attacks on Graphs via Reinforcement Learning"	""	"ArXiv"	""	""	"abs/1909.06543"	""	""	""	""	""	""	""	""	""	""	"Node Injection Attacks on Graphs via Reinforcement Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Suvak; M. F. Anjos; L. Brotcorne; D. Cattaruzza"	"2021"	"Design of Poisoning Attacks on Linear Regression Using Bilevel Optimization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Design of Poisoning Attacks on Linear Regression Using Bilevel Optimization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Taheri; R. Javidan; M. Shojafar; Z. Pooranian; A. Miri; M. Conti"	"2020"	"On defending against label flipping attacks on malware detection systems"	""	"Neural Computing and Applications"	""	""	""	""	""	"1-20"	""	""	""	""	""	""	""	"On defending against label flipping attacks on malware detection systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Tan; N. Lai"	"2019"	"CS 229: Milestone Learning Adversarially Robust and Rich Image Transformations for Object Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"CS 229: Milestone Learning Adversarially Robust and Rich Image Transformations for Object Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Tanay; J. T. A. Andrews; L. D. Griffin"	"2018"	"Built-in Vulnerabilities to Imperceptible Adversarial Perturbations"	""	"ArXiv"	""	""	"abs/1806.07409"	""	""	""	""	""	""	""	""	""	""	"Built-in Vulnerabilities to Imperceptible Adversarial Perturbations"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. J.-W. Tann; E.-C. Chang"	"2021"	"Poisoning of Online Learning Filters: DDoS Attacks and Countermeasures"	""	"ArXiv"	""	""	"abs/2107.12612"	""	""	""	""	""	""	""	""	""	""	"Poisoning of Online Learning Filters: DDoS Attacks and Countermeasures"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Tao; L. Feng; H. Wei; J. Yi; S.-J. Huang; S. Chen"	"2022"	"Can Adversarial Training Be Manipulated By Non-Robust Features?"	""	"ArXiv"	""	""	"abs/2201.13329"	""	""	""	""	""	""	""	""	""	""	"Can Adversarial Training Be Manipulated By Non-Robust Features?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Tao; L. Feng; J. Yi; S.-J. Huang; S. Chen"	"2021"	"Provable Defense Against Delusive Poisoning"	""	"ArXiv"	""	""	"abs/2102.04716"	""	""	""	""	""	""	""	""	""	""	"Provable Defense Against Delusive Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"L. Tao; L. Feng; J. Yi; S.-J. Huang; S. Chen"	"2021"	"Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. Tavallali; V. Behzadan; P. Tavallali; M. Singhal"	"2021"	"Adversarial Poisoning Attacks and Defense for General Multi-Class Models Based On Synthetic Reduced Nearest Neighbors"	""	"ArXiv"	""	""	"abs/2102.05867"	""	""	""	""	""	""	""	""	""	""	"Adversarial Poisoning Attacks and Defense for General Multi-Class Models Based On Synthetic Reduced Nearest Neighbors"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"V. Y. Terziyan; M. Golovianko; S. Gryshko"	"2018"	"Industry 4.0 Intelligence under Attack : From Cognitive Hack to Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Industry 4.0 Intelligence under Attack : From Cognitive Hack to Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Thakur; B. Li"	"2021"	"PAT: Pseudo-Adversarial Training For Detecting Adversarial Videos"	""	"ArXiv"	""	""	"abs/2109.05695"	""	""	""	""	""	""	""	""	""	""	"PAT: Pseudo-Adversarial Training For Detecting Adversarial Videos"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Tian; Z. Zhong; V. Ordonez; G. E. Kaiser; B. Ray"	"2020"	"Testing DNN Image Classifiers for Confusion & Bias Errors"	""	"2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)"	""	""	""	""	""	"304-305"	""	""	""	""	""	""	""	"Testing DNN Image Classifiers for Confusion & Bias Errors"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"V. Tolpegin; S. Truex; M. E. Gursoy; L. Liu"	"2020"	"Data Poisoning Attacks Against Federated Learning Systems"	""	"ESORICS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks Against Federated Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. J. Tomsett; A. Widdicombe; T. Xing; S. Chakraborty; S. J. Julier; P. K. Gurram; R. M. Rao; M. B. Srivastava"	"2018"	"Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning"	""	"2018 21st International Conference on Information Fusion (FUSION)"	""	""	""	""	""	"838-845"	""	""	""	""	""	""	""	"Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. M. Tonni; F. Farokhi; D. Vatsalan; D. Kaafar"	"2020"	"Data and Model Dependencies of Membership Inference Attack"	""	"ArXiv"	""	""	"abs/2002.06856"	""	""	""	""	""	""	""	""	""	""	"Data and Model Dependencies of Membership Inference Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Tooba; A. Akhunzada; T. Giannetsos; J. Malik"	"2020"	"Orchestrating SDN Control Plane towards Enhanced IoT Security"	""	"2020 6th IEEE Conference on Network Softwarization (NetSoft)"	""	""	""	""	""	"457-464"	""	""	""	""	""	""	""	"Orchestrating SDN Control Plane towards Enhanced IoT Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Toreini; M. Aitken; K. P. L. Coopamootoo; K. Elliott; V. González-Zelaya; P. Missier; M. Ng; A. v. Moorsel"	"2020"	"Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context"	""	"ArXiv"	""	""	"abs/2007.08911"	""	""	""	""	""	""	""	""	""	""	"Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Torkamani"	"2014"	"Adversarial Structured Output Prediction by"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Structured Output Prediction by"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Torkamani"	"2016"	"Robust Large Margin Approaches for Machine Learning in Adversarial Settings"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust Large Margin Approaches for Machine Learning in Adversarial Settings"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N.-T. Tran; V.-H. Tran; N.-B. Nguyen; T.-K. Nguyen; N.-M. Cheung"	"2020"	"Towards Good Practices for Data Augmentation in GAN Training"	""	"ArXiv"	""	""	"abs/2006.05338"	""	""	""	""	""	""	""	""	""	""	"Towards Good Practices for Data Augmentation in GAN Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Tretschk; S. J. Oh; M. Fritz"	"2018"	"Sequential Attacks on Agents for Long-Term Adversarial Goals"	""	"ArXiv"	""	""	"abs/1805.12487"	""	""	""	""	""	""	""	""	""	""	"Sequential Attacks on Agents for Long-Term Adversarial Goals"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. B. Truong; K. Sun; S. Wang; F. Guitton; Y. Guo"	"2020"	"Privacy Preservation in Federated Learning: Insights from the GDPR Perspective"	""	"ArXiv"	""	""	"abs/2011.05411"	""	""	""	""	""	""	""	""	""	""	"Privacy Preservation in Federated Learning: Insights from the GDPR Perspective"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W.-C. Tseng; W.-T. Kao; H.-y. Lee"	"2021"	"Membership Inference Attacks Against Self-supervised Speech Models"	""	"ArXiv"	""	""	"abs/2111.05113"	""	""	""	""	""	""	""	""	""	""	"Membership Inference Attacks Against Self-supervised Speech Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Tu; K. Zhu; N. C. Luong; D. T. Niyato; Y. Zhang; J. Li"	"2021"	"Incentive Mechanisms for Federated Learning: From Economic and Game Theoretic Perspective"	""	"ArXiv"	""	""	"abs/2111.11850"	""	""	""	""	""	""	""	""	""	""	"Incentive Mechanisms for Federated Learning: From Economic and Game Theoretic Perspective"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"U. Udhayakumar; G. Murugaboopathi"	"2020"	"International Journal of Recent Technology and Engineering (IJRTE)"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"International Journal of Recent Technology and Engineering (IJRTE)"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"O.-A. Ugot; C. O. Yinka-banjo; S. Misra"	"2021"	"Biometric Fingerprint Generation Using Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Biometric Fingerprint Generation Using Generative Adversarial Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Using"	"2019"	"GENERATING ROBUST AUDIO ADVERSARIAL EXAM"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"GENERATING ROBUST AUDIO ADVERSARIAL EXAM"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Vahid; S.-C. Lin; I.-H. Wang; Y.-C. Lai"	"2021"	"Content Delivery over Broadcast Erasure Channels with Distributed Random Cache"	""	"ArXiv"	""	""	"abs/2105.00323"	""	""	""	""	""	""	""	""	""	""	"Content Delivery over Broadcast Erasure Channels with Distributed Random Cache"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Vamosi; M. D. Platzer; T. Reutterer"	"2022"	"AI-based Re-identification of Behavioral Clickstream Data"	""	"ArXiv"	""	""	"abs/2201.10351"	""	""	""	""	""	""	""	""	""	""	"AI-based Re-identification of Behavioral Clickstream Data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Vasu; S. Seetharaman; S. Malaviya; M. Shukla; S. P. Lodha"	"2021"	"Gradient-based Data Subversion Attack Against Binary Classifiers"	""	"ArXiv"	""	""	"abs/2105.14803"	""	""	""	""	""	""	""	""	""	""	"Gradient-based Data Subversion Attack Against Binary Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. R. S. Vicarte; G. Wang; C. W. Fletcher"	"2021"	"Double-Cross Attacks: Subverting Active Learning Systems"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Double-Cross Attacks: Subverting Active Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"A. Viloria; N. A. L. Zelaya; N. Varela"	"2020"	"Unsupervised learning algorithms applied to grouping problems"	""	"FNC/MobiSPC"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Unsupervised learning algorithms applied to grouping problems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Vorobeychik; M. Kantarcioglu"	"2018"	"Adversarial Machine Learning"	""	"Synthesis Lectures on Artificial Intelligence and Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. E. Vorobieva; A. S. Petrenko; S. Petrenko"	"2021"	"Searching for the Strong AI for Cybersecurity"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Searching for the Strong AI for Cybersecurity"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"A. Wainakh; E. Zimmer; S. Subedi; J. Keim; T. Grube; S. Karuppayah; A. S. Guinea; M. Mühlhäuser"	"2021"	"Federated Learning Attacks Revisited: A Critical Discussion of Gaps, Assumptions, and Evaluation Setups"	""	"ArXiv"	""	""	"abs/2111.03363"	""	""	""	""	""	""	""	""	""	""	"Federated Learning Attacks Revisited: A Critical Discussion of Gaps, Assumptions, and Evaluation Setups"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Wang; J. Gao; Y. Qi"	"2016"	"A T HEORETICAL F RAMEWORK FOR R OBUSTNESS OF ( D EEP ) C LASSIFIERS U NDER A DVERSARIAL N OISE"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A T HEORETICAL F RAMEWORK FOR R OBUSTNESS OF ( D EEP ) C LASSIFIERS U NDER A DVERSARIAL N OISE"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"B. Wang; J. Gao; Y. Qi"	"2017"	"LASSIFIERS A GAINST A DVERSARIAL E XAMPLES"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"LASSIFIERS A GAINST A DVERSARIAL E XAMPLES"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Wang; J. Gao; Y. Qi"	"2017"	"A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples"	""	"arXiv: Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"C. Wang"	"2018"	"Generative modelling and adversarial learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generative modelling and adversarial learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Wang; J. Chen; Y. Yang; X. Ma; J. Liu"	"2021"	"Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects"	""	"Digital Communications and Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Wang; W.-d. Jin; Y. Wu; A. Khan"	"2021"	"Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN"	""	"ArXiv"	""	""	"abs/2103.04513"	""	""	""	""	""	""	""	""	""	""	"Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Wang; K. K. Sreenivasan; S. Rajput; H. Vishwakarma; S. Agarwal; J.-y. Sohn; K. Lee; D. Papailiopoulos"	"2020"	"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"	""	"ArXiv"	""	""	"abs/2007.05084"	""	""	""	""	""	""	""	""	""	""	"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Wang; X. Liu; J. Yi; Z.-H. Zhou; C.-J. Hsieh"	"2019"	"Evaluating the Robustness of Nearest Neighbor Classifiers: A Primal-Dual Perspective"	""	"ArXiv"	""	""	"abs/1906.03972"	""	""	""	""	""	""	""	""	""	""	"Evaluating the Robustness of Nearest Neighbor Classifiers: A Primal-Dual Perspective"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Wang; T. Chen; P. Yao; S. Liu; I. Rajapakse; A. O. Hero"	"2021"	"ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense"	""	"ArXiv"	""	""	"abs/2106.14300"	""	""	""	""	""	""	""	""	""	""	"ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Wang; S. Chen; T. Chen; S. Nepal; C. Rudolph; M. Grobler"	"2020"	"Generating Semantic Adversarial Examples via Feature Manipulation"	""	"ArXiv"	""	""	"abs/2001.02297"	""	""	""	""	""	""	""	""	""	""	"Generating Semantic Adversarial Examples via Feature Manipulation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Wang; J. Hayase; G. C. Fanti; S. Oh"	"2022"	"Towards a Defense against Backdoor Attacks in Continual Federated Learning"	""	"ArXiv"	""	""	"abs/2205.11736"	""	""	""	""	""	""	""	""	""	""	"Towards a Defense against Backdoor Attacks in Continual Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Wang; S. Nepal; C. Rudolph; M. Grobler; S. Chen; T. Chen"	"2020"	"Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models"	""	"ArXiv"	""	""	"abs/2001.03274"	""	""	""	""	""	""	""	""	""	""	"Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Wang; T. Wang; L. Wang; N. Luo; P. Zhou; D. X. Song; R. Jia"	"2021"	"DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing"	""	"Proceedings on Privacy Enhancing Technologies"	""	""	"2021"	""	""	"163 - 183"	""	""	""	""	""	""	""	"DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Wang; K. Chaudhuri"	"2018"	"Data Poisoning Attacks against Online Learning"	""	"ArXiv"	""	""	"abs/1808.08994"	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks against Online Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Wang; H.-z. Gao; Y.-m. Zhang; Y.-c. Hu; K. Qiu; X. Cheng; C. Jia"	"2017"	"Fortifying Botnet Classification based on Venn-abers Prediction"	""	"DEStech Transactions on Computer Science and Engineering"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Fortifying Botnet Classification based on Venn-abers Prediction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Wang; Q. Kang; X. Zhang; Q. Hu"	"2022"	"Defense Strategies Toward Model Poisoning Attacks in Federated Learning: A Survey"	""	"2022 IEEE Wireless Communications and Networking Conference (WCNC)"	""	""	""	""	""	"548-553"	""	""	""	""	""	""	""	"Defense Strategies Toward Model Poisoning Attacks in Federated Learning: A Survey"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Wang; J. Ma; X. Wang; J. Hu; Z. Qin; K. Ren"	"2022"	"Threats to Training: A Survey of Poisoning Attacks and Defenses on Machine Learning Systems"	""	"ACM Journal of the ACM (JACM)"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Threats to Training: A Survey of Poisoning Attacks and Defenses on Machine Learning Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Wang; M. Tian; C. Jia"	"2017"	"An Active and Dynamic Botnet Detection Approach to Track Hidden Concept Drift"	""	"ICICS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"An Active and Dynamic Botnet Detection Approach to Track Hidden Concept Drift"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Wang; M. Tian; J. Wang; C. Jia"	"2017"	"An Ensemble Learning System to Mitigate Malware Concept Drift Attacks (Short Paper)"	""	"ISPEC"	""	""	""	""	""	""	""	""	""	""	""	""	""	"An Ensemble Learning System to Mitigate Malware Concept Drift Attacks (Short Paper)"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Weerasinghe; T. Abraham; T. Alpcan; S. M. Erfani; C. Leckie; B. I. P. Rubinstein"	"2021"	"Closing the BIG-LID: An Effective Local Intrinsic Dimensionality Defense for Nonlinear Regression Poisoning"	""	"IJCAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Closing the BIG-LID: An Effective Local Intrinsic Dimensionality Defense for Nonlinear Regression Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Weerasinghe; S. M. Erfani; T. Alpcan; C. Leckie; J. Kopacz"	"2020"	"Defending Regression Learners Against Poisoning Attacks"	""	"ArXiv"	""	""	"abs/2008.09279"	""	""	""	""	""	""	""	""	""	""	"Defending Regression Learners Against Poisoning Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Wei; B. Xi; M. Kantarcioglu"	"2018"	"Adversarial Clustering: A Grid Based Clustering Algorithm Against Active Adversaries"	""	"ArXiv"	""	""	"abs/1804.04780"	""	""	""	""	""	""	""	""	""	""	"Adversarial Clustering: A Grid Based Clustering Algorithm Against Active Adversaries"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Wendland; C. Birkenbihl; M. Gomez-Freixa; M. Sood; M. Kschischo; H. Froehlich"	"2021"	"Generation of realistic synthetic data using multimodal neural ordinary differential equations"	""	"medRxiv"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Generation of realistic synthetic data using multimodal neural ordinary differential equations"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"E. Wenger; J. Passananti; Y. Yao; H. Zheng; B. Y. Zhao"	"2020"	"Backdoor Attacks on Facial Recognition in the Physical World"	""	"ArXiv"	""	""	"abs/2006.14580"	""	""	""	""	""	""	""	""	""	""	"Backdoor Attacks on Facial Recognition in the Physical World"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"T. Werner"	"2021"	"Quantitative robustness of instance ranking problems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Quantitative robustness of instance ranking problems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Willetts; A. Camuto; T. Rainforth; S. J. Roberts; C. C. Holmes"	"2021"	"Improving VAEs' Robustness to Adversarial Attack"	""	"arXiv: Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Improving VAEs' Robustness to Adversarial Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Willetts; A. Camuto; S. J. Roberts; C. C. Holmes"	"2019"	"Disentangling Improves VAEs' Robustness to Adversarial Attacks"	""	"ArXiv"	""	""	"abs/1906.00230"	""	""	""	""	""	""	""	""	""	""	"Disentangling Improves VAEs' Robustness to Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"R. K. Winder; M. Jones"	"2021"	"Odyssey: A Systems Approach to Machine Learning Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Odyssey: A Systems Approach to Machine Learning Security"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. R. Wiyatno; A. Xu; O. A. Dia; A. O. d. Berker"	"2019"	"Adversarial Examples in Modern Machine Learning: A Review"	""	"ArXiv"	""	""	"abs/1911.05268"	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples in Modern Machine Learning: A Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. J. Wong; I. WilliamH.Clark; B. Flowers; R. M. Buehrer; A. J. Michaels; W. C. Headley"	"2020"	"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep Learning to Radio Frequency Applications"	""	"arXiv: Signal Processing"	""	""	""	""	""	""	""	""	""	""	""	""	""	"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep Learning to Radio Frequency Applications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Wressnegger"	"2019"	"Efficient machine learning for attack detection"	""	"it - Information Technology"	""	""	"62"	""	""	"279 - 286"	""	""	""	""	""	""	""	"Efficient machine learning for attack detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Wu; W. Fang; Y. Zhang; L. Yang; X. Xu; H. Luo; X. Yu"	"2022"	"Adversarial Attacks and Defenses in Physiological Computing: A Systematic Review"	""	"ArXiv"	""	""	"abs/2102.02729"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks and Defenses in Physiological Computing: A Systematic Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Wu; M. M. Nekovee; Y. Wang"	"2019"	"An Adaptive Deep Learning Algorithm Based Autoencoder for Interference Channels"	""	"MLN"	""	""	""	""	""	""	""	""	""	""	""	""	""	"An Adaptive Deep Learning Algorithm Based Autoencoder for Interference Channels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Wu"	"2020"	"PLFG: A Privacy Attack Method Based on Gradients for Federated Learning"	""	"SPDE"	""	""	""	""	""	""	""	""	""	""	""	""	""	"PLFG: A Privacy Attack Method Based on Gradients for Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Wu; Z. Hu; B. Gu"	"2021"	"Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. Wu; N. Saxena; R. Jain"	"2021"	"Poisoning the Search Space in Neural Architecture Search"	""	"ArXiv"	""	""	"abs/2106.14406"	""	""	""	""	""	""	""	""	""	""	"Poisoning the Search Space in Neural Architecture Search"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Xi; R. Pang; S. Ji; T. Wang"	"2021"	"Graph Backdoor"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Graph Backdoor"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Xiao"	"2017"	"Adversarial and Secure Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial and Secure Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"H. Xiao; B. Biggio; G. Brown; G. Fumera; C. Eckert; F. Roli"	"2015"	"Is Feature Selection Secure against Training Data Poisoning?"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Is Feature Selection Secure against Training Data Poisoning?"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Xu; R. Singh; H. Bilen; M. Fiore; M. K. Marina; Y. Wang"	"2022"	"CartaGenie: Context-Driven Synthesis of City-Scale Mobile Network Traffic Snapshots"	""	"2022 IEEE International Conference on Pervasive Computing and Communications (PerCom)"	""	""	""	""	""	"119-129"	""	""	""	""	""	""	""	"CartaGenie: Context-Driven Synthesis of City-Scale Mobile Network Traffic Snapshots"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Xu; R. Singh; M. Fiore; M. K. Marina; H. Bilen; M. Usama; H. Benn; C. Ziemlicki"	"2021"	"SpectraGAN: spectrum based generation of city scale spatiotemporal mobile network traffic data"	""	"Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies"	""	""	""	""	""	""	""	""	""	""	""	""	""	"SpectraGAN: spectrum based generation of city scale spatiotemporal mobile network traffic data"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Xu; B. Wang; R. Ran; W. Wen; P. Venkitasubramaniam"	"2022"	"NeuGuard: Lightweight Neuron-Guided Defense against Membership Inference Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"NeuGuard: Lightweight Neuron-Guided Defense against Membership Inference Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Xu"	"2020"	"Non-Orthogonal Waveforms in Secure Communications"	""	"ArXiv"	""	""	"abs/2004.10228"	""	""	""	""	""	""	""	""	""	""	"Non-Orthogonal Waveforms in Secure Communications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Xu"	"2020"	"Waveform-Defined Security: A Framework for Secure Communications"	""	"2020 12th International Symposium on Communication Systems, Networks and Digital Signal Processing (CSNDSP)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Waveform-Defined Security: A Framework for Secure Communications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"T. Xu"	"2021"	"Waveform-Defined Security: A Low-Cost Framework for Secure Communications"	""	"ArXiv"	""	""	"abs/2112.11350"	""	""	""	""	""	""	""	""	""	""	"Waveform-Defined Security: A Low-Cost Framework for Secure Communications"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Xu; J. Zhang; F. Liu; M. Sugiyama; M. S. Kankanhalli"	"2022"	"Adversarial Attacks and Defense for Non-Parametric Two-Sample Tests"	""	"ArXiv"	""	""	"abs/2202.03077"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks and Defense for Non-Parametric Two-Sample Tests"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Xu; B. Kumar; J. D. Abernethy"	"2021"	"Observation-Free Attacks on Stochastic Bandits"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Observation-Free Attacks on Stochastic Bandits"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"C.-H. H. Yang; S. M. Siniscalchi; C.-H. Lee"	"2021"	"PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification"	""	"Interspeech"	""	""	""	""	""	""	""	""	""	""	""	""	""	"PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"F. Yang; S. Ren"	"2020"	"Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based Classifiers"	""	"ArXiv"	""	""	"abs/2006.05594"	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Yang; J. Chen; C.-J. Hsieh; J.-l. Wang; M. I. Jordan"	"2020"	"ML-LOO: Detecting Adversarial Examples with Feature Attribution"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"ML-LOO: Detecting Adversarial Examples with Feature Attribution"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Yang; C. Deng; K.-J. Wei; J. Yan; W. Liu"	"2020"	"Adversarial Learning for Robust Deep Clustering"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Learning for Robust Deep Clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Yang; R. Gao; Y. Li; Q. Lai; Q. Xu"	"2021"	"MixDefense: A Defense-in-Depth Framework for Adversarial Example Detection Based on Statistical and Semantic Analysis"	""	"ArXiv"	""	""	"abs/2104.10076"	""	""	""	""	""	""	""	""	""	""	"MixDefense: A Defense-in-Depth Framework for Adversarial Example Detection Based on Statistical and Semantic Analysis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Yang; R. Gao; Y. Li; Q. Lai; Q. Xu"	"2022"	"What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction"	""	"ArXiv"	""	""	"abs/2201.09650"	""	""	""	""	""	""	""	""	""	""	"What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y. Yang; L. Li; L. Chang; T. Gu"	"2020"	"A Poisoning Attack Against the Recognition Model Trained by the Data Augmentation Method"	""	"ML4CS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Poisoning Attack Against the Recognition Model Trained by the Data Augmentation Method"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y.-Y. Yang; C. Rashtchian; Y. Wang; K. Chaudhuri"	"2019"	"Adversarial Examples for Non-Parametric Methods: Attacks, Defenses and Large Sample Limits"	""	"ArXiv"	""	""	"abs/1906.03310"	""	""	""	""	""	""	""	""	""	""	"Adversarial Examples for Non-Parametric Methods: Attacks, Defenses and Large Sample Limits"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Y.-Y. Yang; C. Rashtchian; Y. Wang; K. Chaudhuri"	"2020"	"Robustness for Non-Parametric Classification: A Generic Attack and Defense"	""	"AISTATS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robustness for Non-Parametric Classification: A Generic Attack and Defense"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"F. Yao"	"2020"	"DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Ye; Z. Zhu"	"2018"	"Bayesian Adversarial Learning"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Bayesian Adversarial Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Yi; R. Mudumbai; W. Xu"	"2020"	"Derivation of Information-Theoretically Optimal Adversarial Attacks with Applications to Robust Machine Learning"	""	"ArXiv"	""	""	"abs/2007.14042"	""	""	""	""	""	""	""	""	""	""	"Derivation of Information-Theoretically Optimal Adversarial Attacks with Applications to Robust Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Yin"	"2019"	"Towards More Scalable and Robust Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Towards More Scalable and Robust Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"D. Yin; K. Ramchandran; P. L. Bartlett"	"2019"	"Rademacher Complexity for Adversarially Robust Generalization"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Rademacher Complexity for Adversarially Robust Generalization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Yin; S. Li; G. K. Rohde"	"2020"	"Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection"	""	"ArXiv"	""	""	"abs/2012.06568"	""	""	""	""	""	""	""	""	""	""	"Analyzing and Improving Generative Adversarial Training for Generative Modeling and Out-of-Distribution Detection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Q. Ying; X. Hu; X. F. Zhang; Z. Qian; X. Zhang"	"2021"	"RWN: Robust Watermarking Network for Image Cropping Localization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"RWN: Robust Watermarking Network for Image Cropping Localization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Q. Ying; X. Hu; H. Zhou; X. Zhang; Z. You; Z. Qian"	"2021"	"No way to crop: On robust image crop localization"	""	"ArXiv"	""	""	"abs/2110.05687"	""	""	""	""	""	""	""	""	""	""	"No way to crop: On robust image crop localization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Q. Ying; H. Zhou; Z. Qian; S. Li; X. Zhang"	"2022"	"Robust Image Protection Countering Cropping Manipulation"	""	"ArXiv"	""	""	"abs/2206.02405"	""	""	""	""	""	""	""	""	""	""	"Robust Image Protection Countering Cropping Manipulation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Q. Ying; H. Zhou; X. Zeng; H. Xu; Z. Qian; X. Zhang"	"2021"	"Hiding Images into Images with Real-world Robustness"	""	"ArXiv"	""	""	"abs/2110.05689"	""	""	""	""	""	""	""	""	""	""	"Hiding Images into Images with Real-world Robustness"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. O. Yinka-banjo; O.-A. Ugot"	"2019"	"A review of generative adversarial networks and its application in cybersecurity"	""	"Artificial Intelligence Review"	""	""	"53"	""	""	"1721-1736"	""	""	""	""	""	""	""	"A review of generative adversarial networks and its application in cybersecurity"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. H. Yoo; H. Jeong; J. Lee; T. M. Chung"	"2021"	"Federated Learning: Issues in Medical Application"	""	"ArXiv"	""	""	"abs/2109.00202"	""	""	""	""	""	""	""	""	""	""	"Federated Learning: Issues in Medical Application"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Yu; W. Mao; Y. Lv; C. Zhang; Y. Xie"	"2022"	"A survey on federated learning in data mining"	""	"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"	""	""	"12"	""	""	""	""	""	""	""	""	""	""	"A survey on federated learning in data mining"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"N. Yu; M. Fritz"	"2022"	"R ELAX L OSS : D EFENDING M EMBERSHIP I NFERENCE A TTACKS WITHOUT L OSING U TILITY"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"R ELAX L OSS : D EFENDING M EMBERSHIP I NFERENCE A TTACKS WITHOUT L OSING U TILITY"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Yu; H. Chen; H. Yu; Z. Zhang; X. Liang; W. Qin; Y. Xie; P. Shi"	"2020"	"Elastic Net based Feature Ranking and Selection"	""	"ArXiv"	""	""	"abs/2012.14982"	""	""	""	""	""	""	""	""	""	""	"Elastic Net based Feature Ranking and Selection"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Yu; W. Yang; Y.-P. Tan; A. C. Kot"	"2022"	"Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond"	""	"ArXiv"	""	""	"abs/2203.16931"	""	""	""	""	""	""	""	""	""	""	"Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"C.-H. Yuan; S.-H. Wu"	"2021"	"Neural Tangent Generalization Attacks"	""	"ICML"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Neural Tangent Generalization Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Yuan; L. Zhang"	"2022"	"Membership Inference Attacks and Defenses in Neural Network Pruning"	""	"ArXiv"	""	""	"abs/2202.03335"	""	""	""	""	""	""	""	""	""	""	"Membership Inference Attacks and Defenses in Neural Network Pruning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"I. Zakariyya; H. K. Kalutarage; M. O. Al-Kadri"	"2022"	"Robust, Effective and Resource Efficient Deep Neural Network for Intrusion Detection in IoT Networks"	""	"Proceedings of the 8th ACM on Cyber-Physical System Security Workshop"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Robust, Effective and Resource Efficient Deep Neural Network for Intrusion Detection in IoT Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Zamanipour"	"2019"	"A Survey on Deep-Learning based Techniques for Modeling and Estimation of MassiveMIMO Channels"	""	"ArXiv"	""	""	"abs/1910.03390"	""	""	""	""	""	""	""	""	""	""	"A Survey on Deep-Learning based Techniques for Modeling and Estimation of MassiveMIMO Channels"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"S. Zawad; A. Ali; P.-Y. Chen; A. Anwar; Y. Zhou; N. Baracaldo; Y. Tian; F. Yan"	"2021"	"Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zhang; J. Wang; H. Wang; X. Y. Luo"	"2022"	"Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks"	""	"ArXiv"	""	""	"abs/2204.12050"	""	""	""	""	""	""	""	""	""	""	"Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Zhang; D. Wu; C. Liu; B. Chen"	"2020"	"Defending Poisoning Attacks in Federated Learning via Adversarial Training Method"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Defending Poisoning Attacks in Federated Learning via Adversarial Training Method"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"J. Zhang; B. Zhang; B. Zhang"	"2019"	"Defending Adversarial Attacks on Cloud-aided Automatic Speech Recognition Systems"	""	"SCC '19"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Defending Adversarial Attacks on Cloud-aided Automatic Speech Recognition Systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"L. Zhang; K. K. Thekumparampil; S. Oh; N. He"	"2022"	"Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization"	""	"ArXiv"	""	""	"abs/2206.00363"	""	""	""	""	""	""	""	""	""	""	"Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Zhang; K. Huang; Z. Xu"	"2019"	"On Model Robustness Against Adversarial Examples"	""	"ArXiv"	""	""	"abs/1911.06479"	""	""	""	""	""	""	""	""	""	""	"On Model Robustness Against Adversarial Examples"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"S. Zhang; J. Li"	"2021"	"KNN Classification with One-step Computation"	""	"ArXiv"	""	""	"abs/2012.06047"	""	""	""	""	""	""	""	""	""	""	"KNN Classification with One-step Computation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Zhang; N. Wang; S. Ji; H. Shen; T. Wang"	"2020"	"Interpretable Deep Learning under Fire"	""	"USENIX Security Symposium"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Interpretable Deep Learning under Fire"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Zhang; M. Zitnik"	"2020"	"GNNGuard: Defending Graph Neural Networks against Adversarial Attacks"	""	"ArXiv"	""	""	"abs/2006.08149"	""	""	""	""	""	""	""	""	""	""	"GNNGuard: Defending Graph Neural Networks against Adversarial Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Zhang; A. Albarghouthi; L. D'antoni"	"2022"	"BagFlip: A Certified Defense against Data Poisoning"	""	"ArXiv"	""	""	"abs/2205.13634"	""	""	""	""	""	""	""	""	""	""	"BagFlip: A Certified Defense against Data Poisoning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Zhang; A. Schlüter; C. Waibel"	"2022"	"SolarGAN: Synthetic Annual Solar Irradiance Time Series on Urban Building Facades via Deep Generative Networks"	""	"ArXiv"	""	""	"abs/2206.00747"	""	""	""	""	""	""	""	""	""	""	"SolarGAN: Synthetic Annual Solar Irradiance Time Series on Urban Building Facades via Deep Generative Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Zhang; Z. Zhang; Y. Zhou; Y. Shen; R. Jin; D. Dou"	"2020"	"To appear in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020 (NeurIPS’20). Adversarial Attacks on Deep Graph Matching"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"To appear in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020 (NeurIPS’20). Adversarial Attacks on Deep Graph Matching"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"Z. Zhang; Z. Zhang; Y. Zhou; Y. Shen; R. Jin; D. Dou"	"2020"	"Adversarial Attacks on Deep Graph Matching"	""	"NeurIPS"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Adversarial Attacks on Deep Graph Matching"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Zhao; Y. Lao"	"2020"	"Class-Oriented Poisoning Attack"	""	"ArXiv"	""	""	"abs/2008.00047"	""	""	""	""	""	""	""	""	""	""	"Class-Oriented Poisoning Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"B. Zhao; Y. Lao"	"2022"	"Towards Class-Oriented Poisoning Attacks Against Neural Networks"	""	"2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"	""	""	""	""	""	"2244-2253"	""	""	""	""	""	""	""	"Towards Class-Oriented Poisoning Attacks Against Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Zhao; T. Le; P. Montague; O. Y. d. Vel; T. Abraham; D. Q. Phung"	"2019"	"Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions"	""	"ArXiv"	""	""	"abs/1910.01329"	""	""	""	""	""	""	""	""	""	""	"Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"H. Zhao; T. Le; P. Montague; O. Y. d. Vel; T. Abraham; D. Q. Phung"	"2020"	"Towards Understanding Pixel Vulnerability under Adversarial Attacks for Images"	""	"ArXiv"	""	""	"abs/2010.06131"	""	""	""	""	""	""	""	""	""	""	"Towards Understanding Pixel Vulnerability under Adversarial Attacks for Images"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Zhao"	"2018"	"Advanced attack and defense techniques in machine learning systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Advanced attack and defense techniques in machine learning systems"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Zhao; B. An; W. Gao; T. Zhang"	"2017"	"Efficient Label Contamination Attacks Against Black-Box Learning Models"	""	"IJCAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Efficient Label Contamination Attacks Against Black-Box Learning Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"M. Zhao; B. An; Y. Yu; S. Liu; S. J. Pan"	"2018"	"Data Poisoning Attacks on Multi-Task Relationship Learning"	""	"AAAI"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks on Multi-Task Relationship Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"P. Zhao; S. Wang; C. Gongye; Y. Wang; Y. Fei; X. Lin"	"2019"	"Fault Sneaking A ack : a Stealthy Framework for Misleading Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"Fault Sneaking A ack : a Stealthy Framework for Misleading Deep Neural Networks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"W. Zhao; Y. Lao; P. Li"	"2022"	"Integrity Authentication in Tree Models"	""	"ArXiv"	""	""	"abs/2205.15444"	""	""	""	""	""	""	""	""	""	""	"Integrity Authentication in Tree Models"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Zhao; M. C. Stamm"	"2020"	"Defenses Against Multi-sticker Physical Domain Attacks on Classifiers"	""	"ECCV Workshops"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Defenses Against Multi-sticker Physical Domain Attacks on Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Zhao; H. Zhu; R. Liang; Q. Shen; S. Zhang; K. Chen"	"2018"	"Seeing isn't Believing: Practical Adversarial Attack Against Object Detectors"	""	"arXiv: Computer Vision and Pattern Recognition"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Seeing isn't Believing: Practical Adversarial Attack Against Object Detectors"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Zhao; H. Zhu; Q. Shen; R. Liang; K. Chen; S. Zhang"	"2018"	"Practical Adversarial Attack Against Object Detector"	""	"ArXiv"	""	""	"abs/1812.10217"	""	""	""	""	""	""	""	""	""	""	"Practical Adversarial Attack Against Object Detector"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"Z. Zhao; A. Kunar; R. Birke; L. Y. Chen"	"2022"	"CTAB-GAN+: Enhancing Tabular Data Synthesis"	""	"ArXiv"	""	""	"abs/2204.00401"	""	""	""	""	""	""	""	""	""	""	"CTAB-GAN+: Enhancing Tabular Data Synthesis"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zheng; F. Yang; H. Shen; X. Tang; M. Chen; L. Song; X. Wei"	"2022"	"Learning from Attacks: Attacking Variational Autoencoder for Improving Image Classification"	""	"ArXiv"	""	""	"abs/2203.07027"	""	""	""	""	""	""	""	""	""	""	"Learning from Attacks: Attacking Variational Autoencoder for Improving Image Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Zheng; B. Wang; D. M. Kalathil; L. Xie"	"2021"	"Generative Adversarial Networks-Based Synthetic PMU Data Creation for Improved Event Classification"	""	"IEEE Open Access Journal of Power and Energy"	""	""	"8"	""	""	"68-76"	""	""	""	""	""	""	""	"Generative Adversarial Networks-Based Synthetic PMU Data Creation for Improved Event Classification"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Conference Proceedings"	"X. Zheng; N. Xu; L. Trinh; D. Wu; T. Huang; S. Sivaranjani; Y. Liu; L. Xie"	"2021"	"A Multi-scale Time-series Dataset with Benchmark for Machine Learning in Decarbonized Energy Grids"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Multi-scale Time-series Dataset with Benchmark for Machine Learning in Decarbonized Energy Grids"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"X. Zheng; N. Xu; L. Trinh; D. Wu; T. Huang; S. Sivaranjani; Y. Liu; L. Xie"	"2021"	"PSML: A Multi-scale Time-series Dataset for Machine Learning in Decarbonized Energy Grids"	""	"ArXiv"	""	""	"abs/2110.06324"	""	""	""	""	""	""	""	""	""	""	"PSML: A Multi-scale Time-series Dataset for Machine Learning in Decarbonized Energy Grids"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zhong; X. Liu; C.-J. Hsieh"	"2020"	"Improving the Speed and Quality of GAN by Adversarial Training"	""	"ArXiv"	""	""	"abs/2008.03364"	""	""	""	""	""	""	""	""	""	""	"Improving the Speed and Quality of GAN by Adversarial Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Zhou; Y. Gao; A. Fu; K. Chen; Z. Dai; Z. Zhang; M. Xue; Y. Zhang"	"2022"	"PPA: Preference Profiling Attack Against Federated Learning"	""	"ArXiv"	""	""	"abs/2202.04856"	""	""	""	""	""	""	""	""	""	""	"PPA: Preference Profiling Attack Against Federated Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Zhou; N. Wang; X. Gao; B. Han; J. Yu; X. Wang; T. Liu"	"2021"	"Improving White-box Robustness of Pre-processing Defenses via Joint Adversarial Training"	""	"ArXiv"	""	""	"abs/2106.05453"	""	""	""	""	""	""	""	""	""	""	"Improving White-box Robustness of Pre-processing Defenses via Joint Adversarial Training"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zhou; Y. Chen; C. Shen; Y. Zhang"	"2022"	"Property Inference Attacks Against GANs"	""	"ArXiv"	""	""	"abs/2111.07608"	""	""	""	""	""	""	""	""	""	""	"Property Inference Attacks Against GANs"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Zhu; H. Wang; Y. Zhuang; J. Li; Y. Cao"	"2022"	"A Sparsity-Limitation-Based High-Dimensional Distribution Searching Algorithm for Adversarial Attack"	""	"Journal of Sensors"	""	""	""	""	""	""	""	""	""	""	""	""	""	"A Sparsity-Limitation-Based High-Dimensional Distribution Searching Algorithm for Adversarial Attack"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Zhu; D. P. Guralnik; X. Wang; X. Li; B. Moran"	"2015"	"Statistical Properties of the Single Linkage Hierarchical Clustering Estimator"	""	"arXiv: Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Statistical Properties of the Single Linkage Hierarchical Clustering Estimator"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Zhu; D. P. Guralnik; X. Wang; X. Li; B. Moran"	"2015"	"Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering"	""	"arXiv: Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zhu; R. Hou; D. Meng"	"2022"	"TACC"	""	"Proceedings of the 15th ACM International Conference on Systems and Storage"	""	""	""	""	""	""	""	""	""	""	""	""	""	"TACC"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"J. Zhu; R. Hou; D. Meng"	"2022"	"TACC: a secure accelerator enclave for AI workloads"	""	"Proceedings of the 15th ACM International Conference on Systems and Storage"	""	""	""	""	""	""	""	""	""	""	""	""	""	"TACC: a secure accelerator enclave for AI workloads"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"R. S. Zimmermann; L. Schott; Y. Song; B. A. Dunn; D. A. Klindt"	"2021"	"Score-Based Generative Classifiers"	""	"ArXiv"	""	""	"abs/2110.00473"	""	""	""	""	""	""	""	""	""	""	"Score-Based Generative Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"C. Qi; J. Gao; K. Chen; L. Shu; S. Pearson"	"2022"	"Tea Chrysanthemum Detection by Leveraging Generative Adversarial Networks and Edge Computing"	""	"Front Plant Sci"	""	""	"13"	""	""	"850606"	""	""	""	"2022/04/26"	""	""	""	"Tea Chrysanthemum Detection by Leveraging Generative Adversarial Networks and Edge Computing"	""	"1664-462X (Print)
1664-462X (Linking)"	"10.3389/fpls.2022.850606"	""	""	""	""	"PMC9021924"	""	""	""	""	""	""	"35463441"	""	""	"NVIDIA Jetson TX2
deep learning
edge computing
generative adversarial network
tea chrysanthemum
commercial or financial relationships that could be construed as a potential
conflict of interest."	"A high resolution dataset is one of the prerequisites for tea chrysanthemum detection with deep learning algorithms. This is crucial for further developing a selective chrysanthemum harvesting robot. However, generating high resolution datasets of the tea chrysanthemum with complex unstructured environments is a challenge. In this context, we propose a novel tea chrysanthemum - generative adversarial network (TC-GAN) that attempts to deal with this challenge. First, we designed a non-linear mapping network for untangling the features of the underlying code. Then, a customized regularization method was used to provide fine-grained control over the image details. Finally, a gradient diversion design with multi-scale feature extraction capability was adopted to optimize the training process. The proposed TC-GAN was compared with 12 state-of-the-art generative adversarial networks, showing that an optimal average precision (AP) of 90.09% was achieved with the generated images (512 x 512) on the developed TC-YOLO object detection model under the NVIDIA Tesla P100 GPU environment. Moreover, the detection model was deployed into the embedded NVIDIA Jetson TX2 platform with 0.1 s inference time, and this edge computing device could be further developed into a perception system for selective chrysanthemum picking robots in the future."	"Qi, Chao
Gao, Junfeng
Chen, Kunjie
Shu, Lei
Pearson, Simon
eng
Switzerland
Front Plant Sci. 2022 Apr 7;13:850606. doi: 10.3389/fpls.2022.850606. eCollection 2022."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35463441"	""	"College of Engineering, Nanjing Agricultural University, Nanjing, China.
Lincoln Agri-Robotics Centre, Lincoln Institute for Agri-Food Technology, University of Lincoln, Lincoln, United Kingdom."	""	""	""	""	""	""	""	""
"Journal Article"	"M. H. Fang; M. H. Sun; Q. Li; N. Z. Q. Gong; J. Tian; J. Liu"	"2021"	"Data Poisoning Attacks and Defenses to Crowdsourcing Systems"	""	"Proceedings of the World Wide Web Conference 2021 (Www 2021)"	""	""	""	""	""	"969-980"	""	""	""	""	""	""	""	"Data Poisoning Attacks and Defenses to Crowdsourcing Systems"	""	""	"10.1145/3442381.3450066"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000733621800086"	""	""	"data poisoning attacks
crowdsourcing
truth discovery
truth discovery"	"A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain underexplored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks."	"Bs5mc
Times Cited:0
Cited References Count:46"	""	"<Go to ISI>://WOS:000733621800086"	""	"Iowa State Univ, Ames, IA 50011 USA
Ohio State Univ, Columbus, OH 43210 USA
Duke Univ, Durham, NC 27706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Bergadano; F. Carretto; F. Cogno; D. Ragno"	"2019"	"Defacement Detection with Passive Adversaries"	""	"Algorithms"	""	""	"12"	""	"8"	"150"	""	""	""	""	"Aug"	""	""	"Defacement Detection with Passive Adversaries"	"Algorithms"	""	"ARTN 150
10.3390/a12080150"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000482943300013"	""	""	"adversarial learning
anomaly detection
defacement response
security incident and event management
security operations center"	"A novel approach to defacement detection is proposed in this paper, addressing explicitly the possible presence of a passive adversary. Defacement detection is an important security measure for Web Sites and Applications, aimed at avoiding unwanted modifications that would result in significant reputational damage. As in many other anomaly detection contexts, the algorithm used to identify possible defacements is obtained via an Adversarial Machine Learning process. We consider an exploratory setting, where the adversary can observe the detector's alarm-generating behaviour, with the purpose of devising and injecting defacements that will pass undetected. It is then necessary to make to learning process unpredictable, so that the adversary will be unable to replicate it and predict the classifier's behaviour. We achieve this goal by introducing a secret key-a key that our adversary does not know. The key will influence the learning process in a number of different ways, that are precisely defined in this paper. This includes the subset of examples and features that are actually used, the time of learning and testing, as well as the learning algorithm's hyper-parameters. This learning methodology is successfully applied in this context, by using the system with both real and artificially modified Web sites. A year-long experimentation is also described, referred to the monitoring of the new Web Site of a major manufacturing company."	"It5xx
Times Cited:3
Cited References Count:44"	""	"<Go to ISI>://WOS:000482943300013"	""	"Univ Torino, Dipartimento Informat, Corso Svizzera 185, I-10149 Turin, Italy
Certimeter Grp, Corso Svizzera 185, I-10149 Turin, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Liu; G. Ditzler"	"2021"	"Data poisoning against information-theoretic feature selection"	""	"Information Sciences"	""	""	"573"	""	""	"396-411"	""	""	""	""	"Sep"	""	""	"Data poisoning against information-theoretic feature selection"	"Inform Sciences"	"0020-0255"	"10.1016/j.ins.2021.05.049"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000687216400005"	""	""	"feature selection
information theory
adversarial learning
mutual information
classifiers"	"A typical assumption made in machine learning is that a learning model does not consider an adversary's existence that can subvert a classifier's objective. As a result, machine learn-ing pipelines exhibit vulnerabilities in an adversarial environment. Feature Selection (FS) is an essential preprocessing stage in data analytics and has been widely used in security-sensitive machine learning applications; however, FS research in adversarial machine learning has been largely overlooked. Recently, empirical works demonstrated that the FS is also vulnerable in an adversarial environment. In the past decade, although the research community has made extensive efforts to promote the classifiers' robustness and develop countermeasures against adversaries, only a few contributions investigated FS's behavior in a malicious environment. Given that machine learning pipelines increas-ingly rely on FS to combat the "curse of dimensionality" and overfitting, insecure FS can be the "Achilles heel" of data pipelines. In this contribution, we explore the weaknesses of information-theoretic FS methods by designing a generic FS poisoning algorithm. We also show the transferability of the proposed poisoning method across seven information-theoretic FS methods. The experiments on 16 benchmark datasets demon -strate the efficacy of our proposed poisoning algorithm and the existence of transferability. (c) 2021 Elsevier Inc. All rights reserved."	"Ud4zp
Times Cited:1
Cited References Count:45"	""	"<Go to ISI>://WOS:000687216400005"	""	"Univ Arizona, Dept Elect & Comp Engn, 1230 E Speedway Blvd, Tucson, AZ 85721 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Pierazzi; G. Mezzour; Q. Han; M. Colajanni; V. S. Subrahmanian"	"2020"	"A Data-driven Characterization of Modern Android Spyware"	""	"Acm Transactions on Management Information Systems"	""	""	"11"	""	"1"	"1 - 38"	""	""	""	""	"Apr"	""	""	"A Data-driven Characterization of Modern Android Spyware"	"Acm Trans Manag Inf"	"2158-656x"	"Artn 4
10.1145/3382158"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000571472300003"	""	""	"machine learning
malware
android
spyware
characterization
threat"	"According to Nokia's 2017 Threat Intelligence Report, 68.5% of malware targets the Android platform; Windows is second with 28%, followed by iOS and other platforms with 3.5%. The Android spyware family UAPUSH was responsible for the most infections, and several of the top 20 most common Android malware were spyware. Simply put, modern spyware steals the basic information needed to fuel more deadly attacks such as ransomware and banking fraud. Not surprisingly, some forms of spyware are also classified as banking trojans (e.g., ACECARD). We present a data-driven characterization of the principal factors that distinguish modern Android spyware ( July 2016-July 2017) both from goodware and other Android malware, using both traditional and deep ML. First, we propose an Ensemble Late Fusion (ELF) architecture that combines the results of multiple classifiers' predicted probabilities to generate a final prediction. We show that ELF outperforms several of the best-known traditional and deep learning classifiers. Second, we automatically identify key features that distinguish spyware both from goodware and from other malware. Finally we present a detailed analysis of the factors distinguishing five important families of Android spyware: UAPUSH, Pincer, HEHE, USBCLEAVER, and ACECARD (the last is a hybrid spyware-banking trojan)."	"Nr3om
Times Cited:4
Cited References Count:78"	""	"<Go to ISI>://WOS:000571472300003"	""	"Kings Coll London, Dept Informat, London, England
Int Univ Rabat, FIL, TICLab, Rabat, Morocco
Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA
Univ Modena & Reggio Emilia, Reggio Emilia, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Billah; A. Anwar; Z. Rahman; S. M. Galib"	"2021"	"Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes"	""	"Energies"	""	""	"14"	""	"13"	""	""	""	""	""	"Jul"	""	""	"Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes"	"Energies"	""	"ARTN 3887
10.3390/en14133887"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000671998700001"	""	""	"poisoning attack
prediction model
home appliances
energy usage
regression
electricity consumption
energy-consumption
determinants
demand"	"Accurate building energy prediction is useful in various applications starting from building energy automation and management to optimal storage control. However, vulnerabilities should be considered when designing building energy prediction models, as intelligent attackers can deliberately influence the model performance using sophisticated attack models. These may consequently degrade the prediction accuracy, which may affect the efficiency and performance of the building energy management systems. In this paper, we investigate the impact of bi-level poisoning attacks on regression models of energy usage obtained from household appliances. Furthermore, an effective countermeasure against the poisoning attacks on the prediction model is proposed in this paper. Attacks and defenses are evaluated on a benchmark dataset. Experimental results show that an intelligent cyber-attacker can poison the prediction model to manipulate the decision. However, our proposed solution successfully ensures defense against such poisoning attacks effectively compared to other benchmark techniques."	"Th3na
Times Cited:0
Cited References Count:37"	""	"<Go to ISI>://WOS:000671998700001"	""	"Jashore Univ Sci & Technol JUST, Dept CSE, Jashore 7400, Bangladesh
Deakin Univ, Ctr Cyber Secur Res & Innovat, Geelong, Vic 3217, Australia
RMIT Univ, Sch Comp Technol, Melbourne, Vic 3001, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. Gur"	"2020"	"Expansive Networks: Exploiting Spectrum Sharing for Capacity Boost and 6G Vision"	""	"Journal of Communications and Networks"	""	""	"22"	""	"6"	"444-454"	""	""	""	""	"Dec"	""	""	"Expansive Networks: Exploiting Spectrum Sharing for Capacity Boost and 6G Vision"	"J Commun Netw-S Kor"	"1229-2370"	"10.23919/Jcn.2020.000037"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000611082700002"	""	""	"6g/beyond 5g networks
dlt
expansive networks
spectrum sharing
network intelligence
unlicensed spectrum
wireless networks
5g
challenges
opportunities
technologies
architecture
coexistence"	"Adaptive capacity with cost-efficient resource provisioning is a crucial capability for future 6G networks. In this work, we conceptualize "expansive networks" which refers to a networking paradigm where networks should be able to extend their resource base by opportunistic but self-controlled expansive actions. To this end, we elaborate on a key aspect of an expansive network as a concrete example: Spectrum resource at the PHY layer. Evidently, future wireless networks need to provide efficient mechanisms to coexist in the licensed and unlicensed bands and operate in expansive mode. In this work, we first describe spectrum sharing issues and possibilities in 6G networks for expansive networks. We then present security implications of expansive networks, an important concern due to more open and coupled systems in expansive networks. We also discuss two key enablers, namely distributed ledger technology (DLT) and network intelligence via machine learning, which are promising to realize expansive networks for the spectrum sharing aspect."	"Px0vj
Times Cited:4
Cited References Count:64"	""	"<Go to ISI>://WOS:000611082700002"	""	"Zurich Univ Appl Sci ZHAW, Inst Appl Informat Technol InIT, Winterthur, Switzerland"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Papernot; P. McDaniel; A. Sinha; M. P. Wellman"	"2018"	"SoK: Security and Privacy in Machine Learning"	""	"2018 3rd Ieee European Symposium on Security and Privacy (Euros&P 2018)"	""	""	""	""	""	"399-414"	""	""	""	""	""	""	""	"SoK: Security and Privacy in Machine Learning"	""	""	"10.1109/EuroSP.2018.00035"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000568604800027"	""	""	""	"Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive-new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, 'a la PAC theory, will foster a science of security and privacy in ML."	"Bp8zs
Times Cited:101
Cited References Count:113"	""	"<Go to ISI>://WOS:000568604800027"	""	"Penn State Univ, University Pk, PA 16802 USA
Univ Michigan, Ann Arbor, MI 48109 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Wiedeman; G. Wang"	"2022"	"Disrupting adversarial transferability in deep neural networks"	""	"Patterns (N Y)"	""	""	"3"	""	"5"	"100472"	""	""	""	"2022/05/25"	"May 13"	""	""	"Disrupting adversarial transferability in deep neural networks"	""	"2666-3899 (Electronic)
2666-3899 (Linking)"	"10.1016/j.patter.2022.100472"	""	""	""	""	"PMC9122968"	""	""	""	""	""	""	"35607626"	""	""	"adversarial attacks
artificial intelligence
attack transferability
computer vision
decorrelation
deep learning
radiomics"	"Adversarial attack transferability is well recognized in deep learning. Previous work has partially explained transferability by recognizing common adversarial subspaces and correlations between decision boundaries, but little is known beyond that. We propose that transferability between seemingly different models is due to a high linear correlation between the feature sets that different networks extract. In other words, two models trained on the same task that are distant in the parameter space likely extract features in the same fashion, linked by trivial affine transformations between the latent spaces. Furthermore, we show how applying a feature correlation loss, which decorrelates the extracted features in corresponding latent spaces, can reduce the transferability of adversarial attacks between models, suggesting that the models complete tasks in semantically different ways. Finally, we propose a dual-neck autoencoder (DNA), which leverages this feature correlation loss to create two meaningfully different encodings of input information with reduced transferability."	"Wiedeman, Christopher
Wang, Ge
eng
Patterns (N Y). 2022 Mar 24;3(5):100472. doi: 10.1016/j.patter.2022.100472. eCollection 2022 May 13."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35607626"	""	"Rensselaer Polytechnic Institute, Department of Electrical and Computer Systems Engineering, Troy, NY, USA.
Rensselaer Polytechnic Institute, Department of Biomedical Engineering, Troy, NY, USA."	""	""	""	""	""	""	""	""
"Journal Article"	"Y. Mathov; L. Rokach; Y. Elovici"	"2022"	"Enhancing real-world adversarial patches through 3D modeling of complex target scenes"	""	"Neurocomputing"	""	""	"499"	""	""	"11-22"	""	""	""	""	"Aug 14"	""	""	"Enhancing real-world adversarial patches through 3D modeling of complex target scenes"	"Neurocomputing"	"0925-2312"	"10.1016/j.neucom.2022.05.031"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000802967100002"	""	""	"adversarial example
adversarial learning
3d modeling"	"Adversarial examples have proven to be a concerning threat to deep learning models, particularly in the image domain. While many studies have examined adversarial examples in the real world, most of them relied on 2D photos of the attack scene. As a result, the attacks proposed may have limited effectiveness when implemented in realistic environments with 3D objects or varied conditions. Some studies on adversarial learning have used 3D objects, however in many cases, other researchers are unable to replicate the real-world evaluation process. In this study, we present a framework that uses 3D modeling to craft adversarial patches for an existing real-world scene. Our approach uses a 3D digital approximation of the scene to simulate the real world. With the ability to add and manipulate any element in the digital scene, our framework enables the attacker to improve the adversarial patch's impact in real-world settings. We use the framework to create a patch for an everyday scene and evaluate its performance using a novel evaluation process that ensures that our results are reproducible in both the digital space and the real world. Our evaluation results show that the framework can generate adversarial patches that are robust to different settings in the real world.(c) 2022 Elsevier B.V. All rights reserved."	"1q8yi
Times Cited:0
Cited References Count:26"	""	"<Go to ISI>://WOS:000802967100002"	""	"Bengurion Univ Negev, Dept Software & Informat Syst Engn, IL-8410501 Beer Sheva, Israel"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Umer; C. Frederickson; R. Polikar"	"2019"	"Vulnerability of Covariate Shift Adaptation Against Malicious Poisoning Attacks"	""	"2019 International Joint Conference on Neural Networks (Ijcnn)"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"Vulnerability of Covariate Shift Adaptation Against Malicious Poisoning Attacks"	"Ieee Ijcnn"	"2161-4393"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000530893800067"	""	""	""	"Adversarial machine learning has recently risen to prominence due to increased concerns over the vulnerability of machine learning algorithms to malicious attacks. While the impact of malicious poisoning attacks on some popular algorithms, such as deep neural networks, has been well researched, the vulnerability of other approaches has not yet been properly established. In this effort, we explore the vulnerability of unconstrained least squares importance fitting (uLSIF), an algorithm used for computing the importance ratio for covariate shift domain adaptation problems. The uLSIF algorithm is an accurate and efficient technique to compute the importance ratio; however, we show that the approach is susceptible to a poisoning attack, where an intelligent adversary - having full or partial access to the training data - can inject well crafted malicious samples into the training data, resulting in an incorrect estimation of the importance values. Through strategically designed synthetic as well as real world datasets, we demonstrate that importance ratio estimation through uLSIF algorithm can be easily compromised with the insertion of even modest number of attack points into the training data. We also show that incorrect estimation of importance values can then cripple the performance of a subsequent covariate shift adaptation."	"Bo9ha
Times Cited:0
Cited References Count:27
IEEE International Joint Conference on Neural Networks (IJCNN)"	""	"<Go to ISI>://WOS:000530893800067"	""	"Rowan Univ, Glassboro, NJ 08028 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Platzer; T. Reutterer"	"2021"	"Holdout-Based Empirical Assessment of Mixed-Type Synthetic Data"	""	"Front Big Data"	""	""	"4"	""	""	"679939"	""	""	""	"2021/07/17"	""	""	""	"Holdout-Based Empirical Assessment of Mixed-Type Synthetic Data"	""	"2624-909X (Electronic)
2624-909X (Linking)"	"10.3389/fdata.2021.679939"	""	""	""	""	"PMC8276128"	""	""	""	""	""	""	"34268491"	""	""	"anonymization
fidelity
mixed-type data
privacy
self-supervised learning
statistical disclosure control
structured data
synthetic data
remaining author declares that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
conflict of interest."	"AI-based data synthesis has seen rapid progress over the last several years and is increasingly recognized for its promise to enable privacy-respecting high-fidelity data sharing. This is reflected by the growing availability of both commercial and open-sourced software solutions for synthesizing private data. However, despite these recent advances, adequately evaluating the quality of generated synthetic datasets is still an open challenge. We aim to close this gap and introduce a novel holdout-based empirical assessment framework for quantifying the fidelity as well as the privacy risk of synthetic data solutions for mixed-type tabular data. Measuring fidelity is based on statistical distances of lower-dimensional marginal distributions, which provide a model-free and easy-to-communicate empirical metric for the representativeness of a synthetic dataset. Privacy risk is assessed by calculating the individual-level distances to closest record with respect to the training data. By showing that the synthetic samples are just as close to the training as to the holdout data, we yield strong evidence that the synthesizer indeed learned to generalize patterns and is independent of individual training records. We empirically demonstrate the presented framework for seven distinct synthetic data solutions across four mixed-type datasets and compare these then to traditional data perturbation techniques. Both a Python-based implementation of the proposed metrics and the demonstration study setup is made available open-source. The results highlight the need to systematically assess the fidelity just as well as the privacy of these emerging class of synthetic data generators."	"Platzer, Michael
Reutterer, Thomas
eng
Switzerland
Front Big Data. 2021 Jun 29;4:679939. doi: 10.3389/fdata.2021.679939. eCollection 2021."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34268491"	""	"MOSTLY AI, Vienna, Austria.
Department of Marketing, WU Vienna University of Economics and Business, Vienna, Austria."	""	""	""	""	""	""	""	""
"Journal Article"	"P. P. K. Chan; J. Zheng; H. Liu; E. C. C. Tsang; D. S. Yeung"	"2021"	"Robustness analysis of classical and fuzzy decision trees under adversarial evasion attack"	""	"Applied Soft Computing"	""	""	"107"	""	""	"107311"	""	""	""	""	"Aug"	""	""	"Robustness analysis of classical and fuzzy decision trees under adversarial evasion attack"	"Appl Soft Comput"	"1568-4946"	"ARTN 107311
10.1016/j.asoc.2021.107311"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000663736000009"	""	""	"adversarial learning
decision tree
fuzzy decision tree
robustness
evasion attack
classifiers
security"	"Although decision trees have been widely applied to different security related applications, their security has not been investigated extensively in an adversarial environment. This work aims to study the robustness of classical decision tree (DT) and Fuzzy decision tree (FDT) under evasion attack that manipulate the features in order to mislead the decision of a classifier. To the best of our knowledge, existing attack methods cannot be applied to DT due to non-differentiation of its decision function. This is the first attack model designed for both DT and FDT. Our model quantifies the influence of changing a feature on the decision. The effectiveness of our method is compared with Papernot (PPNT) and Robustness Verification of Tree-based Models (RVTM), which are state-of-the-art attack methods for DT, and the attack methods employing surrogate and Generative Adversarial Network (GAN) methods. The experimental results suggest that the fuzzifying process increases the robustness of DT. Moreover, FDT with more membership functions is more vulnerable since a smaller number of features is usually used. This study fills the gap of examining the security issue of fuzzy systems in an adversarial environment. (C) 2021 Elsevier B.V. All rights reserved."	"Sv3qc
Times Cited:4
Cited References Count:60"	""	"<Go to ISI>://WOS:000663736000009"	""	"South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Peoples R China
Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China
Macau Univ Sci & Technol, Fac Informat Technol, Taipa, Macao, Peoples R China
SMC Soc IEEE, Hong Kong, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Wang; G. Li; X. Liu; L. Lin"	"2022"	"A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack and Learning"	""	"IEEE Trans Pattern Anal Mach Intell"	""	""	"44"	""	"4"	"1725-1737"	""	""	""	"2020/10/20"	"Apr"	""	""	"A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack and Learning"	""	"1939-3539 (Electronic)
0098-5589 (Linking)"	"10.1109/TPAMI.2020.3032061"	""	""	""	""	""	""	""	""	""	""	""	"33074803"	""	""	"*Algorithms
Monte Carlo Method
*Neural Networks, Computer"	"Although deep convolutional neural networks (CNNs) have demonstrated remarkable performance on multiple computer vision tasks, researches on adversarial learning have shown that deep models are vulnerable to adversarial examples, which are crafted by adding visually imperceptible perturbations to the input images. Most of the existing adversarial attack methods only create a single adversarial example for the input, which just gives a glimpse of the underlying data manifold of adversarial examples. An attractive solution is to explore the solution space of the adversarial examples and generate a diverse bunch of them, which could potentially improve the robustness of real-world systems and help prevent severe security threats and vulnerabilities. In this paper, we present an effective method, called Hamiltonian Monte Carlo with Accumulated Momentum (HMCAM), aiming to generate a sequence of adversarial examples. To improve the efficiency of HMC, we propose a new regime to automatically control the length of trajectories, which allows the algorithm to move with adaptive step sizes along the search direction at different positions. Moreover, we revisit the reason for high computational cost of adversarial training under the view of MCMC and design a new generative method called Contrastive Adversarial Training (CAT), which approaches equilibrium distribution of adversarial examples with only few iterations by building from small modifications of the standard Contrastive Divergence (CD) and achieve a trade-off between efficiency and accuracy. Both quantitative and qualitative analysis on several natural image datasets and practical systems have confirmed the superiority of the proposed algorithm."	"Wang, Hongjun
Li, Guanbin
Liu, Xiaobai
Lin, Liang
eng
Research Support, Non-U.S. Gov't
IEEE Trans Pattern Anal Mach Intell. 2022 Apr;44(4):1725-1737. doi: 10.1109/TPAMI.2020.3032061. Epub 2022 Mar 4."	""	"https://www.ncbi.nlm.nih.gov/pubmed/33074803"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"P. Porambage; G. Gur; D. P. M. Osorio; M. Liyanage; A. Gurtov; M. Ylianttila"	"2021"	"The Roadmap to 6G Security and Privacy"	""	"Ieee Open Journal of the Communications Society"	""	""	"2"	""	""	"1094-1122"	""	""	""	""	""	""	""	"The Roadmap to 6G Security and Privacy"	"Ieee Open J Comm Soc"	""	"10.1109/Ojcoms.2021.3078081"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000655503800002"	""	""	"6g mobile communication
security
privacy
5g mobile communication
computer architecture
wireless communication
distributed ledger
6g
security
security threats
ai
ml security
dlt
physical layer security
privacy
quantum computing
physical layer security
artificial-intelligence
wireless communication
edge intelligence
smart contract
blockchain
networks
systems
challenges
5g"	"Although the fifth generation (5G) wireless networks are yet to be fully investigated, the visionaries of the 6th generation (6G) echo systems have already come into the discussion. Therefore, in order to consolidate and solidify the security and privacy in 6G networks, we survey how security may impact the envisioned 6G wireless systems, possible challenges with different 6G technologies, and the potential solutions. We provide our vision on 6G security and security key performance indicators (KPIs) with the tentative threat landscape based on the foreseen 6G network architecture. Moreover, we discuss the security and privacy challenges that may encounter with the available 6G requirements and potential 6G applications. We also give the reader some insights into the standardization efforts and research-level projects relevant to 6G security. In particular, we discuss the security considerations with 6G enabling technologies such as distributed ledger technology (DLT), physical layer security, distributed AI/ML, visible light communication (VLC), THz, and quantum computing. All in all, this work intends to provide enlightening guidance for the subsequent research of 6G security and privacy at this initial phase of vision towards reality."	"Sj4li
Times Cited:15
Cited References Count:172"	""	"<Go to ISI>://WOS:000655503800002"	""	"Univ Oulu, Ctr Wireless Commun, Oulu 90570, Finland
Zurich Univ Appl Sci ZHAW, Inst Appl Informat Technol InIT, CH-8400 Winterthur, Switzerland
Univ Coll Dublin, Sch Comp Sci, Dublin D04 V1W8, Ireland
Linkoping Univ, Dept Comp & Informat Sci, S-58183 Linkoping, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Oestreich; D. Chen; J. L. Schultze; M. Fritz; M. Becker"	"2021"	"Privacy considerations for sharing genomics data"	""	"EXCLI J"	""	""	"20"	""	""	"1243-1260"	""	""	""	"2021/08/05"	""	""	""	"Privacy considerations for sharing genomics data"	""	"1611-2156 (Print)
1611-2156 (Linking)"	"10.17179/excli2021-4002"	""	""	""	""	"PMC8326502"	""	""	""	""	""	""	"34345236"	""	""	"data privacy
data sharing
epigenomic data
genomic data
transcriptomic data"	"An increasing amount of attention has been geared towards understanding the privacy risks that arise from sharing genomic data of human origin. Most of these efforts have focused on issues in the context of genomic sequence data, but the popularity of techniques for collecting other types of genome-related data has prompted researchers to investigate privacy concerns in a broader genomic context. In this review, we give an overview of different types of genome-associated data, their individual ways of revealing sensitive information, the motivation to share them as well as established and upcoming methods to minimize information leakage. We further discuss the concise threats that are being posed, who is at risk, and how the risk level compares to potential benefits, all while addressing the topic in the context of modern technology, methodology, and information sharing culture. Additionally, we will discuss the current legal situation regarding the sharing of genomic data in a selection of countries, evaluating the scope of their applicability as well as their limitations. We will finalize this review by evaluating the development that is required in the scientific field in the near future in order to improve and develop privacy-preserving data sharing techniques for the genomic context."	"Oestreich, Marie
Chen, Dingfan
Schultze, Joachim L
Fritz, Mario
Becker, Matthias
eng
Review
Germany
EXCLI J. 2021 Jul 16;20:1243-1260. doi: 10.17179/excli2021-4002. eCollection 2021."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34345236"	""	"Systems Medicine, Deutsches Zentrum fur Neurodegenerative Erkrankungen (DZNE), Venusberg-Campus 1/99, 53127 Bonn, Germany.
CISPA Helmholtz Center for Information Security, Saarbrucken, Germany, Stuhlsatzenhaus 5, 66123 Saarbrucken, Germany.
Genomics and Immunoregulation, Life & Medical Sciences (LIMES) Institute, University of Bonn, Bonn, Germany, Carl-Troll-Strasse 31, 53115 Bonn, Germany.
PRECISE Platform for Single Cell Genomics and Epigenomics at Deutsches Zentrum fur Neurodegenerative Erkrankungen (DZNE) and the University of Bonn, Germany, Venusberg-Campus 1/99, 53127 Bonn, Germany."	""	""	""	""	""	""	""	""
"Journal Article"	"B. Lagesse; C. Burkard; J. Perez"	"2016"	"Securing Pervasive Systems Against Adversarial Machine Learning"	""	"2016 Ieee International Conference on Pervasive Computing and Communication Workshops (Percom Workshops)"	""	""	""	""	""	"1-4"	""	""	""	""	""	""	""	"Securing Pervasive Systems Against Adversarial Machine Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000381790800014"	""	""	""	"Applications and middleware in pervasive systems frequently rely on machine learning to provide adaptivity and customization that results in a seamless user experience despite operating in a dynamic environment. Machine learning algorithms have been shown to be vulnerable to covert, strategic attacks through the manipulation of training data. Machine learning algorithms in pervasive systems frequently train on data that could be manipulated by a malicious 3rd party. In this paper, we present our ongoing work to develop a security mechanism that is designed to work in the dynamic environments of pervasive computing as opposed to traditional security mechanisms that are designed for static environments. Furthermore, we present our modular testing framework that will be used to rapidly compare our work with other security mechanisms, applications and adversarial models."	"Bf4zd
Times Cited:0
Cited References Count:13"	""	"<Go to ISI>://WOS:000381790800014"	""	"Univ Washington Bothell, Comp & Software Syst, Bothell, WA 98011 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Soumare; S. Rezgui; N. Gmati; A. Benkahla"	"2021"	"New neural network classification method for individuals ancestry prediction from SNPs data"	""	"BioData Min"	""	""	"14"	""	"1"	"30"	""	""	""	"2021/06/30"	"Jun 28"	""	""	"New neural network classification method for individuals ancestry prediction from SNPs data"	""	"1756-0381 (Print)
1756-0381 (Linking)"	"10.1186/s13040-021-00258-7"	""	""	""	""	"PMC8240223"	""	""	""	""	""	""	"34183066"	""	""	"Artificial neural network
Dimensionality reduction
Input perturbation
Single nucleotide polymorphism
Singular value decomposition"	"Artificial Neural Network (ANN) algorithms have been widely used to analyse genomic data. Single Nucleotide Polymorphisms(SNPs) represent the genetic variations, the most common in the human genome, it has been shown that they are involved in many genetic diseases, and can be used to predict their development. Developing ANN to handle this type of data can be considered as a great success in the medical world. However, the high dimensionality of genomic data and the availability of a limited number of samples can make the learning task very complicated. In this work, we propose a New Neural Network classification method based on input perturbation. The idea is first to use SVD to reduce the dimensionality of the input data and to train a classification network, which prediction errors are then reduced by perturbing the SVD projection matrix. The proposed method has been evaluated on data from individuals with different ancestral origins, the experimental results have shown the effectiveness of the proposed method. Achieving up to 96.23% of classification accuracy, this approach surpasses previous Deep learning approaches evaluated on the same dataset."	"Soumare, H
Rezgui, S
Gmati, N
Benkahla, A
eng
U41HG006941/H3ABioNet
England
BioData Min. 2021 Jun 28;14(1):30. doi: 10.1186/s13040-021-00258-7."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34183066"	""	"The Laboratory of Mathematical Modelling and Numeric in Engineering Sciences, National Engineering School of Tunis, Rue Bechir Salem Belkhiria Campus universitaire, B.P. 37, 1002 Tunis Belvedere, University of Tunis El Manar, Tunis, Tunisia. soumare.harouna@enit.utm.tn.
Laboratory of BioInformatics, bioMathematics, and bioStatistics, 13 place Pasteur, B.P. 74 1002 Tunis, Belvedere, Institut Pasteur de Tunis, University of Tunis El Manar, Tunis, Tunisia. soumare.harouna@enit.utm.tn.
ADAGOS. Le Belvedere centre, 61 rue El Khartoum, El Menzah, Tunis, Tunisia.
College of sciences & Basic and Applied Scientific Research Center, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, 31441, Dammam, Kingdom of Saudi Arabia, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia.
Laboratory of BioInformatics, bioMathematics, and bioStatistics, 13 place Pasteur, B.P. 74 1002 Tunis, Belvedere, Institut Pasteur de Tunis, University of Tunis El Manar, Tunis, Tunisia."	""	""	""	""	""	""	""	""
"Journal Article"	"C. L. Miao; Q. Li; L. Su; M. D. Huai; W. J. Jiang; J. Gao"	"2018"	"Attack under Disguise: An Intelligent Data Poisoning Attack Mechanism in Crowdsourcing"	""	"Web Conference 2018: Proceedings of the World Wide Web Conference (Www2018)"	""	""	""	""	""	"13-22"	""	""	""	""	""	""	""	"Attack under Disguise: An Intelligent Data Poisoning Attack Mechanism in Crowdsourcing"	""	""	"10.1145/3178876.3186032"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000460379000002"	""	""	"crowdsourcing
data poisoning
expectation maximization"	"As an effective way to solicit useful information from the crowd, crowdsourcing has emerged as a popular paradigm to solve challenging tasks. However, the data provided by the participating workers are not always trustworthy. In real world, there may exist malicious workers in crowdsourcing systems who conduct the data poisoning attacks for the purpose of sabotage or financial rewards. Although data aggregation methods such as majority voting are conducted on workers' labels in order to improve data quality, they are vulnerable to such attacks as they treat all the workers equally. In order to capture the variety in the reliability of workers, the Dawid-Skene model, a sophisticated data aggregation method, has been widely adopted in practice. By conducting maximum likelihood estimation (MLE) using the expectation maximization (EM) algorithm, the Dawid-Skene model can jointly estimate each worker's reliability and conduct weighted aggregation, and thus can tolerate the data poisoning attacks to some degree. However, the Dawid-Skene model still has weakness. In this paper, we study the data poisoning attacks against such crowdsourcing systems with the Dawid-Skene model empowered. We design an intelligent attack mechanism, based on which the attacker can not only achieve maximum attack utility but also disguise the attacking behaviors. Extensive experiments based on real-world crowdsourcing datasets are conducted to verify the desirable properties of the proposed mechanism."	"Bm1pp
Times Cited:20
Cited References Count:59"	""	"<Go to ISI>://WOS:000460379000002"	""	"SUNY Buffalo, Buffalo, NY 14260 USA
Univ Illinois, Champaign, IL USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. C. Anindya; M. Kantarcioglu"	"2018"	"Adversarial Anomaly Detection Using Centroid-based Clustering"	""	"2018 Ieee International Conference on Information Reuse and Integration (Iri)"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"Adversarial Anomaly Detection Using Centroid-based Clustering"	""	""	"10.1109/Iri.2018.00009"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000442457000001"	""	""	"anomaly detection
adversarial machine learning
clustering
mimicry attack"	"As cyber attacks are growing with an unprecedented rate in the recent years, organizations are seeking an efficient and scalable solution towards a holistic protection system. As the adversaries are becoming more skilled and organized, traditional rule based detection systems have been proved to be quite ineffective against the continuously evolving cyber attacks. Consequently, security researchers are focusing on applying machine learning techniques and big data analytics to defend against cyber attacks. Over the recent years, several anomaly detection systems have been claimed to be quite successful against the sophisticated cyber attacks including the previously unseen zero-day attacks. But often, these systems do not consider the adversary's adaptive attacking behavior for bypassing the detection procedure. As a result, deploying these systems in active real-world scenarios fails to provide significant benefits in the presence of intelligent adversaries that are carefully manipulating the attack vectors. In this work, we analyze the adversarial impact on anomaly detection models that are built upon centroid-based clustering from game-theoretic aspect and propose adversarial anomaly detection technique for these models. The experimental results show that our game-theoretic anomaly detection models can withstand attacks more effectively compared to the traditional models."	"Bk8br
Times Cited:3
Cited References Count:30"	""	"<Go to ISI>://WOS:000442457000001"	""	"Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. R. S. Vicarte; B. Schreiber; R. Paccagnella; C. W. Fletcher"	"2020"	"Game of Threads: Enabling Asynchronous Poisoning Attacks"	""	"Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (Asplos Xxv)"	""	""	""	""	""	"35-52"	""	""	""	""	""	""	""	"Game of Threads: Enabling Asynchronous Poisoning Attacks"	""	""	"10.1145/3373376.3378462"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000541369300004"	""	""	""	"As data sizes continue to grow at an unprecedented rate, machine learning training is being forced to adopt asynchronous algorithms to maintain performance and scalability. In asynchronous training, many threads share and update model parameters in a racy fashion to avoid costly interthread synchronization.
This paper studies the security implications of these codes by introducing asynchronous poisoning attacks. Our attack influences training outcome-e.g., degrades model accuracy or biases the model towards an adversary-specified label-purely by scheduling asynchronous training threads in a malicious fashion. Since thread scheduling is outside the protections of modern trusted execution environments (TEEs), e.g., Intel SGX, our attack bypasses these protections even when the training set can be verified as correct. To the best of our knowledge, this represents the first example where a class of applications loses integrity guarantees, despite being protected by enclave-based TEEs such as SGX.
We demonstrate both accuracy degradation and model biasing attacks on the CIFAR-10 image recognition task, trained on Resnet-style DNNs using an asynchronous training code published by Pytorch. We also perform proof-of-concept experiments to validate our assumptions on an SGX-enabled machine. Our accuracy degradation attacks are capable of returning a converged model to pre-trained accuracy or to some accuracy in between. Our model biasing attack can force the model to predict an adversary-specified label up to similar to 40% of the time on the CIFAR-10 validation set, depending on parameters. (Whereas the un-attacked model's prediction rate towards any label is similar to 10%.)"	"Bp1xu
Times Cited:2
Cited References Count:77"	""	"<Go to ISI>://WOS:000541369300004"	""	"Univ Illinois, Champaign, IL 61820 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Jagielski; A. Oprea; B. Biggio; C. Liu; C. Nita-Rotaru; B. Li"	"2018"	"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning"	""	"2018 Ieee Symposium on Security and Privacy (Sp)"	""	""	""	""	""	"19-35"	""	""	""	""	""	""	""	"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning"	"P Ieee S Secur Priv"	"1081-6011"	"10.1109/Sp.2018.00057"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000442163200003"	""	""	""	"As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains."	"Bk7ut
Times Cited:168
Cited References Count:55
IEEE Symposium on Security and Privacy"	""	"<Go to ISI>://WOS:000442163200003"	""	"Northeastern Univ, Boston, MA 02115 USA
Univ Cagliary, Cagliari, Italy
Univ Calif Berkeley, Berkeley, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Goldblum; D. Tsipras; C. Xie; X. Chen; A. Schwarzschild; D. Song; A. Madry; B. Li; T. Goldstein"	"2022"	"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"	""	"IEEE Trans Pattern Anal Mach Intell"	""	""	"PP"	""	""	""	""	""	""	"2022/03/26"	"Mar 25"	""	""	"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"	""	"1939-3539 (Electronic)
0098-5589 (Linking)"	"10.1109/TPAMI.2022.3162397"	""	""	""	""	""	""	""	""	""	""	""	"35333711"	""	""	""	"As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space."	"Goldblum, Micah
Tsipras, Dimitris
Xie, Chulin
Chen, Xinyun
Schwarzschild, Avi
Song, Dawn
Madry, Aleksander
Li, Bo
Goldstein, Tom
eng
IEEE Trans Pattern Anal Mach Intell. 2022 Mar 25;PP. doi: 10.1109/TPAMI.2022.3162397."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35333711"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Ma; Q. Xu; J. Zeng; X. Cao; Q. Huang"	"2021"	"Poisoning Attack against Estimating from Pairwise Comparisons"	""	"IEEE Trans Pattern Anal Mach Intell"	""	""	"PP"	""	""	""	""	""	""	"2021/06/09"	"Jun 8"	""	""	"Poisoning Attack against Estimating from Pairwise Comparisons"	""	"1939-3539 (Electronic)
0098-5589 (Linking)"	"10.1109/TPAMI.2021.3087514"	""	""	""	""	""	""	""	""	""	""	""	"34101586"	""	""	""	"As pairwise ranking becomes broadly employed for elections, sports competitions, recommendation, and so on, attackers have strong motivation and incentives to manipulate the ranking list. They could inject malicious comparisons into the training data to fool the victim. Such a technique is called '`poisoning attack'' in regression and classification tasks. In this paper, to the best of our knowledge, we initiate the first systematic investigation of data poisoning attack on pairwise ranking algorithms, which can be formalized as the dynamic and static games between the ranker and the attacker, and can be modeled as certain kinds of integer programming problems. To break the computational hurdle of the underlying integer programming problems, we reformulate them into the distributionally robust optimization (DRO) problems, which are computational tractable. Based on such DRO formulations, we propose two efficient poisoning attack algorithms and establish the associated theoretical guarantees. The effectiveness of the suggested poisoning attack strategies is demonstrated by a series of toy simulations and several real data experiments. These experimental results show that the proposed methods can significantly reduce the performance of the ranker in the sense that the correlation between the true ranking list and the aggregated results can be decreased dramatically."	"Ma, Ke
Xu, Qianqian
Zeng, Jinshan
Cao, Xiaochun
Huang, Qingming
eng
IEEE Trans Pattern Anal Mach Intell. 2021 Jun 8;PP. doi: 10.1109/TPAMI.2021.3087514."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34101586"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"G. L. Liu; L. F. Lai"	"2020"	"Action-Manipulation Attacks on Stochastic Bandits"	""	"2020 Ieee International Conference on Acoustics, Speech, and Signal Processing"	""	""	""	""	""	"3112-3116"	""	""	""	""	""	""	""	"Action-Manipulation Attacks on Stochastic Bandits"	"Int Conf Acoust Spee"	"1520-6149"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000615970403072"	""	""	"stochastic bandits
action-manipulation attack
ucb"	"As stochastic multi-armed bandit model has many important applications, understanding the impact of adversarial attacks on this model is essential for the safe applications of this model. In this paper, we propose a new class of attack named action-manipulation attack, where an adversary can change the action signal selected by the user. We investigate the attack against a very popular and widely used bandit algorithm: Upper Confidence Bound (UCB) algorithm. Without knowledge of mean rewards of arms, our proposed attack scheme can force the user to pull a target arm very frequently by spending only logarithm cost."	"Bq7hu
Times Cited:4
Cited References Count:15
International Conference on Acoustics Speech and Signal Processing ICASSP"	""	"<Go to ISI>://WOS:000615970403072"	""	"Univ Calif Davis, Dept ECE, Davis, CA 95616 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Chakraborty; F. Pierazzi; V. S. Subrahmanian"	"2020"	"EC2: Ensemble Clustering and Classification for Predicting Android Malware Families"	""	"Ieee Transactions on Dependable and Secure Computing"	""	""	"17"	""	"2"	"262-277"	""	""	""	""	"Mar-Apr"	""	""	"EC2: Ensemble Clustering and Classification for Predicting Android Malware Families"	"Ieee T Depend Secure"	"1545-5971"	"10.1109/Tdsc.2017.2739145"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000520505100004"	""	""	"malware
androids
humanoid robots
feature extraction
smart phones
mobile communication
clustering algorithms
android
malware
ensemble
classification
clustering"	"As the most widely used mobile platform, Android is also the biggest target for mobile malware. Given the increasing number of Android malware variants, detecting malware families is crucial so that security analysts can identify situations where signatures of a known malware family can be adapted as opposed to manually inspecting behavior of all samples. We present EC2 (Ensemble Clustering and Classification), a novel algorithm for discovering Android malware families of varying sizes-ranging from very large to very small families (even if previously unseen). We present a performance comparison of several traditional classification and clustering algorithms for Android malware family identification on DREBIN, the largest public Android malware dataset with labeled families. We use the output of both supervised classifiers and unsupervised clustering to design EC2. Experimental results on both the DREBIN and the more recent Koodous malware datasets show that EC2 accurately detects both small and large families, outperforming several comparative baselines. Furthermore, we show how to automatically characterize and explain unique behaviors of specific malware families, such as FakeInstaller, MobileTx, Geinimi. In short, EC2 presents an early warning system for emerging new malware families, as well as a robust predictor of the family (when it is not new) to which a new malware sample belongs, and the design of novel strategies for data-driven understanding of malware behaviors."	"Kv5eg
Times Cited:26
Cited References Count:53"	""	"<Go to ISI>://WOS:000520505100004"	""	"Delhi IIIT D, Indraprastha Inst Informat Technol, Dept Comp Sci & Engn, Delhi 110020, India
Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, I-41125 Modena, Italy
Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Frederickson; M. Moore; G. Dawson; R. Polikar"	"2018"	"Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning"	""	"2018 International Joint Conference on Neural Networks (Ijcnn)"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning"	"Ieee Ijcnn"	"2161-4393"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000585967404043"	""	""	""	"As the prevalence and everyday use of machine learning algorithms, along with our reliance on these algorithms grow dramatically, so do the efforts to attack and undermine these algorithms with malicious intent, resulting in a growing interest in adversarial machine learning. A number of approaches have been developed that can render a machine learning algorithm ineffective through poisoning or other types of attacks. Most attack algorithms typically use sophisticated optimization approaches, whose objective function is designed to cause maximum damage to accuracy or performance of the algorithm with respect to some task. In this effort, we show that while such an objective function is indeed brutally effective in causing maximum damage on an embedded feature selection task, it often results in an attack mechanism that can be easily detected with an embarrassingly simple novelty or outlier detection algorithm. We then propose an equally simple yet elegant solution by adding a regularization term to the attacker's objective function that penalizes outlying attack points."	"Bq3no
Times Cited:1
Cited References Count:37
IEEE International Joint Conference on Neural Networks (IJCNN)"	""	"<Go to ISI>://WOS:000585967404043"	""	"Rowan Univ, Glassboro, NJ 08028 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Liu; G. Ditzler"	"2020"	"Adversarial Audio Attacks that Evade Temporal Dependency"	""	"2020 Ieee Symposium Series on Computational Intelligence (Ssci)"	""	""	""	""	""	"639-646"	""	""	""	""	""	""	""	"Adversarial Audio Attacks that Evade Temporal Dependency"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000682772900087"	""	""	""	"As the real-world applications (image segmentation, speech recognition, machine translation, etc.) are increasingly adopting Deep Neural Networks (DNNs), DNN's vulnerabilities in a malicious environment have become an increasingly important research topic in adversarial machine learning. Adversarial machine learning (AML) focuses on exploring vulnerabilities and defensive techniques for machine learning models. Recent work has shown that most adversarial audio generation methods fail to consider audios' temporal dependency (TD) (i.e., adversarial audios exhibit weaker TD than benign audios). As a result, the adversarial audios are easily detectable by examining their TD. Therefore, one area of interest in the audio AML community is to develop a novel attack that evades a TD-based detection model. In this contribution, we revisit the LSTM model for audio transcription and propose a new audio attack algorithm that evades the TD-based detection by explicitly controlling the TD in generated adversarial audios. The experimental results show that the detectability of our adversarial audio is significantly reduced compared to the stale-of-the-art audio attack algorithms. Furthermore, experiments also show that our adversarial audios remain nearly indistinguishable from benign audios with only negligible perturbation magnitude."	"Bs0gu
Times Cited:0
Cited References Count:26"	""	"<Go to ISI>://WOS:000682772900087"	""	"Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Q. Wang; Q. Liu; Y. P. Chi"	"2020"	"Review of Android Malware Detection Based on Deep Learning"	""	"Ieee Access"	""	""	"8"	""	""	"181102-181126"	""	""	""	""	""	""	""	"Review of Android Malware Detection Based on Deep Learning"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3028370"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000577895900001"	""	""	"android
malware
deep learning
review
system"	"At present, smartphones running the Android operating system have occupied the leading market share. However, due to the Android operating system's open-source nature, Android malware has increased dramatically. Malware can steal user privacy and even maliciously charge fees and steal funds. It has posed a severe threat to cyberspace security because traditional detection methods have many limitations. With the widespread application of deep learning in recent years, the method of detecting Android malware using deep learning has gradually attracted widespread attention from scholars at home and abroad. Although scholars have researched Android malware detection using deep learning, there is currently a lack of a detailed and comprehensive introduction to malware detection's latest research results based on deep learning. In order to solve this problem, this study analyzes and summarizes the latest research results by investigating a large number of the latest domestic and international academic papers, summarizing malware detection architecture and detection schemes, and analyzing existing problems and challenges. This review will help researchers better understand the research status and future research directions in this field."	"Oa6me
Times Cited:7
Cited References Count:103"	""	"<Go to ISI>://WOS:000577895900001"	""	"Beijing Elect Sci & Technol Inst, Dept Cyberspace Secur, Beijing 100071, Peoples R China
State Informat Ctr, Beijing 100000, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Sahay; C. G. Brinton; D. J. Love"	"2021"	"Frequency-based Automated Modulation Classification in the Presence of Adversaries"	""	"Ieee International Conference on Communications (Icc 2021)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Frequency-based Automated Modulation Classification in the Presence of Adversaries"	"Ieee Icc"	"1550-3607"	"10.1109/Icc42927.2021.9500583"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000719386002017"	""	""	"adversarial attacks
automatic modulation classification
machine learning
privacy
security"	"Automatic modulation classification (AMC) aims to improve the efficiency of crowded radio spectrums by automatically predicting the modulation constellation of wireless RF signals. Recent work has demonstrated the ability of deep learning to achieve robust AMC performance using raw inphase and quadrature (IQ) time samples. Yet, deep learning models are highly susceptible to adversarial interference, which cause intelligent prediction models to misclassify received samples with high confidence. Furthermore, adversarial interference is often transferable, allowing an adversary to attack multiple deep learning models with a single perturbation crafted for a particular classification network. In this work, we present a novel receiver architecture consisting of deep learning models capable of withstanding transferable adversarial interference. Specifically, we show that adversarial attacks crafted to fool models trained on time-domain features are not easily transferable to models trained using frequency-domain features. In this capacity, we demonstrate classification performance improvements greater than 30% on recurrent neural networks (RNNs) and greater than 50% on convolutional neural networks (CNNs). We further demonstrate our frequency feature-based classification models to achieve accuracies greater than 99% in the absence of attacks."	"Bs4hx
Times Cited:0
Cited References Count:18
IEEE International Conference on Communications"	""	"<Go to ISI>://WOS:000719386002017"	""	"Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Pham; K. Q. Xiong"	"2021"	"A survey on security attacks and defense techniques for connected and autonomous vehicles"	""	"Computers & Security"	""	""	"109"	""	""	"102269"	""	""	""	""	"Oct"	""	""	"A survey on security attacks and defense techniques for connected and autonomous vehicles"	"Comput Secur"	"0167-4048"	"ARTN 102269
10.1016/j.cose.2021.102269"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000685459300002"	""	""	"intelligent transportation system
cybersecurity
traffic engineering
connected and autonomous vehicles
simultaneous localization
spoofing detection
receiver
classification
vulnerability
antenna
impact
phase
dsrc"	"Autonomous Vehicle has been transforming intelligent transportation systems. As telecom-munication technology improves, autonomous vehicles are getting connected to each other and to infrastructures, forming Connected and Autonomous Vehicles (CAVs). CAVs will help humans achieve safe, efficient, and autonomous transportation systems. However, CAVs will face significant security challenges because many of their components are vulnerable to attacks, and a successful attack on a CAV may have significant impacts on other CAVs and infrastructures due to their interconnectivity. In this paper, we conduct a survey on 189 papers from 2000 to 2020 to understand state-of-the-art CAV attacks and defense tech-niques. Of those 189 papers, 131 were directly concerned with attack models or defense strategies for CAVs. This survey first presents a comprehensive overview of security attacks and their corresponding countermeasures on CAVs. We then discuss the details of attack models based on the targeted CAV components of attacks, access requirements, and attack motives. Finally, we identify some current research challenges and trends from the per-spectives of both academic research and industrial development. Based on our studies of academic literature and industrial publications, we have not found any strong connection between academic research and industry's implementation of CAV-related security issues. While efforts from CAV manufacturers to secure CAVs have been reported, there is no evi-dence to show that CAVs on the market can defend against some novel attack models that the research community has recently found. This survey may give researchers and engi-neers a better understanding of the current status and trend of CAV security for CAV's future improvement. (c) 2021 Elsevier Ltd. All rights reserved."	"Ua9ff
Times Cited:5
Cited References Count:188"	""	"<Go to ISI>://WOS:000685459300002"	""	"Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA
Univ S Florida, Intelligent Comp Networking & Secur Lab, Florida Ctr Cybersecur, Dept Math & Stat,Dept Elect Engn, Tampa, FL 33620 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"K. El Emam; L. Mosquera; X. Fang; A. El-Hussuna"	"2022"	"Utility Metrics for Evaluating Synthetic Health Data Generation Methods: Validation Study"	""	"JMIR Med Inform"	""	""	"10"	""	"4"	"e35734"	""	""	""	"2022/04/08"	"Apr 7"	""	""	"Utility Metrics for Evaluating Synthetic Health Data Generation Methods: Validation Study"	""	"2291-9694 (Print)"	"10.2196/35734"	""	""	""	""	"PMC9030990"	""	""	""	""	""	""	"35389366"	""	""	"binary prediction model
data privacy
data utility
generative models
logistic regression
medical informatics
model validation
prediction model
synthetic data
synthetic data generation
utility metric"	"BACKGROUND: A regular task by developers and users of synthetic data generation (SDG) methods is to evaluate and compare the utility of these methods. Multiple utility metrics have been proposed and used to evaluate synthetic data. However, they have not been validated in general or for comparing SDG methods. OBJECTIVE: This study evaluates the ability of common utility metrics to rank SDG methods according to performance on a specific analytic workload. The workload of interest is the use of synthetic data for logistic regression prediction models, which is a very frequent workload in health research. METHODS: We evaluated 6 utility metrics on 30 different health data sets and 3 different SDG methods (a Bayesian network, a Generative Adversarial Network, and sequential tree synthesis). These metrics were computed by averaging across 20 synthetic data sets from the same generative model. The metrics were then tested on their ability to rank the SDG methods based on prediction performance. Prediction performance was defined as the difference between each of the area under the receiver operating characteristic curve and area under the precision-recall curve values on synthetic data logistic regression prediction models versus real data models. RESULTS: The utility metric best able to rank SDG methods was the multivariate Hellinger distance based on a Gaussian copula representation of real and synthetic joint distributions. CONCLUSIONS: This study has validated a generative model utility metric, the multivariate Hellinger distance, which can be used to reliably rank competing SDG methods on the same data set. The Hellinger distance metric can be used to evaluate and compare alternate SDG methods."	"El Emam, Khaled
Mosquera, Lucy
Fang, Xi
El-Hussuna, Alaa
eng
Canada
JMIR Med Inform. 2022 Apr 7;10(4):e35734. doi: 10.2196/35734."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35389366"	""	"School of Epidemiology and Public Health, University of Ottawa, Ottawa, ON, Canada.
Children's Hospital of Eastern Ontario Research Institute, Ottawa, ON, Canada.
Replica Analytics Ltd, Ottawa, ON, Canada.
Open Source Research Collaboration, Aarlberg, Denmark."	""	""	""	""	""	""	""	""
"Journal Article"	"B. Yu; Y. Fang; Q. Yang; Y. Tang; L. Liu"	"2018"	"A survey of malware behavior description and analysis"	""	"Frontiers of Information Technology & Electronic Engineering"	""	""	"19"	""	"5"	"583-603"	""	""	""	""	"May"	""	""	"A survey of malware behavior description and analysis"	"Front Inform Tech El"	"2095-9184"	"10.1631/Fitee.1601745"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000438996800001"	""	""	"malware behavior
static analysis
dynamic analysis
behavior data expression
behavior analysis
machine learning
semantics-based analysis
behavior visualization
malware evolution
classification
framework
system"	"Behavior-based malware analysis is an important technique for automatically analyzing and detecting malware, and it has received considerable attention from both academic and industrial communities. By considering how malware behaves, we can tackle the malware obfuscation problem, which cannot be processed by traditional static analysis approaches, and we can also derive the as-built behavior specifications and cover the entire behavior space of the malware samples. Although there have been several works focusing on malware behavior analysis, such research is far from mature, and no overviews have been put forward to date to investigate current developments and challenges. In this paper, we conduct a survey on malware behavior description and analysis considering three aspects: malware behavior description, behavior analysis methods, and visualization techniques. First, existing behavior data types and emerging techniques for malware behavior description are explored, especially the goals, principles, characteristics, and classifications of behavior analysis techniques proposed in the existing approaches. Second, the inadequacies and challenges in malware behavior analysis are summarized from different perspectives. Finally, several possible directions are discussed for future research."	"Gn4mc
Times Cited:15
Cited References Count:117"	""	"<Go to ISI>://WOS:000438996800001"	""	"Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Wang; L. Barriga; A. Vahidi; S. Raza"	"2019"	"Machine Learning for Security at the IoT Edge - A Feasibility Study"	""	"2019 Ieee 16th International Conference on Mobile Ad Hoc and Sensor Systems Workshops (Massw 2019)"	""	""	""	""	""	"7-12"	""	""	""	""	""	""	""	"Machine Learning for Security at the IoT Edge - A Feasibility Study"	"Ieee Int Conf Mob"	"2155-6806"	"10.1109/Massw.2019.00009"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768255900002"	""	""	"machine learning
ai
iot
security
edge"	"Benefits of edge computing include reduced latency and bandwidth savings, privacy-by-default and by-design in compliance with new privacy regulations that encourage sharing only the minimal amount of data. This creates a need for processing data locally rather than sending everything to a cloud environment and performing machine learning there. However, most IoT edge devices are resource-constrained in comparison and it is not evident whether current machine learning methods are directly employable on IoT edge devices. In this paper, we analyze the slate-of-the-art machine learning (ML) algorithms for solving security problems (e.g. intrusion detection) at the edge. Starting from the characteristics and limitations of edge devices in IoT networks, we assess a selected set of commonly used ML algorithms based on four metrics: computation complexity, memory footprint, storage requirement and accuracy. We also compare the suitability of ML algorithms to different cybersecurity problems and discuss the possibility of utilizing these methods for use cases."	"Bs7wm
Times Cited:3
Cited References Count:18
IEEE International Conference on Mobile Ad-hoc and Sensor Systems"	""	"<Go to ISI>://WOS:000768255900002"	""	"RISE Res Inst Sweden, RISE Cybersecur, Gothenburg, Sweden
Ericsson AB, Ericsson Res, Kista, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Nelson; T. Olovsson"	"2016"	"Security and Privacy for Big Data: A Systematic Literature Review"	""	"2016 Ieee International Conference on Big Data (Big Data)"	""	""	""	""	""	"3693-3702"	""	""	""	""	""	""	""	"Security and Privacy for Big Data: A Systematic Literature Review"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000399115003092"	""	""	"differential privacy
efficient
query"	"Big data is currently a hot research topic, with four million hits on Google scholar in October 2016. One reason for the popularity of big data research is the knowledge that can be extracted from analyzing these large data sets. However, data can contain sensitive information, and data must therefore be sufficiently protected as it is stored and processed. Furthermore, it might also be required to provide meaningful, proven, privacy guarantees if the data can be linked to individuals.
To the best of our knowledge, there exists no systematic overview of the overlap between big data and the area of security and privacy. Consequently, this review aims to explore security and privacy research within big data, by outlining and providing structure to what research currently exists. Moreover, we investigate which papers connect security and privacy with big data, and which categories these papers cover. Ultimately, is security and privacy research for big data different from the rest of the research within the security and privacy domain?
To answer these questions, we perform a systematic literature review (SLR), where we collect recent papers from top conferences, and categorize them in order to provide an overview of the security and privacy topics present within the context of big data. Within each category we also present a qualitative analysis of papers representative for that specific area. Furthermore, we explore and visualize the relationship between the categories. Thus, the objective of this review is to provide a snapshot of the current state of security and privacy research for big data, and to discover where further research is required."	"Bh2pl
Times Cited:29
Cited References Count:44"	""	"<Go to ISI>://WOS:000399115003092"	""	"Chalmers Univ Technol, Dept Comp Sci & Engn, Gothenburg, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. F. Xue; C. He; J. Wang; W. Q. Liu"	"2020"	"LOPA: A linear offset based poisoning attack method against adaptive fingerprint authentication system"	""	"Computers & Security"	""	""	"99"	""	""	"102046"	""	""	""	""	"Dec"	""	""	"LOPA: A linear offset based poisoning attack method against adaptive fingerprint authentication system"	"Comput Secur"	"0167-4048"	"ARTN 102046
10.1016/j.cose.2020.102046"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000591706300014"	""	""	"artificial intelligence security
poisoning attacks
adaptive biometric authentication systems
fingerprint authentication
template self-update
biometric systems
minutiae
update"	"Biological characteristics have been widely used in various identity authentication systems. The authentication systems typically store one or several biometric templates to identify whether a claimed user is legitimate. However, since the biological characteristics of users may undergo intra-class variabilities (such as aging or injuring by accidents) as time goes by, those initial enrolled templates may be not able to match the latest characteristics of the users. Therefore, some adaptive systems have been proposed to continuously update the en-rolled templates by using collected run-time data. However, a smart attacker can leverage this self-updating procedure to drift the stored templates by constructing and submitting a set of well-designed poisoning samples. In this paper, for the first time, we propose a novel linear offset based poisoning attack method, named "LOPA", against the online self-update fingerprint authentication systems. By making minor linear transformation to the minutia representation matrix of a victim's fingerprint template, the proposed attack method can generate a series of poisoning samples which are then submitted to the fingerprint authentication system. In this way, the initial template stored in the system will be imperceptibly and stealthily poisoned (i.e., updated), and eventually becomes unusable. Experimental results show that the proposed LOPA method is effective, where the stored fingerprint templates have been successfully poisoned, and those target fingers are incorrectly denied by the target system after a certain time. Specifically, the performance (the average GAR of all target fingers) of the fingerprint authentication system has dropped by 42.86%. In addition, the average matching score and the average matched minutia pairs of all target fingers have both declined, which indicate the universality of the proposed poisoning attack. This work reveals a novel security threat to the fingerprint authentication systems, and can hopefully provide references for developing future countermeasures. (C) 2020 Elsevier Ltd. All rights reserved."	"Ou7li
Times Cited:0
Cited References Count:31"	""	"<Go to ISI>://WOS:000591706300014"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China
Nanjing Univ Aeronaut & Astronaut, Coll Elect & Informat Engn, Nanjing 210016, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Wang; C. R. Tang; H. Huang; H. Wang; J. Q. Li"	"2021"	"Blind identification of convolutional codes based on deep learning"	""	"Digital Signal Processing"	""	""	"115"	""	""	"103086"	""	""	""	""	"Aug"	""	""	"Blind identification of convolutional codes based on deep learning"	"Digit Signal Process"	"1051-2004"	"ARTN 103086
10.1016/j.dsp.2021.103086"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000657761300004"	""	""	"blind identification
convolutional codes
deep learning
convolutional neural network (cnn)
deep residual network (drn)
neural-networks
cyclic codes
recognition
reconstruction"	"Blind identification of channel codes is becoming increasingly important in signal interception and intelligent communication systems. However, most existing channel codes recognition algorithms extract features manually, which makes them highly demanding in real-world application. Thus, efficiently identifying channel codes is difficult using present technologies. This paper presents a deep residual network-based deep learning (DL) approach on the blind identification of convolutional code parameters for a given soft-decision sequence. The proposed method can blindly identify the convolutional codes without the need for the prior information about its coding parameters, and it achieves over 88% of recognition accuracy for 17 forms of convolutional codes when SNR exceeds or equals zero. Furthermore, we investigate factors affecting the accuracy of channel codes recognition including input length, model depth and data type. A comparison of the recognition accuracy between the proposed algorithm, log-likelihood ratio (LLR)-based traditional blind identification algorithm, and DL-based algorithm are then made. Experiment results show that deep residual network-based approaches could provide significant improvements over the traditional algorithm or existing DL-based algorithms in the blind identification of convolutional codes. (C) 2021 Elsevier Inc. All rights reserved."	"Sm7dm
Times Cited:1
Cited References Count:52"	""	"<Go to ISI>://WOS:000657761300004"	""	"Univ Elect Sci & Technol China, Coll Elect Sci & Technol, Chengdu, Peoples R China
Dalian Airforce Commun NCO Acad, Dalian, Peoples R China
UESTC, Chengdu, Peoples R China
Sci & Technol Elect Informat Control Lab, Chengdu, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Wang; F. Y. Lui; Z. Q. Shen; J. Y. Wang; D. Hu; H. G. Xu"	"2019"	"Revealing a Head-on Major Merger in the Nearby NGC 6338 Group with Chandra and VLA Observations"	""	"Astrophysical Journal"	""	""	"870"	""	"2"	"1 - 19"	""	""	""	""	"Jan 10"	""	""	"Revealing a Head-on Major Merger in the Nearby NGC 6338 Group with Chandra and VLA Observations"	"Astrophys J"	"0004-637x"	"ARTN 132
10.3847/1538-4357/aaf234"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000456063900013"	""	""	"galaxies: groups: individual (ngc 6338 group)
ism: kinematics and dynamics
x-rays: galaxies: clusters
early-type galaxies
cold fronts
bow shock
hot gas
dark-matter
cluster
bubbles
core
view
halo"	"By analyzing the Chandra archival data of the nearby NGC 6338 galaxy group, we identify two X-ray bright clumps (N-clump and S-clump) within the central 100 h(73)(-1) kpc, and detect an arc-like X-ray brightness discontinuity at the south boundary of the N-clump, which is defined as a cold front with a gas flow Mach number of M < 0.8. Furthermore, at the northeast boundary of the S-clump (dominated by galaxy NGC 6338) another X-ray edge is detected that corresponds to a weaker cold front. Therefore, the two clumps are approaching each other approximately from opposite directions, and the group is undergoing a head-on collision that is in a stage of pre-core passage. This merger scenario is also supported by the study of the line-of-sight velocity distribution of the group member galaxies. The merger mass ratio is about 1:1.8 as estimated from the central gas temperature of the two clumps, which suggests the merger is most likely to be a major merger. We also analyze the Very Large Array 1.4 and 4.9 GHz radio data, but we do not detect any extended radio emission that is associated with the merger."	"Hh9mw
Times Cited:0
Cited References Count:53"	""	"<Go to ISI>://WOS:000456063900013"	""	"Shanghai Univ Engn Sci, Sch Math Phys & Stat, 333 Longteng Rd, Shanghai 201620, Peoples R China
Chinese Acad Sci, Shanghai Astron Observ, 80 Nandan Rd, Shanghai 200030, Peoples R China
Univ Western Cape, Dept Phys & Astron, ZA-7535 Cape Town, South Africa
Shanghai Jiao Tong Univ, Sch Phys & Astron, 800 Dongchuan Rd, Shanghai 200240, Peoples R China
Shanghai Jiao Tong Univ, Tsung Dao Lee Inst, 800 Dongchuan Rd, Shanghai 200240, Peoples R China
Shanghai Jiao Tong Univ, IFSA Collaborat Innovat Ctr, 800 Dongchuan Rd, Shanghai 200240, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. A. Laharotte; R. Billot; N. E. El Faouzi"	"2020"	"Detecting Dynamic Critical Links within Large Scale Network for Traffic State Prediction"	""	"20th Ieee International Conference on Data Mining Workshops (Icdmw 2020)"	""	""	""	""	""	"820-827"	""	""	""	""	""	""	""	"Detecting Dynamic Critical Links within Large Scale Network for Traffic State Prediction"	"Int Conf Dat Min Wor"	"2375-9232"	"10.1109/Icdmw51313.2020.00119"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000657112800111"	""	""	"feature selection
network
spatiotemporal dynamic
traffic
multi-input multi-output prediction"	"Can we expose the relationship between the physical dynamics of a network and its predictability? To contribute to this point, we propose a dimensionality reduction method for network states prediction based on spatiotemporal data. The method is intended to deal with large scale networks, where only a subset of critical links can be relevant for accurate multidimensional prediction (MIMO) performances. The algorithm is based on Latent Dirichlet Allocation (LDA) to highlight relevant topics in terms of networks dynamics. The feature selection trick relies on the assumption that the most representative links of the most dominant topics are critical links for short term prediction. The method is fully implemented to an original application field: short term road traffic prediction on large scale urban networks based on GPS data. Results highlight significant reductions in dimensionality and execution time, a global improvement of prediction performances as well as a better resilience to non recurrent traffic flow conditions."	"Br5sn
Times Cited:0
Cited References Count:33
International Conference on Data Mining Workshops"	""	"<Go to ISI>://WOS:000657112800111"	""	"Univ Gustave Eiffel, Univ Lyon, UMR LICIT, ENTPE, F-69675 Lyon, France
IMT Atlantique, LUSSI, UMR CNRS Lab STICC DECIDE 6285, CS 83818, Brest, France"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Madani; N. Vlajic"	"2019"	"Near-optimal Evasion of Randomized Convex-inducing Classifiers in Adversarial Environments"	""	"14th International Conference on Availability, Reliability and Security (Ares 2019)"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Near-optimal Evasion of Randomized Convex-inducing Classifiers in Adversarial Environments"	""	""	"10.1145/3339252.3340520"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000552726400032"	""	""	"adverserial machine learning
active learning
evasion
moving target defense"	"Classifiers are often used to detect malicious activities in adversarial environments. Sophisticated adversaries would attempt to find information about deployed classifiers in order to strategise different evasion techniques. It is a widely held belief that randomization of decision boundaries/rules of detection systems would introduce further complexities in attempts made by the adversaries for finding minimal adversarial cost (MAC) evading instances. We have extended the results obtained by Nelson et al. [14] and further presented a novel algorithm that can find optimal evading instances in randomized convex-inducing classifiers using polynomial-many queries. Our results have demonstrated that the complexity introduced through randomization only increases the complexity of finding an optimal evading instance by a constant factor and thus the risk of optimal evasion is still present."	"Bp4jn
Times Cited:0
Cited References Count:18"	""	"<Go to ISI>://WOS:000552726400032"	""	"York Univ, Dept Elect Engn & Comp Sci EECS, Toronto, ON, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. K. Deka; M. H. Bhuyan; Y. Kadobayashi; E. Elmroth"	"2019"	"Adversarial Impact on Anomaly Detection in Cloud Datacenters"	""	"2019 Ieee 24th Pacific Rim International Symposium on Dependable Computing (Prdc 2019)"	""	""	""	""	""	"188-197"	""	""	""	""	""	""	""	"Adversarial Impact on Anomaly Detection in Cloud Datacenters"	"Ieee Pac Rim Int Sym"	"1555-094x"	"10.1109/Prdc47002.2019.00049"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000530687800038"	""	""	"adversarial learning
anomaly detection
poisoning
attack
cloud services
datacenter"	"Cloud datacenters are engineered to meet the requirements of generalised and specialised workloads including mission-critical applications that not only generate tremendous amounts of data traces but also opens opportunities for attackers. The increasing volume and rapid changing behaviour of metric streams (e.g., CPU, network, latency, memory) in the cloud datacenters create difficulties to ensure high availability, security, and performance to cloud service providers. Several anomaly detection techniques have been developed to combat system anomalies in cloud datacenters. By injecting a fraction of well-crafted malicious samples in cloud datacenter traces, attackers can subvert the learning process and results in unacceptable false alarms. These security issues cause threats to all categories of anomaly detection. Hence, it is crucial to assess these techniques against adversaries to improve scalability and robustness. We propose a linear regression-based optimisation framework with the ability to poison data in the training phase and demonstrate its effectiveness on cloud datacenter traces. Finally, we investigate the worst-case analysis of poisoning attacks on robust statistics-based anomaly detection techniques to quantify and assess the detection accuracy. We validate this framework using benchmark resource traces obtained from Yahoo's service cluster as well as traces collected from an experimental testbed with realistic service composition."	"Bo9cc
Times Cited:0
Cited References Count:25
IEEE Pacific Rim International Symposium on Dependable Computing"	""	"<Go to ISI>://WOS:000530687800038"	""	"Galgotias Univ, Sch Comp Sci & Engn, Greater Noida, India
NAIST, Lab Cyber Resilience, Nara 6300192, Japan
Umea Univ, Dept Comp Sci, Umea 90187, Sweden
Assam Kaziranga Univ, Dept Comp Sci & Engn, Jorhat, Assam, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. G. Chrysos; J. Kossaifi; S. Zafeiriou"	"2020"	"RoCGAN: Robust Conditional GAN"	""	"International Journal of Computer Vision"	""	""	"128"	""	"10-11"	"2665-2683"	""	""	""	""	"Nov"	""	""	"RoCGAN: Robust Conditional GAN"	"Int J Comput Vision"	"0920-5691"	"10.1007/s11263-020-01348-5"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000548483300001"	""	""	"conditional gan
unsupervised learning
autoencoder
robust regression
super-resolution
adversarial attacks
cross-noise experiments"	"Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, calledRoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli)."	"Sp. Iss. SI
Ns4ky
Times Cited:2
Cited References Count:67"	""	"<Go to ISI>://WOS:000548483300001"	""	"Imperial Coll London, Dept Comp, 180 Queens Gate, London SW7 2AZ, England
NVIDIA, Santa Clara, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Qayyum; M. Usama; J. Qadir; A. Al-Fuqaha"	"2020"	"Securing Connected & Autonomous Vehicles: Challenges Posed by Adversarial Machine Learning and the Way Forward"	""	"Ieee Communications Surveys and Tutorials"	""	""	"22"	""	"2"	"998-1026"	""	""	""	""	""	""	""	"Securing Connected & Autonomous Vehicles: Challenges Posed by Adversarial Machine Learning and the Way Forward"	"Ieee Commun Surv Tut"	""	"10.1109/Comst.2020.2975048"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000538038400008"	""	""	"autonomous vehicles
security
machine learning
automation
tutorials
robustness
taxonomy
connected and autonomous vehicles
machine
deep learning
adversarial machine learning
adversarial perturbation
perturbation detection
robust machine learning
robustness
attacks
issues
impact"	"Connected and autonomous vehicles (CAVs) will form the backbone of future next-generation intelligent transportation systems (ITS) providing travel comfort, road safety, along with a number of value-added services. Such a transformation-which will be fuelled by concomitant advances in technologies for machine learning (ML) and wireless communications-will enable a future vehicular ecosystem that is better featured and more efficient. However, there are lurking security problems related to the use of ML in such a critical setting where an incorrect ML decision may not only be a nuisance but can lead to loss of precious lives. In this paper, we present an in-depth overview of the various challenges associated with the application of ML in vehicular networks. In addition, we formulate the ML pipeline of CAVs and present various potential security issues associated with the adoption of ML methods. In particular, we focus on the perspective of adversarial ML attacks on CAVs and outline a solution to defend against adversarial attacks in multiple settings."	"Lu9ab
Times Cited:40
Cited References Count:192"	""	"<Go to ISI>://WOS:000538038400008"	""	"Informat Technol Univ, Dept Comp Sci, Lahore 54000, Pakistan
Informat Technol Univ, Elect Engn, Lahore 54000, Pakistan
Hamad Bin Khalifa Univ, Informat & Comp Technol Div, Coll Sci & Engn, Doha, Qatar
Western Michigan Univ, Coll Engn & Appl Sci, Dept Comp Sci, Kalamazoo, MI 49008 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Shu; T. P. Xia; L. Williams; T. Menzies"	"2022"	"Omni: automated ensemble with unexpected models against adversarial evasion attack"	""	"Empirical Software Engineering"	""	""	"27"	""	"1"	"1-32"	""	""	""	""	"Jan"	""	""	"Omni: automated ensemble with unexpected models against adversarial evasion attack"	"Empir Softw Eng"	"1382-3256"	"ARTN 26
10.1007/s10664-021-10064-8"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000724001300001"	""	""	"hyperparameter optimization
ensemble defense
adversarial evasion attack"	"Context Machine learning-based security detection models have become prevalent in modern malware and intrusion detection systems. However, previous studies show that such models are susceptible to adversarial evasion attacks. In this type of attack, inputs (i.e., adversarial examples) are specially crafted by intelligent malicious adversaries, with the aim of being misclassified by existing state-of-the-art models (e.g., deep neural networks). Once the attackers can fool a classifier to think that a malicious input is actually benign, they can render a machine learning-based malware or intrusion detection system ineffective. Objective To help security practitioners and researchers build a more robust model against non-adaptive, white-box and non-targeted adversarial evasion attacks through the idea of ensemble model. Method We propose an approach called Omni, the main idea of which is to explore methods that create an ensemble of "unexpected models"; i.e., models whose control hyperparameters have a large distance to the hyperparameters of an adversary's target model, with which we then make an optimized weighted ensemble prediction. Results In studies with five types of adversarial evasion attacks (FGSM, BIM, JSMA, DeepFool and Carlini-Wagner) on five security datasets (NSL-KDD, CIC-IDS-2017, CSE-CIC-IDS2018, CICAndMal2017 and the Contagio PDF dataset), we show Omni is a promising approach as a defense strategy against adversarial attacks when compared with other baseline treatments. Conclusions When employing ensemble defense against adversarial evasion attacks, we suggest to create ensemble with unexpected models that are distant from the attacker's expected model (i.e., target model) through methods such as hyperparameter optimization."	"Xf3vg
Times Cited:0
Cited References Count:95"	""	"<Go to ISI>://WOS:000724001300001"	""	"North Carolina State Univ, Dept Comp Sci, Raleigh, NC 27695 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Korycki; B. Krawczyk"	"2022"	"Adversarial concept drift detection under poisoning attacks for robust data stream mining"	""	"Mach Learn"	""	""	""	""	""	"1-36"	""	""	""	"2022/06/08"	"Jun 2"	""	""	"Adversarial concept drift detection under poisoning attacks for robust data stream mining"	""	"0885-6125 (Print)
0885-6125 (Linking)"	"10.1007/s10994-022-06177-w"	""	""	""	""	"PMC9162121"	""	""	""	""	""	""	"35668720"	""	""	"Adversarial learning
Boltzmann machine
Concept drift
Data stream mining
Poisoning attacks
Robust machine learning
financial interests or personal relationships that could have appeared to
influence the work reported in this paper."	"Continuous learning from streaming data is among the most challenging topics in the contemporary machine learning. In this domain, learning algorithms must not only be able to handle massive volume of rapidly arriving data, but also adapt themselves to potential emerging changes. The phenomenon of evolving nature of data streams is known as concept drift. While there is a plethora of methods designed for detecting its occurrence, all of them assume that the drift is connected with underlying changes in the source of data. However, one must consider the possibility of a malicious injection of false data that simulates a concept drift. This adversarial setting assumes a poisoning attack that may be conducted in order to damage the underlying classification system by forcing an adaptation to false data. Existing drift detectors are not capable of differentiating between real and adversarial concept drift. In this paper, we propose a framework for robust concept drift detection in the presence of adversarial and poisoning attacks. We introduce the taxonomy for two types of adversarial concept drifts, as well as a robust trainable drift detector. It is based on the augmented restricted Boltzmann machine with improved gradient computation and energy function. We also introduce Relative Loss of Robustness-a novel measure for evaluating the performance of concept drift detectors under poisoning attacks. Extensive computational experiments, conducted on both fully and sparsely labeled data streams, prove the high robustness and efficacy of the proposed drift detection framework in adversarial scenarios."	"Korycki, Lukasz
Krawczyk, Bartosz
eng
Mach Learn. 2022 Jun 2:1-36. doi: 10.1007/s10994-022-06177-w."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35668720"	""	"Department of Computer Science, Virginia Commonwealth University, Richmond, VA USA.grid.224260.00000 0004 0458 8737"	""	""	""	""	""	""	""	""
"Journal Article"	"Y. J. Zhang; G. D. Bai; M. Y. Zhong; X. Li; R. K. L. Ko"	"2021"	"Differentially Private Collaborative Coupling Learning for Recommender Systems"	""	"Ieee Intelligent Systems"	""	""	"36"	""	"1"	"16-23"	""	""	""	""	"Jan 1"	""	""	"Differentially Private Collaborative Coupling Learning for Recommender Systems"	"Ieee Intell Syst"	"1541-1672"	"10.1109/Mis.2020.3005930"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000631200900003"	""	""	"couplings
learning systems
differential privacy
training data
mathematical model
intelligent systems
collaborative work
privacy
recommender systems
coupling learning
differential privacy
collaborative learning"	"Coupling learning is designed to estimate, discover, and extract the interactions and relationships among learning components. It provides insights into complex interactive data, and has been extensively incorporated into recommender systems to enhance the interpretability of sophisticated relationships between users and items. Coupling learning can be further fostered once the trending collaborative learning can be engaged to take advantage of the cross-platform data. To facilitate this, privacy-preserving solutions are in high demand-it is desired that the collaboration should not expose either the private data of each individual owner or the model parameters trained on their datasets. In this article, we develop a distributed collaborative coupling learning system, which enables differential privacy. The proposed system defends against the adversary who has gained full knowledge of the training mechanism and the access to the model trained collaboratively. It also addresses the privacy-utility tradeoff by a provable tight sensitivity bound. Our experiments demonstrate that the proposed system guarantees favorable privacy gains at a modest cost in recommendation quality, even in scenarios with a large number of training epochs."	"Ra1ts
Times Cited:1
Cited References Count:20"	""	"<Go to ISI>://WOS:000631200900003"	""	"Univ Queensland, Sch Informat Technol & Elect Engn, St Lucia, Qld 4072, Australia
Univ Queensland, St Lucia, Qld 4072, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Y. Chen; J. Li"	"2019"	"Data Poisoning Attacks on Cross-domain Recommendation"	""	"Proceedings of the 28th Acm International Conference on Information & Knowledge Management (Cikm '19)"	""	""	""	""	""	"2177-2180"	""	""	""	""	""	""	""	"Data Poisoning Attacks on Cross-domain Recommendation"	""	""	"10.1145/3357384.3358116"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000539898202040"	""	""	""	"Cross-domain recommendation has attracted growing interests given their simplicity and effectiveness. In the cross-domain scenarios, we may improve predictive accuracy in one domain by transferring knowledge from the other, which alleviates the data sparsity issue. However, the relatedness of these domains can be exploited by a malicious party to launch data poisoning attacks. Here we study the vulnerability of cross-domain recommendation under data poisoning attacks. We show that data poisoning attacks can be formulated as a bilevel optimization problem. Our experimental results show that cross-domain system can be compromised under attacks, highlighting the need for countermeasures against data poisoning attacks in cross-domain recommendation."	"Bp1kl
Times Cited:1
Cited References Count:16"	""	"<Go to ISI>://WOS:000539898202040"	""	"Case Western Reserve Univ, EECS, Cleveland, OH 44106 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Moustafa; K. K. R. Choo; I. Radwan; S. Camtepe"	"2019"	"Outlier Dirichlet Mixture Mechanism: Adversarial Statistical Learning for Anomaly Detection in the Fog"	""	"Ieee Transactions on Information Forensics and Security"	""	""	"14"	""	"8"	"1975-1987"	""	""	""	""	"Aug"	""	""	"Outlier Dirichlet Mixture Mechanism: Adversarial Statistical Learning for Anomaly Detection in the Fog"	"Ieee T Inf Foren Sec"	"1556-6013"	"10.1109/Tifs.2018.2890808"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000467523400002"	""	""	"adversarial statistical/machine learning
outlier detection
dirichlet mixture model
anomaly detection
fog computing
intrusion detection system
cloud
security
prevention
attacks
models"	"Current anomaly detection systems (ADSs) apply statistical and machine learning algorithms to discover zero-day attacks, but such algorithms are vulnerable to advanced persistent threat actors. In this paper, we propose an adversarial statistical learning mechanism for anomaly detection, outlier Dirichlet mixture-based ADS (ODM-ADS), which has three new capabilities. First, it can self-adapt against data poisoning attacks that inject malicious instances in the training phase for disrupting the learning process. Second, it establishes a statistical legitimate profile and considers variations from the baseline of the profile as anomalies using a proposed outlier function. Third, to deal with dynamic and large-scale networks such as Internet of Things and cloud and fog computing, we suggest a framework for deploying the mechanism as Software as a Service in the fog nodes. The fog enables the proposed mechanism to concurrently process streaming data at the edge of the network. The ODM-ADS mechanism is evaluated using both NSL-KDD and UNSW-NB15 datasets, whose findings indicate that ODM-ADS outperforms seven other peer algorithms in terms of accuracy, detection rates, false positive rates, and computational time."	"Hx6oq
Times Cited:46
Cited References Count:66"	""	"<Go to ISI>://WOS:000467523400002"	""	"Univ New South Wales, Sch Engn & Informat Technol, ADFA, Canberra, ACT 2612, Australia
Univ Texas San Antonio, Dept Informat Syst & Cyber Secur, San Antonio, TX 78249 USA
Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA
Univ South Australia, Sch Informat Technol & Math Sci, Adelaide, SA 5095, Australia
Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 0200, Australia
Australian Natl Univ, Coll Business & Econ, Canberra, ACT 0200, Australia
CSIRO, Data61, Marsfield, NSW 2122, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Capra; B. Bussolino; A. Marchisio; G. Masera; M. Martina; M. Shafique"	"2020"	"Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead"	""	"Ieee Access"	""	""	"8"	""	""	"225134-225180"	""	""	""	""	""	""	""	"Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3039858"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000603734400001"	""	""	"hardware
neurons
biological neural networks
computer architecture
computational modeling
field programmable gate arrays
training
machine learning
ml
artificial intelligence
ai
deep learning
deep neural networks
dnns
convolutional neural networks
cnns
capsule networks
spiking neural networks
vlsi
computer architecture
hardware accelerator
adversarial attacks
data flow
optimization
efficiency
performance
power consumption
energy
area
latency
model compression
space exploration
spiking neurons
efficient
memory
architectures
processor
design
methodology
coprocessor"	"Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them."	"Pm3ws
Times Cited:15
Cited References Count:334"	""	"<Go to ISI>://WOS:000603734400001"	""	"Politecn Torino, Dept Elect Elect & Telecommun Engn, I-10129 Turin, Italy
Tech Univ Wien TU Wien, Inst Comp Engn, A-1040 Vienna, Austria
New York Univ, Div Engn, Abu Dhabi, U Arab Emirates"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Sabottke; D. Chen; L. Layman; T. Dumitras"	"2019"	"How to trick the Borg: threat models against manual and automated techniques for detecting network attacks"	""	"Computers & Security"	""	""	"81"	""	""	"25-40"	""	""	""	""	"Mar"	""	""	"How to trick the Borg: threat models against manual and automated techniques for detecting network attacks"	"Comput Secur"	"0167-4048"	"10.1016/j.cose.2018.07.022"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000458595300003"	""	""	"cyber attack
human factors
unsupervised learning
dbscan
log analysis
heuristics
alarms"	"Cyber attackers constantly craft new attacks previously unknown to the security community. There are two approaches for detecting such attacks: (1) employing human analysts who can observe the data and identify anomalies that correspond to malicious intent; and (2) utilizing unsupervised automated techniques, such as clustering, that do not rely on ground truth. We conduct a security analysis of the two approaches, utilizing attacks against a real-world website. Through two experiments-a user study with 65 security analysts and an experimental analysis of attack discovery using DBSCAN clustering-we compare the strategies and features employed by human analysts and clustering system for detecting attacks. Building on these observations, we propose threat models for the human analysis process and for the unsupervised techniques when operating in adversarial settings. Based on our analysis, we propose and evaluate two attacks against the DBSCAN clustering algorithm and a defense. Finally, we discuss the implications of our insights for hybrid systems that utilize the strengths of automation and of human analysis to complement their respective weaknesses. (C) 2018 Elsevier Ltd. All rights reserved."	"Hl3ei
Times Cited:2
Cited References Count:35"	""	"<Go to ISI>://WOS:000458595300003"	""	"Univ Maryland, Inst Adv Comp Studies, 2126 AVW Bldg, College Pk, MD 20742 USA
Fraunhofer Ctr Expt Software Engn, 5825 Univ Res Ct,Suite 1300, College Pk, MD 20740 USA
Univ N Carolina, CIS Bldg,601 S Coll Rd, Wilmington, NC 28403 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. L. He; Y. Zhang"	"2021"	"Quantifying and Mitigating Privacy Risks of Contrastive Learning"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"845-863"	""	""	""	""	""	""	""	"Quantifying and Mitigating Privacy Risks of Contrastive Learning"	""	""	"10.1145/3460120.3484571"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478300053"	""	""	"contrastive learning
membership inference attacks
attribute inference attacks
privacy-preserving machine learning"	"Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well.
In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.(1)"	"Bs7xr
Times Cited:2
Cited References Count:67"	""	"<Go to ISI>://WOS:000768478300053"	""	"CISPA Helmholtz Ctr Informat Secur, Saarbrucken, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Z. Ma; X. J. Zhu; J. Hsu"	"2019"	"Data Poisoning against Differentially-Private Learners: Attacks and Defenses"	""	"Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence"	""	""	"abs/1903.09860"	""	""	"4732-4738"	""	""	""	""	""	""	""	"Data Poisoning against Differentially-Private Learners: Attacks and Defenses"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000761735104122"	""	""	""	"Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that private learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary is allowed to poison more data. We emprically evaluate this protection by designing attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items."	"Bs7ht
Times Cited:16
Cited References Count:22"	""	"<Go to ISI>://WOS:000761735104122"	""	"Univ Wisconsin, Madison, WI 53706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"O. Kayode; A. S. Tosun"	"2020"	"Deep Q-Network for Enhanced Data Privacy and Security of IoT Traffic"	""	"2020 Ieee 6th World Forum on Internet of Things (Wf-Iot)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Deep Q-Network for Enhanced Data Privacy and Security of IoT Traffic"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000627822200103"	""	""	"internet of things (iot)
data privacy
security
deep reinforcement learning
distributed machine learning
industrial iot"	"Data privacy and security of Internet enabled devices has become a major concern of many users and manufacturers. The proliferation of Internet of Things (IoT) devices and the increasing network traffic have heightened the attack surface. Proxy and man-in-the-middle attacks can be used to exploit vulnerability inherent in IoT devices. Due to the limited resources and features of IoT devices, transmitted data might be at risk of traffic interception and possible decryption of encrypted data on the fly using these attacks. In this paper, we discuss a vulnerability in IoT communication and propose an effective approach based on Deep Q-Network (DQN) and Generative Adversarial Network (GAN) for proxy detection. Our main aim is a robust detection mechanism using network connection information. We further propose a use case for distributed machine learning suitable for real-time monitoring and proxy detection."	"Br0bz
Times Cited:0
Cited References Count:27"	""	"<Go to ISI>://WOS:000627822200103"	""	"Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Krishnan; M. J. Franklin; K. Goldberg; J. N. Wang; E. Wu"	"2016"	"ActiveClean: An Interactive Data Cleaning Framework For Modern Machine Learning"	""	"Sigmod'16: Proceedings of the 2016 International Conference on Management of Data"	""	""	""	""	""	"2117-2120"	""	""	""	""	""	""	""	"ActiveClean: An Interactive Data Cleaning Framework For Modern Machine Learning"	""	""	"10.1145/2882903.2899409"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000452538600150"	""	""	""	"Databases can be corrupted with various errors such as missing, incorrect, or inconsistent values. Increasingly, modern data analysis pipelines involve Machine Learning, and the effects of dirty data can be difficult to debug. Dirty data is often sparse, and naive sampling solutions are not suited for high-dimensional models. We propose ActiveClean, a progressive framework for training Machine Learning models with data cleaning. Our framework updates a model iteratively as the analyst cleans small batches of data, and includes numerous optimizations such as importance weighting and dirty data detection. We designed a visual interface to wrap around this framework and demonstrate ActiveClean for a video classification problem and a topic modeling problem."	"Bl5ri
Times Cited:22
Cited References Count:13"	""	"<Go to ISI>://WOS:000452538600150"	""	"Univ Calif Berkeley, Berkeley, CA 94720 USA
Simon Fraser Univ, Burnaby, BC, Canada
Columbia Univ, New York, NY 10027 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Pestana; W. Liu; D. Glance; A. Mian"	"2021"	"Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty"	""	"2021 Ieee Winter Conference on Applications of Computer Vision (Wacv 2021)"	""	""	""	""	""	"556-565"	""	""	""	""	""	""	""	"Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty"	"Ieee Wint Conf Appl"	"2472-6737"	"10.1109/Wacv48630.2021.00060"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000692171000056"	""	""	""	"Dataset bias is a problem in adversarial machine learning, especially in the evaluation of defenses. An adversarial attack or defense algorithm may show better results on the reported dataset than can be replicated on other datasets. Even when two algorithms are compared, their relative performance can vary depending on the dataset. Deep learning offers state-of-the-art solutions for image recognition, but deep models are vulnerable even to small perturbations. Research in this area focuses primarily on adversarial attacks and defense algorithms. In this paper, we report for the first time, a class of robust images that are both resilient to attacks and that recover better than random images under adversarial attacks using simple defense techniques. Thus, a test dataset with a high proportion of robust images gives a misleading impression about the performance of an adversarial attack or defense. We propose three metrics to determine the proportion of robust images in a dataset and provide scoring to determine the dataset bias. We also provide an ImageNet-R dataset of 15000+ robust images to facilitate further research on this intriguing phenomenon of image strength under attack. Our dataset, combined with the proposed metrics, is valuable for unbiased benchmarking of adversarial attack and defense algorithms."	"Bs1kx
Times Cited:2
Cited References Count:41
IEEE Winter Conference on Applications of Computer Vision"	""	"<Go to ISI>://WOS:000692171000056"	""	"Univ Western Australia, 35 Stirling Hwy, Crawley, WA 6009, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Bond-Taylor; A. Leach; Y. Long; C. G. Willcocks"	"2021"	"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models"	""	"IEEE Trans Pattern Anal Mach Intell"	""	""	"PP"	""	""	""	""	""	""	"2021/10/01"	"Sep 30"	""	""	"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models"	""	"1939-3539 (Electronic)
0098-5589 (Linking)"	"10.1109/TPAMI.2021.3116668"	""	""	""	""	""	""	""	""	""	""	""	"34591756"	""	""	""	"Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations."	"Bond-Taylor, Sam
Leach, Adam
Long, Yang
Willcocks, Chris G
eng
MR/S003916/1/MRC_/Medical Research Council/United Kingdom
MR/S003916/2/MRC_/Medical Research Council/United Kingdom
IEEE Trans Pattern Anal Mach Intell. 2021 Sep 30;PP. doi: 10.1109/TPAMI.2021.3116668."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34591756"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"D. Pasquini; M. Mingione; M. Bernaschi"	"2019"	"Adversarial Out-domain Examples for Generative Models"	""	"2019 4th Ieee European Symposium on Security and Privacy Workshops (Euros&Pw)"	""	""	"abs/1903.02926"	""	""	"272-280"	""	""	""	""	""	""	""	"Adversarial Out-domain Examples for Generative Models"	""	""	"10.1109/EuroSPW.2019.00037"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000485315600031"	""	""	"generative adversarial models
attacks against machine learning
adversarial input"	"Deep generative models are rapidly becoming a common tool for researchers and developers. However, as exhaustively shown for the family of discriminative models, the test-time inference of deep neural networks cannot be fully controlled and erroneous behaviors can be induced by an attacker. In the present work, we show how a malicious user can force a pre-trained generator to reproduce arbitrary data instances by feeding it suitable adversarial inputs. Moreover, we show that these adversarial latent vectors can be shaped so as to be statistically indistinguishable from the set of genuine inputs. The proposed attack technique is evaluated with respect to various GAN images generators using different architectures, training processes and for both conditional and not-conditional setups."	"Bn6iw
Times Cited:2
Cited References Count:37"	""	"<Go to ISI>://WOS:000485315600031"	""	"Sapienza Univ, Dept Comp Sci, Rome, Italy
Sapienza Univ, Dept Stat, Rome, Italy
CNR, IAC, Rome, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. M. Liu; L. H. Xie; Y. P. Wang; J. Zou; J. B. Xiong; Z. B. Ying; A. V. Vasilakos"	"2021"	"Privacy and Security Issues in Deep Learning: A Survey"	""	"Ieee Access"	""	""	"9"	""	""	"4566-4593"	""	""	""	""	""	""	""	"Privacy and Security Issues in Deep Learning: A Survey"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3045078"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000607679500001"	""	""	"deep learning
dl privacy
dl security
model extraction attack
model inversion attack
adversarial attack
poisoning attack
adversarial defense
privacy-preserving
differential evolution
neural-networks
efficient
attacks"	"Deep Learning (DL) algorithms based on artificial neural networks have achieved remarkable success and are being extensively applied in a variety of application domains, ranging from image classification, automatic driving, natural language processing to medical diagnosis, credit risk assessment, intrusion detection. However, the privacy and security issues of DL have been revealed that the DL model can be stolen or reverse engineered, sensitive training data can be inferred, even a recognizable face image of the victim can be recovered. Besides, the recent works have found that the DL model is vulnerable to adversarial examples perturbed by imperceptible noised, which can lead the DL model to predict wrongly with high confidence. In this paper, we first briefly introduces the four types of attacks and privacy-preserving techniques in DL. We then review and summarize the attack and defense methods associated with DL privacy and security in recent years. To demonstrate that security threats really exist in the real world, we also reviewed the adversarial attacks under the physical condition. Finally, we discuss current challenges and open problems regarding privacy and security issues in DL."	"Ps1gu
Times Cited:36
Cited References Count:217"	""	"<Go to ISI>://WOS:000607679500001"	""	"Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350108, Peoples R China
Fuzhou Univ, Fujian Prov Key Lab Informat Secur Network Syst, Fuzhou 350108, Peoples R China
Fujian Normal Univ, Coll Math & Informat, Fujian Prov Key Lab Network Secur & Cryptol, Fuzhou 350117, Peoples R China
Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore
Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia
Lulea Univ Technol, Dept Comp Sci Elect & Space Engn, S-97187 Lulea, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. H. Wu; M. Nekovee; Y. Wang"	"2020"	"Deep Learning-Based Autoencoder for m-User Wireless Interference Channel Physical Layer Design"	""	"Ieee Access"	""	""	"8"	""	""	"174679-174691"	""	""	""	""	""	""	""	"Deep Learning-Based Autoencoder for m-User Wireless Interference Channel Physical Layer Design"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3025597"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000575910000001"	""	""	"deep learning
autoencoder
5g physical layer
interference channel
capacity
5g
coordination
management
networks"	"Deep learning (DL) based autoencoder (AE) has been proposed recently as a promising, and potentially disruptive approach to design the physical layer of beyond-5G communication systems. Compared to a traditional communication system with a multiple-block structure, the DL based AE approach provides a new paradigm to physical layer design with a pure data-driven and end-to-end learning based solution. In this article, we address the dynamic interference in a multi-user Gaussian interference channel. We show that standard constellation are not optimal for this context, in particular, for a high interference condition. We propose a novel adaptive DL based AE to overcome this problem. With our approach, dynamic interference can be learned and predicted, which updates the learning processing for the decoder. Compared to other machine learning approaches, our method does not rely on a fixed training function, but is adaptive and applicable to practical systems. In comparison with the conventional system using -n-psk or n-QAM modulation schemes with zero force (ZF) and minimum mean square error (MMSE) equalizer, the proposed adaptive deep learning (ADL) based AE demonstrates a significant achievable BER in the presence of interference, especially in strong and very strong interference scenarios. The proposed approach has laid the foundation of enabling adaptable constellation for 5G and beyond communication systems, where dynamic and heterogeneous network conditions are envisaged."	"Nx7tz
Times Cited:3
Cited References Count:38"	""	"<Go to ISI>://WOS:000575910000001"	""	"Univ Sussex, Ctr Adv Commun Mobile Technol & IoT, Brighton BN1 9QJ, E Sussex, England
Quantrom Technol Ltd, London SE21 8DU, England
Samsung Res & Dev Inst, Network Stand & Res Commun Res Grp, London TW18 4QE, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. R. Manoj; M. Sadeghi; E. G. Larsson"	"2021"	"Adversarial Attacks on Deep Learning Based Power Allocation in a Massive MIMO Network"	""	"Ieee International Conference on Communications (Icc 2021)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Adversarial Attacks on Deep Learning Based Power Allocation in a Massive MIMO Network"	"Ieee Icc"	"1550-3607"	"10.1109/Icc42927.2021.9500424"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000719386001025"	""	""	"adversarial attacks
deep learning
massive mimo
neural networks
power allocation
wireless security"	"Deep learning (DL) is becoming popular as a new tool for many applications in wireless communication systems. However, for many classification tasks (e.g., modulation classification) it has been shown that DL-based wireless systems are susceptible to adversarial examples; adversarial examples are well-crafted malicious inputs to the neural network (NN) with the objective to cause erroneous outputs. In this paper, we extend this to regression problems and show that adversarial attacks can break DL-based power allocation in the downlink of a massive multiple-input-multiple-output (maMIMO) network. Specifically, we extend the fast gradient sign method (FGSM), momentum iterative FGSM, and projected gradient descent adversarial attacks in the context of power allocation in a maMIMO system. We benchmark the performance of these attacks and show that with a small perturbation in the input of the NN, the white-box attacks can result in infeasible solutions up to 86%. Furthermore, we investigate the performance of black-box attacks. All the evaluations conducted in this work are based on an open dataset and NN models, which are publicly available."	"Bs4hx
Times Cited:2
Cited References Count:21
IEEE International Conference on Communications"	""	"<Go to ISI>://WOS:000719386001025"	""	"Linkoping Univ, Dept Elect Engn ISY, Linkoping, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. A. Babu; P. M. Ameer"	"2020"	"Physical Adversarial Attacks Against Deep Learning Based Channel Decoding Systems"	""	"2020 Ieee Region 10 Symposium (Tensymp) - Technology for Impactful Sustainable Development"	""	""	""	""	""	"1511-1514"	""	""	""	""	""	""	""	"Physical Adversarial Attacks Against Deep Learning Based Channel Decoding Systems"	"Ieee Region 10 Symp"	"2640-821x"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000652007300360"	""	""	"adversarial attacks
channel decoding
deep learning
wireless security
neural networks"	"Deep Learning (DL), in spite of its huge success in many new fields, is extremely vulnerable to adversarial attacks. We demonstrate how an attacker applies physical white-box and black-box adversarial attacks to Channel decoding systems based on DL. We show that these attacks can affect the systems and decrease performance. We uncover that these attacks are more effective than conventional jamming attacks. Additionally, we show that classical decoding schemes are more robust than the deep learning channel decoding systems in the presence of both adversarial and jamming attacks."	"Br4nf
Times Cited:0
Cited References Count:9
IEEE Region 10 Symposium"	""	"<Go to ISI>://WOS:000652007300360"	""	"NIT Calicut, ECE Dept, Kozhikode, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Papernot; P. McDaniel; X. Wu; S. Jha; A. Swami"	"2016"	"Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks"	""	"2016 Ieee Symposium on Security and Privacy (Sp)"	""	""	""	""	""	"582-597"	""	""	""	""	""	""	""	"Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks"	"P Ieee S Secur Priv"	"1081-6011"	"10.1109/Sp.2016.41"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000387292800033"	""	""	"security"	"Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10(30). We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested."	"Bg2eq
Times Cited:805
Cited References Count:44
IEEE Symposium on Security and Privacy"	""	"<Go to ISI>://WOS:000387292800033"	""	"Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA
Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA
US Army, Res Lab, Adelphi, MD USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Z. He; G. Z. Meng; K. Chen; X. B. Hu; J. W. He"	"2022"	"Towards Security Threats of Deep Learning Systems: A Survey"	""	"Ieee Transactions on Software Engineering"	""	""	"48"	""	"5"	"1743-1770"	""	""	""	""	"May 1"	""	""	"Towards Security Threats of Deep Learning Systems: A Survey"	"Ieee T Software Eng"	"0098-5589"	"10.1109/Tse.2020.3034721"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000797454000018"	""	""	"deep learning
security
data models
privacy
predictive models
training data
deep learning
poisoning attack
adversarial attack
model extraction attack
model inversion attack
adversarial attacks
robustness
privacy
noise"	"Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning's wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area."	"1i8cj
Times Cited:2
Cited References Count:277"	""	"<Go to ISI>://WOS:000797454000018"	""	"Chinese Acad Sci, Inst Informat Engn, Beijing 100864, Peoples R China
Univ Chinese Acad Sci, Sch Cybersecur, Beijing 100864, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Y. Meng; H. Chen"	"2017"	"MagNet: a Two-Pronged Defense against Adversarial Examples"	""	"Ccs'17: Proceedings of the 2017 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"135-147"	""	""	""	""	""	""	""	"MagNet: a Two-Pronged Defense against Adversarial Examples"	""	""	"10.1145/3133956.3134057"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000440307700010"	""	""	"adversarial example
neural network
autoencoder"	"Deep learning has shown impressive performance on hard perceptual problems. However, researchers found deep learning systems to be vulnerable to small, specially crafted perturbations that are imperceptible to humans. Such perturbations cause deep learning systems to mis-classify adversarial examples, with potentially disastrous consequences where safety or security is crucial. Prior defenses against adversarial examples either targeted specific attacks or were shown to be ineffective.
We propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet neither modifies the protected classifier nor requires knowledge of the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. The detector networks learn to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since they assume no specific process for generating adversarial examples, they generalize well. The reformer network moves adversarial examples towards the manifold of normal examples, which is effective for correctly classifying adversarial examples with small perturbation. We discuss the intrinsic difficulties in defending against whitebox attack and propose a mechanism to defend against gray-box attack. Inspired by the use of randomness in cryptography, we use diversity to strengthen MagNet. We show empirically that MagNet is effective against the most advanced state-of-the-art attacks in blackbox and graybox scenarios without sacrificing false positive rate on normal examples."	"Bk6gz
Times Cited:313
Cited References Count:37"	""	"<Go to ISI>://WOS:000440307700010"	""	"ShanghaiTech Univ, Shanghai, Peoples R China
Univ Calif Davis, Davis, CA 95616 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Akhtar; A. Mian"	"2018"	"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"	""	"Ieee Access"	""	""	"6"	""	""	"14410-14430"	""	""	""	""	""	""	""	"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"	"Ieee Access"	"2169-3536"	"10.1109/Access.2018.2807385"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000429250000001"	""	""	"deep learning
adversarial perturbation
black-box attack
white-box attack
adversarial learning
perturbation detection"	"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction."	"Gb7jf
Times Cited:502
Cited References Count:182"	""	"<Go to ISI>://WOS:000429250000001"	""	"Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. J. Hu; M. J. Yu; Q. Z. Xu; J. Gao"	"2020"	"Classifiers Protected against Attacks by Fusion of Multi-Branch Perturbed GAN"	""	"Mobile Networks & Applications"	""	""	"25"	""	"6"	"2321-2335"	""	""	""	""	"Dec"	""	""	"Classifiers Protected against Attacks by Fusion of Multi-Branch Perturbed GAN"	"Mobile Netw Appl"	"1383-469x"	"10.1007/s11036-020-01618-z"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000551370500001"	""	""	"generative adversarial networks
adversarial perturbation
adversarial learning
black-box & white-box attack
space-like hypersurfaces
minimal graph
level sets
convexity"	"Deep learning is widely used in classification tasks to achieve advanced performance. However, in the face of well-designed image classifications, such as the Fast Gradient Sign Method (FGSM), there are glaring errors. This paper proposes a new technique to eliminate interference using generative adversarial networks (GAN), called multi-branch perturbed generative adversarial networks(MBP-GAN). MBP-GAN minimizes the input feature flow graph in generator noise filtering by introducing multi-branch fusion perturbations. This makes the sample of the generator more aware of this perturbation, thereby improving the ability of the generator and discriminator to resist classification against attacks in combat training. Through this kind of training, this model can be used as a defense against arbitrary attacks. Then we design the loss function, so that the generator and the discriminator can keep accurate results for general images and classification against images. We verify our experimental results on the MNIST, F-MNIST and CelebA datasets. The results show that the MBP-GAN can effectively eliminate the interference from the classification against the attack."	"Sp. Iss. SI
Ox3bf
Times Cited:0
Cited References Count:35"	""	"<Go to ISI>://WOS:000551370500001"	""	"Guangdong Polytechn Normal Univ, Acad Math & Syst Sci, Guangzhou 510665, Peoples R China
South China Normal Univ, Sch Comp Sci, Guangzhou 510631, Peoples R China
Inner Mongolia Agr Univ, Coll Comp & Informat Engn, Hohhot 010018, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. V. Massoli; F. Carrara; G. Amato; F. Falchi"	"2021"	"Detection of Face Recognition Adversarial Attacks"	""	"Computer Vision and Image Understanding"	""	""	"202"	""	""	""	""	""	""	""	"Jan"	""	""	"Detection of Face Recognition Adversarial Attacks"	"Comput Vis Image Und"	"1077-3142"	"ARTN 103103
10.1016/j.cviu.2020.103103"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000616091100009"	""	""	"deep learning
face recognition
adversarial attacks
adversarial detection
adversarial biometrics"	"Deep Learning methods have become state-of-the-art for solving tasks such as Face Recognition (FR). Unfortunately, despite their success, it has been pointed out that these learning models are exposed to adversarial inputs - images to which an imperceptible amount of noise for humans is added to maliciously fool a neural network - thus limiting their adoption in sensitive real-world applications. While it is true that an enormous effort has been spent to train robust models against this type of threat, adversarial detection techniques have recently started to draw attention within the scientific community. The advantage of using a detection approach is that it does not require to re-train any model; thus, it can be added to any system. In this context, we present our work on adversarial detection in forensics mainly focused on detecting attacks against FR systems in which the learning model is typically used only as features extractor. Thus, training a more robust classifier might not be enough to counteract the adversarial threats.
In this frame, the contribution of our work is four-fold: (i) we test our proposed adversarial detection approach against classification attacks, i.e., adversarial samples crafted to fool an FR neural network acing as a classifier; (ii) using a k-Nearest Neighbor (k-NN) algorithm as a guide, we generate deep features attacks against an FR system based on a neural network acing as features extractor, followed by a similarity-based procedure which returns the query identity; (iii) we use the deep features attacks to fool an FR system on the 1:1 face verification task, and we show their superior effectiveness with respect to classification attacks in evading such type of system; (iv) we use the detectors trained on the classification attacks to detect the deep features attacks, thus showing that such approach is generalizable to different classes of offensives."	"Qe3dy
Times Cited:9
Cited References Count:59"	""	"<Go to ISI>://WOS:000616091100009"	""	"ISTI CNR, Via G Moruzzi 1, I-56124 Pisa, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Z. Xu; M. Marwah; M. Arlitt; N. Ramakrishnan"	"2021"	"STAN: Synthetic Network Traffic Generation with Generative Neural Models"	""	"Deployable Machine Learning for Security Defense, Mlhat 2021"	""	""	"1482"	""	""	"3-29"	""	""	""	""	""	""	""	"STAN: Synthetic Network Traffic Generation with Generative Neural Models"	"Comm Com Inf Sc"	"1865-0929"	"10.1007/978-3-030-87839-9_1"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000791032800001"	""	""	""	"Deep learning models have achieved great success in recent years but progress in some domains like cybersecurity is stymied due to a paucity of realistic datasets. Organizations are reluctant to share such data, even internally, due to privacy reasons. An alternative is to use synthetically generated data but existing methods are limited in their ability to capture complex dependency structures, between attributes and across time. This paper presents STAN (Synthetic network Traffic generation with Autoregressive Neural models), a tool to generate realistic synthetic network traffic datasets for subsequent downstream applications. Our novel neural architecture captures both temporal dependencies and dependence between attributes at any given time. It integrates convolutional neural layers with mixture density neural layers and softmax layers, and models both continuous and discrete variables. We evaluate the performance of STAN in terms of quality of data generated, by training it on both a simulated dataset and a real network traffic data set. Finally, to answer the question-can real network traffic data be substituted with synthetic data to train models of comparable accuracy?-we train two anomaly detection models based on self-supervision. The results show only a small decline in accuracy of models trained solely on synthetic data. While current results are encouraging in terms of quality of data generated and absence of any obvious data leakage from training data, in the future we plan to further validate this fact by conducting privacy attacks on the generated data. Other future work includes validating capture of long term dependencies and making model training more efficient."	"Bt0sg
Times Cited:0
Cited References Count:34
Communications in Computer and Information Science"	""	"<Go to ISI>://WOS:000791032800001"	""	"Virginia Tech, Dept Comp Sci, Arlington, VA 22203 USA
Micro Focus, Santa Clara, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Kim; Y. Sagduyu; T. Erpek; S. Ulukus"	"2021"	"Adversarial Attacks on Deep Learning Based Mmwave Beam Prediction in 5g and Beyond"	""	"2021 Ieee Statistical Signal Processing Workshop (Ssp)"	""	""	""	""	""	"590-594"	""	""	""	""	""	""	""	"Adversarial Attacks on Deep Learning Based Mmwave Beam Prediction in 5g and Beyond"	""	""	"10.1109/Ssp49050.2021.9513738"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000722246500119"	""	""	""	"Deep learning provides powerful means to learn from spectrum data and solve complex tasks in 5G and beyond such as beam selection for initial access (IA) in mmWave communications. To establish the IA between the base station (e.g., gNodeB) and user equipment (UE) for directional transmissions, a deep neural network (DNN) can predict the beam that is best slanted to each UE by using the received signal strengths (RSSs) from a subset of possible narrow beams. While improving the latency and reliability of beam selection compared to the conventional IA that sweeps all beams, the DNN itself is susceptible to adversarial attacks. We present an adversarial attack by generating adversarial perturbations to manipulate the over-the-air captured RSSs as the input to the DNN. This attack reduces the IA performance significantly and fools the DNN into choosing the beams with small RSSs compared to jamming attacks with Gaussian or uniform noise."	"Bs4rg
Times Cited:1
Cited References Count:27"	""	"<Go to ISI>://WOS:000722246500119"	""	"Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA
Intelligent Automat Inc, Rockville, MD USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Papernot; P. McDaniel; S. Jha; M. Fredrikson; Z. B. Celik; A. Swami"	"2016"	"The Limitations of Deep Learning in Adversarial Settings"	""	"1st Ieee European Symposium on Security and Privacy"	""	""	""	""	""	"372-387"	""	""	""	""	""	""	""	"The Limitations of Deep Learning in Adversarial Settings"	""	""	"10.1109/EuroSP.2016.36"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000386286200024"	""	""	"security"	"Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification."	"Bg0jt
Times Cited:1146
Cited References Count:36"	""	"<Go to ISI>://WOS:000386286200024"	""	"Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA
Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA
Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA
US Army, Res Lab, Adelphi, MD USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Kaviani; I. Sohn"	"2021"	"Defense against neural trojan attacks: A survey"	""	"Neurocomputing"	""	""	"423"	""	""	"651-667"	""	""	""	""	"Jan 29"	""	""	"Defense against neural trojan attacks: A survey"	"Neurocomputing"	"0925-2312"	"10.1016/j.neucom.2020.07.133"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000599876700001"	""	""	"deep learning
trojan attacks
backdoor attacks
defense"	"Deep learning techniques have become significantly prevalent in many real-world problems including a variety of detection, recognition, and classification tasks. To obtain high-performance neural networks, an enormous amount of training datasets, memory, and time-consuming computations are required which has increased the demands for outsource training among users. As a result, the machine-learning-as-aservice(MLaaS) providers or a third party can gain an opportunity to put the model's security at risk by training the model with malicious inputs. The malicious functionality inserted into the neural network by the adversary will be activated in the presence of specific inputs. These kinds of attacks to neural networks, called trojan or backdoor attacks, are very stealthy and hard to detect because they do not affect the network performance on clean datasets. In this paper, we refer to two important threat models and we focus on the detection and mitigation techniques against these types of attacks on neural networks which has been proposed recently. We summarize, discuss, and compare the defense methods and their corresponding results. (c) 2020 Elsevier B.V. All rights reserved."	"Pg6xq
Times Cited:1
Cited References Count:61"	""	"<Go to ISI>://WOS:000599876700001"	""	"Dongguk Univ, Div Elect & Elect Engn, Seoul, South Korea"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. F. Zhang; Y. Li; Z. Huang; H. Z. Yin"	"2021"	"Privacy Protection in Deep Multi-modal Retrieval"	""	"Sigir '21 - Proceedings of the 44th International Acm Sigir Conference on Research and Development in Information Retrieval"	""	""	""	""	""	"634-643"	""	""	""	""	""	""	""	"Privacy Protection in Deep Multi-modal Retrieval"	""	""	"10.1145/3404835.3462837"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000719807900063"	""	""	"privacy protection
multi-modal retrieval
unsupervised deep learning
adversarial data"	"Deep learning techniques have ushered in significant progress in large-scale multi-modal retrieval. Nevertheless, the advanced techniques may be used nefariously to conduct a search that violates the privacy of individuals. In this paper, we propose a novel PrIvacy Protection method (PIP) against malicious multi-modal retrieval models, which proactively transfers original data into adversarial data with quasi-imperceptible perturbations before releasing them. Consequently, unauthorized malicious parties are not able to use deployed deep models to find out desired sensitive information with them. In addition to privacy preserving, PIP synchronously learns an effective multi-modal retrieval model to facilitate authorized uses, endowed with strong resilience to the perturbations. To the best of our knowledge, it is a very first attempt to consider privacy issues in multi-modal retrieval, and encapsulate both privacy protection against unauthorized retrieval and robust multi-modal learning for authorized uses into a unified framework. This work is conducted in the challenging no-box and unsupervised settings, where neither target malicious models nor supervised information is known. The optimization objective of our versatile PIP is achieved through a two-player game between different components with both the intra- and inter-modality graph alignments and the domain distribution alignment considered. Besides, a high-level similarity matrix is developed to obtain reliable guidance for learning. Empirically, we apply the proposed PIP to hashing based multimodal retrieval scenarios and prove its effectiveness on a range of benchmarks and tasks."	"Bs4kh
Times Cited:0
Cited References Count:60"	""	"<Go to ISI>://WOS:000719807900063"	""	"Univ Queensland, Brisbane, Qld, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Sahay; C. G. Brinton; D. J. Love"	"2022"	"A Deep Ensemble-Based Wireless Receiver Architecture for Mitigating Adversarial Attacks in Automatic Modulation Classification"	""	"Ieee Transactions on Cognitive Communications and Networking"	""	""	"8"	""	"1"	"71-85"	""	""	""	""	"Mar"	""	""	"A Deep Ensemble-Based Wireless Receiver Architecture for Mitigating Adversarial Attacks in Automatic Modulation Classification"	"Ieee T Cogn Commun"	"2332-7731"	"10.1109/Tccn.2021.3114154"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000766629000009"	""	""	"adversarial attacks
automatic modulation classification
machine learning in communications
wireless security
robustness"	"Deep learning-based automatic modulation classification (AMC) models are susceptible to adversarial attacks. Such attacks inject specifically crafted wireless interference into transmitted signals to induce erroneous classification predictions. Furthermore, adversarial interference is transferable in black box environments, allowing an adversary to attack multiple deep learning models with a single perturbation crafted for a particular classification model. In this work, we propose a novel wireless receiver architecture to mitigate the effects of adversarial interference in various black box attack environments. We begin by evaluating the architecture uncertainty environment, where we show that adversarial attacks crafted to fool specific AMC DL architectures are not directly transferable to different DL architectures. Next, we consider the domain uncertainty environment, where we show that adversarial attacks crafted on time domain and frequency domain features to not directly transfer to the altering domain. Using these insights, we develop our Assorted Deep Ensemble (ADE) defense, which is an ensemble of deep learning architectures trained on time and frequency domain representations of received signals. Through evaluation on two wireless signal datasets under different sources of uncertainty, we demonstrate that our ADE obtains substantial improvements in AMC classification performance compared with baseline defenses across different adversarial attacks and potencies."	"Zp7vx
Times Cited:1
Cited References Count:48"	""	"<Go to ISI>://WOS:000766629000009"	""	"Purdue Univ, Elmore Family Sch Elect & Comp Engn, W Lafayette, IN 47907 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. H. Choi; J. H. Kim; J. S. Lee"	"2020"	"Srzoo: An Integrated Repository for Super-Resolution Using Deep Learning"	""	"2020 Ieee International Conference on Acoustics, Speech, and Signal Processing"	""	""	""	""	""	"2508-2512"	""	""	""	""	""	""	""	"Srzoo: An Integrated Repository for Super-Resolution Using Deep Learning"	"Int Conf Acoust Spee"	"1520-6149"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000615970402150"	""	""	"super-resolution
deep learning
image enhancement
image superresolution"	"Deep learning-based image processing algorithms, including image super-resolution methods, have been proposed with significant improvement in performance in recent years. However, their implementations and evaluations are dispersed in terms of various deep learning frameworks and various evaluation criteria. In this paper, we propose an integrated repository for the super-resolution tasks, named SRZoo, to provide state-of-the-art super-resolution models in a single place. Our repository offers not only converted versions of existing pre-trained models, but also documentation and toolkits for converting other models. In addition, SRZoo provides platform-agnostic image reconstruction tools to obtain super-resolved images and evaluate the performance in place. It also brings the opportunity of extension to advanced image-based researches and other image processing models. The software, documentation, and pre-trained models are publicly available on GitHub(1)."	"Bq7hu
Times Cited:1
Cited References Count:16
International Conference on Acoustics Speech and Signal Processing ICASSP"	""	"<Go to ISI>://WOS:000615970402150"	""	"Yonsei Univ, Sch Integrated Technol, Seoul, South Korea"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. K. Han; X. Zhang; J. R. Wang; Z. H. An; S. X. Jia; G. W. Zhang"	"2021"	"Hybrid distance-guided adversarial network for intelligent fault diagnosis under different working conditions"	""	"Measurement"	""	""	"176"	""	""	"109197"	""	""	""	""	"May"	""	""	"Hybrid distance-guided adversarial network for intelligent fault diagnosis under different working conditions"	"Measurement"	"0263-2241"	"ARTN 109197
10.1016/j.measurement.2021.109197"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000641454500006"	""	""	"fault diagnosis
transfer learning
domain adaptation
hybrid distance
adversarial network
deep boltzmann machine
learning-method
algorithm"	"Deep learning, especially transfer learning, has made a great deal of extraordinary achievements in intelligent fault diagnosis. In practical situations, data shift problem is inevitable due to complicated and changeable working conditions. Ignoring this problem may result in considerable degradation of diagnostic accuracy. Thus, domain distance is measured in only one metric space, and the result of domain alignment may not be ideal. This paper proposes a novel transfer learning method named hybrid distance-guided adversarial network (HDAN) to deal with this problem. Specifically, HDAN contains two parts: a feature extractor composed of a convolutional neural network and a shared classifier. Wasserstein distance and multi-kernel maximum mean discrepancy are applied in the proposed method to measure the domain distance in different metric spaces for improving the result of domain alignment. The domain distance of the last two hidden layers is minimized to improve the efficiency of domain-invariant feature extraction. Two experiments are implemented to confirm the superiority of the proposed method. The results of two experiments demonstrate that the proposed HDAN can achieve better feature cluster performance of the same class in different domains than the methods selected for comparison."	"Rp0vk
Times Cited:20
Cited References Count:41"	""	"<Go to ISI>://WOS:000641454500006"	""	"Shandong Univ Sci & Technol, Coll Mech & Elect Engn, Qingdao 266000, Peoples R China
Shandong Jianzhu Univ, Sch Mech & Elect Engn, Jinan 250000, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Borkar; F. Heide; L. Karam"	"2020"	"Defending Against Universal Attacks Through Selective Feature Regeneration"	""	"2020 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr)"	""	""	""	""	""	"706-716"	""	""	""	""	""	""	""	"Defending Against Universal Attacks Through Selective Feature Regeneration"	"Proc Cvpr Ieee"	"1063-6919"	"10.1109/Cvpr42600.2020.00079"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000620679500072"	""	""	"robustness"	"Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Specifically, image-agnostic (universal adversarial) perturbations added to any image can fool a target network into making erroneous predictions. Departing from existing defense strategies that work mostly in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal perturbations. Our approach identifies pre-trained convolutional features that are most vulnerable to adversarial noise and deploys trainable feature regeneration units which transform these DNN filter activations into resilient features that are robust to universal perturbations. Regenerating only the top 50% adversarially susceptible activations in at most 6 DNN layers and leaving all remaining DNN activations unchanged, we outperform existing defense strategies across different network architectures by more than 10% in restored accuracy. We show that without any additional modification, our defense trained on ImageNet with one type of universal attack examples effectively defends against other types of unseen universal attacks."	"Bq8ld
Times Cited:6
Cited References Count:68
IEEE Conference on Computer Vision and Pattern Recognition"	""	"<Go to ISI>://WOS:000620679500072"	""	"Arizona State Univ, Tempe, AZ 85287 USA
Princeton Univ, Princeton, NJ 08544 USA
Algolux, Montreal, PQ, Canada
Lebanese Amer Univ, Beirut, Lebanon"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. W. Zhang; W. M. Zhang; K. J. Chen; J. Y. Liu; Y. J. Liu; N. H. Yu"	"2018"	"Adversarial Examples Against Deep Neural Network based Steganalysis"	""	"Proceedings of the 6th Acm Workshop on Information Hiding and Multimedia Security (Ih&Mmsec'18)"	""	""	""	""	""	"67-72"	""	""	""	""	""	""	""	"Adversarial Examples Against Deep Neural Network based Steganalysis"	""	""	"10.1145/3206004.3206012"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000475951600010"	""	""	"steganography
adversarial examples
deep neural network
steganalysis
security"	"Deep neural network based steganalysis has developed rapidly in recent years, which poses a challenge to the security of steganography. However, there is no steganography method that can effectively resist the neural networks for steganalysis at present. In this paper, we propose a new strategy that constructs enhanced covers against neural networks with the technique of adversarial examples. The enhanced covers and their corresponding stegos are most likely to be judged as covers by the networks. Besides, we use both deep neural network based steganalysis and high-dimensional feature classifiers to evaluate the performance of steganography and propose a new comprehensive security criterion. We also make a tradeoff between the two analysis systems and improve the comprehensive security. The effectiveness of the proposed scheme is verified with the evidence obtained from the experiments on the BOSSbase using the steganography algorithm ofWOWand popular steganalyzers with rich models and three state- of- the- art neural networks."	"Bn2cq
Times Cited:34
Cited References Count:24"	""	"<Go to ISI>://WOS:000475951600010"	""	"Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei, Anhui, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. Echeberria-Barrio; A. Gil-Lerchundi; R. Orduna-Urrutia; I. Mendialdua"	"2022"	"Optimized Parameter Search Approach for Weight Modification Attack Targeting Deep Learning Models"	""	"Applied Sciences-Basel"	""	""	"12"	""	"8"	""	""	""	""	""	"Apr"	""	""	"Optimized Parameter Search Approach for Weight Modification Attack Targeting Deep Learning Models"	"Appl Sci-Basel"	""	"ARTN 3725
10.3390/app12083725"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000786726500001"	""	""	"deep learning vulnerabilities
deep learning attacks
deep learning threats
adversarial attacks
systems"	"Deep neural network models have been developed in different fields, bringing many advances in several tasks. However, they have also started to be incorporated into tasks with critical risks. That worries researchers who have been interested in studying possible attacks on these models, discovering a long list of threats from which every model should be defended. The weight modification attack is presented and discussed among researchers, who have presented several versions and analyses about such a threat. It focuses on detecting multiple vulnerable weights to modify, misclassifying the desired input data. Therefore, analysis of the different approaches to this attack helps understand how to defend against such a vulnerability. This work presents a new version of the weight modification attack. Our approach is based on three processes: input data clusterization, weight selection, and modification of the weights. Data clusterization allows a directed attack to a selected class. Weight selection uses the gradient given by the input data to identify the most-vulnerable parameters. The modifications are incorporated in each step via limited noise. Finally, this paper shows how this new version of fault injection attack is capable of misclassifying the desired cluster completely, converting the 100% accuracy of the targeted cluster to 0-2.7% accuracy, while the rest of the data continues being well-classified. Therefore, it demonstrates that this attack is a real threat to neural networks."	"0t1ib
Times Cited:0
Cited References Count:35"	""	"<Go to ISI>://WOS:000786726500001"	""	"Vicomtech Fdn, Basque Res & Technol Alliance BRTA, Mikeletegi 57, Donostia San Sebastian 20009, Spain
Univ Basque Country UPV EHU, Dept Comp Languages & Syst, Donostia San Sebastian 20018, Spain"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Xu; Y. Ma; H. C. Liu; D. Deb; H. Liu; J. L. Tang; A. K. Jain"	"2020"	"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review"	""	"International Journal of Automation and Computing"	""	""	"17"	""	"2"	"151-178"	""	""	""	""	"Apr"	""	""	"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review"	"Int J Autom Comput"	"1476-8186"	"10.1007/s11633-019-1211-x"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000521915700001"	""	""	"adversarial example
model safety
robustness
defenses
deep learning
deep neural-networks
robustness"	"Deep neural networks (DNN) have achieved unprecedented success in numerous machine learning tasks in various domains. However, the existence of adversarial examples raises our concerns in adopting deep learning to safety-critical applications. As a result, we have witnessed increasing interests in studying attack and defense mechanisms for DNN models on different data types, such as images, graphs and text. Thus, it is necessary to provide a systematic and comprehensive overview of the main threats of attacks and the success of corresponding countermeasures. In this survey, we review the state of the art algorithms for generating adversarial examples and the countermeasures against adversarial examples, for three most popular data types, including images, graphs and text."	"Kz5ic
Times Cited:73
Cited References Count:138"	""	"<Go to ISI>://WOS:000521915700001"	""	"Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48823 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. F. Mao; Y. F. Chen; Y. H. Li; Y. He; H. Xue"	"2020"	"Learning to Characterize Adversarial Subspaces"	""	"2020 Ieee International Conference on Acoustics, Speech, and Signal Processing"	""	""	""	""	""	"2438-2442"	""	""	""	""	""	""	""	"Learning to Characterize Adversarial Subspaces"	"Int Conf Acoust Spee"	"1520-6149"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000615970402136"	""	""	"adversarial examples
subspaces
attack detection"	"Deep Neural Networks (DNNs) are known to be vulnerable to the maliciously generated adversarial examples. To detect these adversarial examples, previous methods use artificially designed metrics to characterize the properties of adversarial subspaces where adversarial examples lie. However, we find these methods are not working in practical attack detection scenarios. Because the artificially defined features are lack of robustness and show limitation in discriminative power to detect strong attacks. To solve this problem, we propose a novel adversarial detection method which identifies adversaries by adaptively learning reasonable metrics to characterize adversarial subspaces. As auxiliary context information, k nearest neighbors are used to represent the surrounded subspace of the detected sample. We propose an innovative model called Neighbor Context Encoder (NCE) to learn from k neighbors context and infer if the detected sample is normal or adversarial. We conduct thorough experiment on CIFAR-10, CIFAR-100 and ImageNet dataset. The results demonstrate that our approach surpasses all existing methods under three settings: attack-aware black-box detection, attack-unaware black-box detection and white-box detection."	"Bq7hu
Times Cited:2
Cited References Count:29
International Conference on Acoustics Speech and Signal Processing ICASSP"	""	"<Go to ISI>://WOS:000615970402136"	""	"Alibaba Grp, Hangzhou, Zhejiang, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. K. Zhang; T. F. Wu"	"2020"	"Learning Ordered Top-k Adversarial Attacks via Adversarial Distillation"	""	"2020 Ieee/Cvf Conference on Computer Vision and Pattern Recognition Workshops (Cvprw 2020)"	""	""	""	""	""	"3364-3373"	""	""	""	""	""	""	""	"Learning Ordered Top-k Adversarial Attacks via Adversarial Distillation"	"Ieee Comput Soc Conf"	"2160-7508"	"10.1109/Cvprw50498.2020.00396"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000788279003049"	""	""	""	"Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially white-box targeted attacks. This paper studies the problem of how aggressive white-box targeted attacks can be to go beyond widely used Top-1 attacks. We propose to learn ordered Top-k attacks (k = 1), which enforce the Top-k predicted labels of an adversarial example to be the k (randomly) selected and ordered labels (the ground-truth label is exclusive). Two methods are presented. First, we extend the vanilla Carlini-Wagner (C&W) method and use it as a strong baseline. Second, we present an Adversarial Distillation (AD) framework consisting of two components: (i) Computing an adversarial probability distribution for a given ordered Top-k targeted labels. (ii) Learning adversarial examples by minimizing the Kullback-Leibler (KL) divergence between the adversarial distribution and the predicted distribution, together with the perturbation energy penalty. In computing adversarial distributions, we explore how to leverage label semantic similarities, leading to knowledge-oriented attacks. In experiments, we test Top-k (k = 1, 2, 5, 10) attacks in the ImageNet-1000 val dataset using three representative DNNs trained with the clean ImageNet-1000 train dataset, ResNet-50 [11], DenseNet-121 [14] and AOGNet-12M [21]. Overall, the proposed AD approach obtains the best results, especially by a large margin when computation budget is limited. It reduces the perturbation energy consistently with the same attack success rate on all the four k's, and improve the attack success rate by a large margin against the modified C&W method for k = 10."	"Bt0jz
Times Cited:0
Cited References Count:43
IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"	""	"<Go to ISI>://WOS:000788279003049"	""	"NC State Univ, Dept ECE, Raleigh, NC 27695 USA
NC State Univ, Visual Narrat Initiat, Raleigh, NC USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Wang; C. R. Li; S. Wen; S. Nepal; Y. Xiang"	"2021"	"Man-in-the-Middle Attacks Against Machine Learning Classifiers Via Malicious Generative Models"	""	"Ieee Transactions on Dependable and Secure Computing"	""	""	"18"	""	"5"	"2074-2087"	""	""	""	""	"Sept 1"	""	""	"Man-in-the-Middle Attacks Against Machine Learning Classifiers Via Malicious Generative Models"	"Ieee T Depend Secure"	"1545-5971"	"10.1109/Tdsc.2020.3021008"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000692208200001"	""	""	"deep neural network
adversarial example
security"	"Deep Neural Networks (DNNs) are vulnerable to deliberately crafted adversarial examples. In the past few years, many efforts have been spent on exploring query-optimisation attacks to find adversarial examples of either black-box or white-box DNN models, as well as the defending countermeasures against those attacks. In this article, we explore vulnerabilities of DNN models under the umbrella of Man-in-the-Middle (MitM) attacks, which have not been investigated before. From the perspective of an MitM adversary, the aforementioned adversarial example attacks are not viable anymore. First, such attacks must acquire the outputs from the models multiple times before actually launching attacks, which is difficult for the MitM adversary in practice. Second, such attacks are one-off and cannot be directly generalised onto new data examples, which decreases the rate of return for the attacker. In contrast, using generative models to craft adversarial examples on the fly can mitigate the drawbacks. However, the adversarial capability of the generative models, such as Variational Auto-Encoder (VAE), has not been extensively studied. Therefore, given a classifier, we investigate using a VAE decoder to either transform benign inputs to their adversarial counterparts or decode outputs from benign VAE encoders to be adversarial examples. The proposed method can endue more capability to MitM attackers. Based on our evaluation, the proposed attack can achieve above 95 percent success rates on both MNIST and CIFAR10 datasets, which is better or comparable with state-of-the-art query-optimisation attacks. In the meantime, the attack is $10 boolean AND 4$104 times faster than the query-optimisation attacks."	"Uk8ik
Times Cited:0
Cited References Count:47"	""	"<Go to ISI>://WOS:000692208200001"	""	"Swinburne Univ Technol, Sch Software & Elect Engn, Hawthorn, Vic 3122, Australia
CSIROs Data 61, N Ryde, NSW 2113, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. T. Xiao; C. M. Pun; B. Liu"	"2020"	"Adversarial example generation with adaptive gradient search for single and ensemble deep neural network"	""	"Information Sciences"	""	""	"528"	""	""	"147-167"	""	""	""	""	"Aug"	""	""	"Adversarial example generation with adaptive gradient search for single and ensemble deep neural network"	"Inform Sciences"	"0020-0255"	"10.1016/j.ins.2020.04.022"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000532827200009"	""	""	"deep neural networks
adversarial attack
adaptive gradient
perturbation
models"	"Deep Neural Networks (DNNs) have achieved remarkable success in specific domains, such as computer vision, audio processing, and natural language processing. However, researches indicate that deep neural networks are facing many security issues (e.g., adversarial attack, information forgery). In the field of image classification, adversarial samples generated by specific adversarial attack strategies can easily fool deep neural classification models into making unreliable predictions. We find that such adversarial attack algorithms induce large-scale pixel modifications in crafted images to maintain the effectiveness of the adversarial attack. Massive pixel modifications change the inherent characteristics of generated examples and cause large image distortion. To address the mentioned issues, we introduce an adaptive gradient-based adversarial attack method named Adaptive Iteration Fast Gradient Method (AI-FGM), which focuses on seeking the input's preceding gradient and adjusts the accumulation of perturbed entity adaptively for performing adversarial attacks. By maximizing the specific loss for generating adaptive gradient-based entities, AI-FGM calls for several gradient-based operators on the clean input to map crafted sample with the corresponding prediction directly. AI-FGM helps to reduce unnecessary gradient-based entity accumulation when processing adversary by adaptive gradient-based seeking strategy. Experimental results show that AI-FGM outperforms other gradient-based adversarial attackers in attacking deep neural classification models with fewer pixel modifications (AMP is 0.0017 with L-2 norm in fooling Inception-v3) and higher success rate of invasion on deep neural classification networks in white-box and black-box attack strategy on public image datasets with different resolution. (C) 2020 Elsevier Inc. All rights reserved."	"Ln3fh
Times Cited:12
Cited References Count:49"	""	"<Go to ISI>://WOS:000532827200009"	""	"Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. F. Li; M. H. Xue; B. M. Zhao; H. J. Zhu; X. P. Zhang"	"2021"	"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization"	""	"Ieee Transactions on Dependable and Secure Computing"	""	""	"18"	""	"5"	"2088-2105"	""	""	""	""	"Sept 1"	""	""	"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization"	"Ieee T Depend Secure"	"1545-5971"	"10.1109/Tdsc.2020.3021407"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000690440600005"	""	""	"training
data models
machine learning
perturbation methods
neural networks
inspection
image color analysis
backdoor attacks
steganography
deep neural networks"	"Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this article, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches."	"Ui2jp
Times Cited:12
Cited References Count:53"	""	"<Go to ISI>://WOS:000690440600005"	""	"Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China
Univ Adelaide, Adelaide, SA 5005, Australia
Univ New South Wales, Sydney, NSW, Australia
Data61 CSIRO, Canberra, ACT 2601, Australia
Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Z. Lan; K. W. Nixon; Q. L. Guo; G. H. Zhang; Y. C. Xu; H. Li; Y. R. Chen"	"2020"	"FCDM: A Methodology Based on Sensor Pattern Noise Fingerprinting for Fast Confidence Detection to Adversarial Attacks"	""	"Ieee Transactions on Computer-Aided Design of Integrated Circuits and Systems"	""	""	"39"	""	"12"	"4791-4804"	""	""	""	""	"Dec"	""	""	"FCDM: A Methodology Based on Sensor Pattern Noise Fingerprinting for Fast Confidence Detection to Adversarial Attacks"	"Ieee T Comput Aid D"	"0278-0070"	"10.1109/Tcad.2020.2969982"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000592111400038"	""	""	"perturbation methods
computational modeling
data integrity
detectors
optimization
field programmable gate arrays
hardware
adversarial attacks
confidence detection
deep neural networks (dnns)
fpga-based hardware architecture
sensor pattern noise (spn)"	"Deep neural networks (DNNs) have shown phenomenal success in many real-world applications. However, a concerning weakness of DNNs is their vulnerability to adversarial attacks. Although there exist some methods to detect adversarial attacks, they often suffer from high computational cost and constraints on certain types of attacks, and ignore external features that could aid during attack detection. In this article, we propose fast confidence detection method (FCDM), an innovative method for fast confidence detection of adversarial attacks based on measuring the integrity of sensor pattern noise fingerprinting embedded in input examples. We note that the existing adversarial detectors are often designed as a binary classifier to differentiate clean or adversarial examples. However, the detection of adversarial examples can be much more complicated than such a scenario. Our key insight is that the confidence level of detecting an input sample as an adversarial example is a more useful info for the system to properly take an action to resist potential attacks. The experimental results show that FCDM is capable to give a confidence distribution model of the most popular adversarial attacks. And, using the confidence distribution model, FCDM can quickly determine the confidence level of the input sample. Based on different properties of the confidence distribution models associated with these adversarial attacks, FCDM can provide early attack warning including even the possible attack types of the adversarial attack examples. FCDM also has the following advantages: 1) it is effective for both a white-box attack and black-box attack; 2) it do not depend on the class of adversarial attacks and can be used as both known attack defense and unknown attack defense; and 3) it does not need to know the details of the DNN model and does not affect the functionality of the DNN. Since fast confidence detection method (FCDM) is a computationally heavy task, we propose an FPGA-based accelerator based on a series of optimization techniques, such as the quantization, data reuse and operation replacement, etc. We implement our method on an FPGA platform and achieve a system clock frequency of 279 MHz with a power consumption of the only 0.7626 W. Moreover, in the real system performance test, we obtain a high efficiency of 29.740 IPS/W and a low latency of just 44.1 ms with very marginal accuracy loss."	"Ov3jt
Times Cited:0
Cited References Count:49"	""	"<Go to ISI>://WOS:000592111400038"	""	"Duke Univ, Dept Elect & Comp Engn, Durham, NC 27701 USA
Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China
Univ Chinese Acad Sci, Dept Comp Sci & Technol, Beijing 100089, Peoples R China
Xi An Jiao Tong Univ, Dept Microelect, Xian 710049, Peoples R China
Capital Normal Univ, Dept Comp Sci & Technol, Beijing 100048, Peoples R China
Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. Lang; U. Kotlicki; S. Avidan"	"2021"	"Geometric Adversarial Attacks and Defenses on 3D Point Clouds"	""	"2021 International Conference on 3d Vision (3dv 2021)"	""	""	""	""	""	"1196-1205"	""	""	""	""	""	""	""	"Geometric Adversarial Attacks and Defenses on 3D Point Clouds"	"Int Conf 3d Vision"	"2378-3826"	"10.1109/3dv53792.2021.00127"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000786496000117"	""	""	""	"Deep neural networks are prone to adversarial examples that maliciously alter the network's outcome. Due to the increasing popularity of 3D sensors in safety-critical systems and the vast deployment of deep learning models for 3D point sets, there is a growing interest in adversarial attacks and defenses for such models. So far, the research has focused on the semantic level, namely, deep point cloud classifiers. However, point clouds are also widely used in a geometric-related form that includes encoding and reconstructing the geometry.
In this work, we are the first to consider the problem of adversarial examples at a geometric level. In this setting, the question is how to craft a small change to a clean source point cloud that leads, after passing through an autoencoder model, to the reconstruction of a different target shape. Our attack is in sharp contrast to existing semantic attacks on 3D point clouds. While such works aim to modify the predicted label by a classifier, we alter the entire reconstructed geometry. Additionally, we demonstrate the robustness of our attack in the case of defense, where we show that remnant characteristics of the target shape are still present at the output after applying the defense to the adversarial input. Our code is publicly available(1)."	"Bt0bs
Times Cited:0
Cited References Count:51
International Conference on 3D Vision"	""	"<Go to ISI>://WOS:000786496000117"	""	"Tel Aviv Univ, Tel Aviv, Israel"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. T. Shang; B. H. Wang; T. Y. Li; J. W. Tian; K. R. Cao; R. X. Guo"	"2020"	"Adversarial Examples on Deep-Learning-Based ADS-B Spoofing Detection"	""	"Ieee Wireless Communications Letters"	""	""	"9"	""	"10"	"1734-1737"	""	""	""	""	"Oct"	""	""	"Adversarial Examples on Deep-Learning-Based ADS-B Spoofing Detection"	"Ieee Wirel Commun Le"	"2162-2337"	"10.1109/Lwc.2020.3002914"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000577969000030"	""	""	"receivers
encoding
transponders
perturbation methods
detectors
bit error rate
wireless communication
automatic dependent surveillance-broadcast (ads-b)
spoofing detection
wireless security
adversarial machine learning"	"Deep neural networks have been applied to many tasks in air traffic management, ranging from anomaly detection to flight trajectory prediction. However, it has been shown that such algorithms are susceptible to adversarial examples. In this letter, we manage to show that current deep learning algorithms proposed for spoofing detection are vulnerable to maliciously crafted ADS-B data. To inject the false messages into the ADS-B system without being detected, we need to find adversarial perturbations to balance the need of overwhelming the channel noise and keeping the decoding error low. Simulation results demonstrate the viability of our approach to evade the DNN-based spoofing detector without increasing the decoding error."	"Oa7nx
Times Cited:0
Cited References Count:12"	""	"<Go to ISI>://WOS:000577969000030"	""	"Air Force Engn Univ, Coll Informat & Nav, Xian 710077, Peoples R China
Natl Univ Def Technol, Sch Informat & Commun, Xian 710106, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Aledhari; R. Razzak; R. M. Parizi; G. Srivastava"	"2021"	"Multimodal Machine Learning for Pedestrian Detection"	""	"2021 Ieee 93rd Vehicular Technology Conference (Vtc2021-Spring)"	""	""	""	""	""	"1-7"	""	""	""	""	""	""	""	"Multimodal Machine Learning for Pedestrian Detection"	"Veh Technol Confe"	""	"10.1109/VTC2021-Spring51267.2021.9448692"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000687839600061"	""	""	"convolutional neural networks
multimodal machine learning
pedestrian detection
autonomous vehicles
k-means
driving safely"	"Designing and developing autonomous vehicles that are capable of moving safely on roads by sensing the environment has motivated researchers to focus on pedestrian detection systems so they can detect people as fast and accurately as possible. However, for pedestrian detection, it is crucial to consider not only the pedestrians themselves but their color as well, because color has the advantage of being invariant to changes in scaling, rotation, and partial occlusion. Therefore, considering skin color detection for implementing pedestrian detection systems is an essential required step in ensuring autonomous vehicles are further incorporated into our society. Detecting human skin has proven to be a challenging problem because skin color can vary dramatically in its appearance due to many factors such as illumination, race, imaging conditions, and others. Recently it has been noted that pedestrian detection systems for autonomous vehicles perform poorly at detecting people with darker skin tones. Such findings indicate that there is a larger problem that is causing these issues: algorithmic bias. Algorithmic bias in pedestrian detection systems could be the leading factor of their poor performance due to the methods implemented and datasets used. Unfortunately, the algorithmic bias in this context has not been considered closely and it seems that many studies do not cover this aspect closely when discussing pedestrian detection systems for autonomous vehicles. To alleviate this, we attempt to explore different techniques that can be used to detect pedestrians while minimizing bias. In our experiment, we use both a YOLO v3 convolutional neural network and K-Means clustering for classifying skin-tones."	"Bs1dn
Times Cited:1
Cited References Count:41
IEEE Vehicular Technology Conference VTC"	""	"<Go to ISI>://WOS:000687839600061"	""	"Kennesaw State Univ, Coll Comp & Software Engn, Kennesaw, GA 30144 USA
Brandon Univ, Dept Math & Comp Sci, Brandon, MB, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Fawzi; H. Fawzi; O. Fawzi"	"2018"	"Adversarial vulnerability for any classifier"	""	"Advances in Neural Information Processing Systems 31 (Nips 2018)"	""	""	"31"	""	""	""	""	""	""	""	""	""	""	"Adversarial vulnerability for any classifier"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000461823301019"	""	""	""	"Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets."	"Bm3bf
Times Cited:22
Cited References Count:42
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000461823301019"	""	"DeepMind, London, England
Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge, England
Univ Lyon, ENS Lyon, CNRS, UCBL,LIP, F-69342 Lyon 07, France"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Kherchouche; S. A. Fezza; W. Hamidouche"	"2021"	"Detect and defense against adversarial examples in deep learning using natural scene statistics and adaptive denoising"	""	"Neural Computing & Applications"	""	""	"abs/2107.05780"	""	""	""	""	""	""	""	"Jul 21"	""	""	"Detect and defense against adversarial examples in deep learning using natural scene statistics and adaptive denoising"	"Neural Comput Appl"	"0941-0643"	"10.1007/s00521-021-06330-x"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000675333400007"	""	""	"deep learning
adversarial attack
defense
detection
natural scene statistics
denoising
adaptive threshold
classification
image quality assessment"	"Despite the enormous performance of deep neural networks (DNNs), recent studies have shown their vulnerability to adversarial examples (AEs), i.e., carefully perturbed inputs designed to fool the targeted DNN. Currently, the literature is rich with many effective attacks to craft such AEs. Meanwhile, many defense strategies have been developed to mitigate this vulnerability. However, these latter showed their effectiveness against specific attacks and does not generalize well to different attacks. In this paper, we propose a framework for defending DNN classifier against adversarial samples. The proposed method is based on a two-stage framework involving a separate detector and a denoising block. The detector aims to detect AEs by characterizing them through the use of natural scene statistic (NSS), where we demonstrate that these statistical features are altered by the presence of adversarial perturbations. The denoiser is based on block matching 3D (BM3D) filter fed by an optimum threshold value estimated by a convolutional neural network (CNN) to project back the samples detected as AEs into their data manifold. We conducted a complete evaluation on three standard datasets, namely MNIST, CIFAR-10 and Tiny-ImageNet. The experimental results show that the proposed defense method outperforms the state-of-the-art defense techniques by improving the robustness against a set of attacks under black-box, gray-box and white-box settings. The source code is available at: https://github.com/kherchouche-anouar/2DAE."	"Tm1so
Times Cited:0
Cited References Count:68"	""	"<Go to ISI>://WOS:000675333400007"	""	"Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, Rennes, France
Natl Inst Telecommun & ICT, Oran, Algeria"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Zhao; S. Y. Wang; G. Y. Cheng; Y. Z. Wang; Y. S. Fei; X. Lin"	"2019"	"Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks"	""	"Proceedings of the 2019 56th Acm/Edac/Ieee Design Automation Conference (Dac)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks"	""	""	"10.1145/3316781.3317825"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000482058200165"	""	""	"deep neural networks
fault injection
admm"	"Despite the great achievements of deep neural networks (DNNs), the vulnerability of state-of-the-art DNNs raises security concerns of DNNs in many application domains requiring high reliability. We propose the fault sneaking attack on DNNs, where the adversary aims to misclassify certain input images into any target labels by modifying the DNN parameters. We apply ADMM (alternating direction method of multipliers) for solving the optimization problem of the fault sneaking attack with two constraints: 1) the classification of the other images should be unchanged and 2) the parameter modifications should be minimized. Specifically, the first constraint requires us not only to inject designated faults (misclassifications), but also to hide the faults for stealthy or sneaking considerations by maintaining model accuracy. The second constraint requires us to minimize the parameter modifications (using l(0) norm to measure the number of modifications and l(2) norm to measure the magnitude of modifications). Comprehensive experimental evaluation demonstrates that the proposed framework can inject multiple sneaking faults without losing the overall test accuracy performance."	"Bn4jz
Times Cited:17
Cited References Count:38"	""	"<Go to ISI>://WOS:000482058200165"	""	"Northeastern Univ, Boston, MA 02115 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Sotgiu; A. Demontis; M. Melis; B. Biggio; G. Fumera; X. Y. Feng; F. Roli"	"2020"	"Deep neural rejection against adversarial examples"	""	"Eurasip Journal on Information Security"	""	""	"2020"	""	"1"	"1-10"	""	""	""	""	"Apr 7"	""	""	"Deep neural rejection against adversarial examples"	"Eurasip J Inf Secur"	"2510-523x"	"ARTN 5
10.1186/s13635-020-00105-y"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000524322900001"	""	""	"adversarial machine learning
deep neural networks
adversarial examples"	"Despite the impressive performances reported by deep neural networks in different application domains, they remain largely vulnerable to adversarial examples, i.e., input samples that are carefully perturbed to cause misclassification at test time. In this work, we propose a deep neural rejection mechanism to detect adversarial examples, based on the idea of rejecting samples that exhibit anomalous feature representations at different network layers. With respect to competing approaches, our method does not require generating adversarial examples at training time, and it is less computationally demanding. To properly evaluate our method, we define an adaptive white-box attack that is aware of the defense mechanism and aims to bypass it. Under this worst-case setting, we empirically show that our approach outperforms previously proposed methods that detect adversarial examples by only analyzing the feature representation provided by the output network layer."	"Lb0ix
Times Cited:7
Cited References Count:40"	""	"<Go to ISI>://WOS:000524322900001"	""	"Univ Cagliari, DIEE, Piazza Armi, I-09123 Cagliari, Italy
Pluribus One, Cagliari, Italy
Northwestern Polytech Univ, Xian, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. B. Li; J. G. Zhang; K. Q. Huang"	"2021"	"Universal adversarial perturbations against object detection"	""	"Pattern Recognition"	""	""	"110"	""	""	"107584"	""	""	""	""	"Feb"	""	""	"Universal adversarial perturbations against object detection"	"Pattern Recogn"	"0031-3203"	"ARTN 107584
10.1016/j.patcog.2020.107584"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000585302200003"	""	""	"adversarial examples
object detection
universal adversarial perturbation"	"Despite the remarkable success of deep neural networks on many visual tasks, they have been proved to be vulnerable to adversarial examples. For visual tasks, adversarial examples are images added with visu-ally imperceptible perturbations that result in failure for recognition. Previous works have demonstrated that adversarial perturbations can cause neural networks to fail on object detection. But these methods focus on generating an adversarial perturbation for a specific image, which is the image-specific perturbation. This paper tries to extend such image-level adversarial perturbations to detector-level, which are universal (image-agnostic) adversarial perturbations. Motivated by this, we propose a Universal Dense Object Suppression (U-DOS) algorithm to derive the universal adversarial perturbations against object detection and show that such perturbations with visual imperceptibility can lead the state-of-the-art detectors to fail in finding any objects in most images. Compared to image-specific perturbations, the results of image-agnostic perturbations are more interesting and also pose more challenges in AI security, because they are more convenient to be applied in the real physical world. We also analyze the generalization of such universal adversarial perturbations across different detectors and datasets under the black-box attack settings, showing it's a simple but promising adversarial attack approach against object detection. Furthermore, we validate the class-specific universal perturbations, which can remove the detection results of the target class and keep others unchanged. (c) 2020 Published by Elsevier Ltd."	"Ol4hb
Times Cited:2
Cited References Count:55"	""	"<Go to ISI>://WOS:000585302200003"	""	"Chinese Acad Sci, Inst Automat, CRISE, Beijing, Peoples R China
Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China
CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"E. R. Balda; A. Behboodi; R. Mathar"	"2019"	"Perturbation Analysis of Learning Algorithms: Generation of Adversarial Examples From Classification to Regression"	""	"Ieee Transactions on Signal Processing"	""	""	"67"	""	"23"	"6078-6091"	""	""	""	""	"Dec 1"	""	""	"Perturbation Analysis of Learning Algorithms: Generation of Adversarial Examples From Classification to Regression"	"Ieee T Signal Proces"	"1053-587x"	"10.1109/Tsp.2019.2943232"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000501765700001"	""	""	"perturbation methods
task analysis
training
signal processing algorithms
robustness
neural networks
cost function
artificial neural networks
machine learning
robustness
computer vision
robustness
networks"	"Despite the tremendous success of deep neural networks in various learning problems, it has been observed that adding intentionally designed adversarial perturbations to inputs of these architectures leads to erroneous classification with high confidence in the prediction. In this work, we show that adversarial examples can be generated using a generic approach that relies on the perturbation analysis of learning algorithms. Formulated as a convex program, the proposed approach retrieves many current adversarial attacks as special cases. It is used to propose novel attacks against learning algorithms for classification and regression tasks under various new constraints with closed-form solutions in many instances. In particular, we derive new attacks against classification algorithms which are shown to be top-performing on various architectures. Although classification tasks have been the main focus of adversarial attacks, we use the proposed approach to generate adversarial perturbations for various regression tasks. Designed for single pixel and single subset attacks, these attacks are applied to autoencoding, image colorization and real-time object detection tasks, showing that adversarial perturbations can degrade equally gravely the output of regression tasks. (1) (1) In the spirit of encouraging reproducible research, the implementations used in this paper have been made available at: github.com/ebalda/adversarialconvex."	"Ju6dr
Times Cited:4
Cited References Count:52"	""	"<Go to ISI>://WOS:000501765700001"	""	"Rhein Westfal TH Aachen, Inst Theoret Informat Technol TI, D-52074 Aachen, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. K. Sharma; D. Sheet; P. K. Biswas"	"2020"	"Spatiotemporal deep networks for detecting abnormality in videos"	""	"Multimedia Tools and Applications"	""	""	"79"	""	"15-16"	"11237-11268"	""	""	""	""	"Apr"	""	""	"Spatiotemporal deep networks for detecting abnormality in videos"	"Multimed Tools Appl"	"1380-7501"	"10.1007/s11042-020-08786-w"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000523109000001"	""	""	"video surveillance
deep learning
abnormality detection and localization
adversarial attack and defense
anomaly detection
object motion
surveillance"	"Detecting and localizing anomalous behavior in the surveillance video is explored and spatiotemporal model, which jointly learns the appearance and motion-based feature is proposed. The general solution is to learn from the normal-data as reference models and uses various hands designed features. However, huge variations can occur within normal-behavior patterns. It is a challenge to represent higher-level concepts of a normal or abnormal event explicitly from raw input data. In the proposed framework, spatiotemporal features learned at various hidden layer are analyzed. Based on the learned representation, the reconstruction of video volumes are performed. Finally, the structural distortion based abnormality score is computed by considering luminance, contrast, and structural information to detect the presence of abnormality and localize them. Further, we also explored the performance of GMM and one-class SVM in a given scenario. The proposed structural distortion based abnormality detection and localization are evaluated on the publicly available UCSD and UMN dataset. The performance of the developed system is found to outperform the existing state-of-art methods for detecting and localizing abnormality at the frame as well as pixel-level. Recently, deep architecture is also found to be vulnerable to adversarial attacks and can easily be tricked to fool the system. However, most of the existing attacks are designed for the classification task. In this work, we utilize the gradient-based approach to generate adversarial samples for an abnormality detection system. Finally, we build the defense mechanism to detect the abnormality in the presence of such adversarial attacks."	"Lq1pc
Times Cited:3
Cited References Count:66"	""	"<Go to ISI>://WOS:000523109000001"	""	"IIT Kharagpur, Kharagpur, W Bengal, India
IIT Kharagpur, Dept Elect & Elect Commun Engn, Kharagpur, W Bengal, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. Chu; I. F. Ilyas; S. Krishnan; J. N. Wang"	"2016"	"Data Cleaning: Overview and Emerging Challenges"	""	"Sigmod'16: Proceedings of the 2016 International Conference on Management of Data"	""	""	""	""	""	"2201-2206"	""	""	""	""	""	""	""	"Data Cleaning: Overview and Emerging Challenges"	""	""	"10.1145/2882903.2912574"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000452538600171"	""	""	"violations
query"	"Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis."	"Bl5ri
Times Cited:102
Cited References Count:81"	""	"<Go to ISI>://WOS:000452538600171"	""	"Univ Waterloo, Waterloo, ON, Canada
Univ Calif Berkeley, Berkeley, CA USA
Simon Fraser Univ, Burnaby, BC, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Bernau; G. Eibl; P. W. Grassal; H. Keller; F. Kerschbaum"	"2021"	"Quantifying identifiability to choose and audit epsilon in differentially private deep learning"	""	"Proceedings of the Vldb Endowment"	""	""	"14"	""	"13"	"3335-3347"	""	""	""	""	"Sep"	""	""	"Quantifying identifiability to choose and audit epsilon in differentially private deep learning"	"Proc Vldb Endow"	"2150-8097"	"10.14778/3484224.3484231"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000742944600008"	""	""	"composition theorem"	"Differential privacy allows bounding the influence that training data records have on a machine learning model. To use differential privacy in machine learning, data scientists must choose privacy parameters (epsilon, delta). Choosing meaningful privacy parameters is key, since models trained with weak privacy parameters might result in excessive privacy leakage, while strong privacy parameters might overly degrade model utility. However, privacy parameter values are difficult to choose for two main reasons. First, the theoretical upper bound on privacy loss (epsilon, delta) might be loose, depending on the chosen sensitivity and data distribution of practical datasets. Second, legal requirements and societal norms for anonymization often refer to individual identifiability, to which (epsilon, delta) are only indirectly related.
We transform (epsilon, delta) to a bound on the Bayesian posterior belief of the adversary assumed by differential privacy concerning the presence of any record in the training dataset. The bound holds for multidimensional queries under composition, and we show that it can be tight in practice. Furthermore, we derive an identifiability bound, which relates the adversary assumed in differential privacy to previous work on membership inference adversaries. We formulate an implementation of this differential privacy adversary that allows data scientists to audit model training and compute empirical identifiability scores and empirical (epsilon, delta)."	"Yh1ou
Times Cited:0
Cited References Count:47"	""	"<Go to ISI>://WOS:000742944600008"	""	"Sap SE, Karlsruhe, Germany
Salzburg Univ Appl Sci, Salzburg, Austria
Heidelberg Univ, Heidelberg, Germany
Univ Waterloo, Waterloo, ON, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Wang; A. Pal; Q. Yang; K. Kant; K. Zhu; S. Guo"	"2022"	"Collaborative Machine Learning: Schemes, Robustness, and Privacy"	""	"IEEE Trans Neural Netw Learn Syst"	""	""	"PP"	""	""	""	""	""	""	"2022/05/27"	"May 26"	""	""	"Collaborative Machine Learning: Schemes, Robustness, and Privacy"	""	"2162-2388 (Electronic)
2162-237X (Linking)"	"10.1109/TNNLS.2022.3169347"	""	""	""	""	""	""	""	""	""	""	""	"35617185"	""	""	""	"Distributed machine learning (ML) was originally introduced to solve a complex ML problem in a parallel way for more efficient usage of computation resources. In recent years, such learning has been extended to satisfy other objectives, namely, performing learning in situ on the training data at multiple locations and keeping the training datasets private while still allowing sharing of the model. However, these objectives have led to considerable research on the vulnerabilities of distributed learning both in terms of privacy concerns of the training data and the robustness of the learned overall model due to bad or maliciously crafted training data. This article provides a comprehensive survey of various privacy, security, and robustness issues in distributed ML."	"Wang, Junbo
Pal, Amitangshu
Yang, Qinglin
Kant, Krishna
Zhu, Kaiming
Guo, Song
eng
IEEE Trans Neural Netw Learn Syst. 2022 May 26;PP. doi: 10.1109/TNNLS.2022.3169347."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35617185"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. Umer; C. Frederickson; R. Polikar"	"2018"	"Adversarial Poisoning of Importance Weighting in Domain Adaptation"	""	"2018 Ieee Symposium Series on Computational Intelligence (Ieee Ssci)"	""	""	""	""	""	"381-388"	""	""	""	""	""	""	""	"Adversarial Poisoning of Importance Weighting in Domain Adaptation"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000459238800052"	""	""	"covariate shift"	"Domain adaptation techniques such as importance weighting modify the training data to better represent a different lest data distribution, a process that may be particularly vulnerable to a malicious attack in an adversarial machine learning scenario. In this work, we explore the level of such vulnerability of importance weighting to poisoning attacks. Importance weighting, like other domain adaptation approaches, assumes that the distributions of training and test data are different but related. An intelligent adversary, having full or partial access to the training data, can take advantage of the expected difference between the distributions, and can inject well crafted malicious samples into the training data, resulting in an incorrect estimation of the importance ratio. In this work, we demonstrate the vulnerability of one of the simplest yet most effective approaches for directly estimating the importance ratio, namely, modifying the training distribution using a discriminative classifier such as the logistic regression. We test the robustness of the importance weighting process using well-controlled synthetic datasets, with an increasing number of attack points in the training data. Under the worst case perfect knowledge scenario, where the attacker has full access to the training data, we demonstrate that importance weighting can be dramatically compromised with the insertion of even a single attack point. We then show that even under limited knowledge scenario, where the attacker has limited access to the training data, the estimation process can still be significantly compromised."	"Bm0wb
Times Cited:3
Cited References Count:21"	""	"<Go to ISI>://WOS:000459238800052"	""	"Rowan Univ, Glassboro, NJ 08028 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. L. Liu; L. F. Lai"	"2020"	"Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense"	""	"Ieee Transactions on Signal Processing"	""	""	"68"	""	""	"5152-5165"	""	""	""	""	""	""	""	"Action-Manipulation Attacks Against Stochastic Bandits: Attacks and Defense"	"Ieee T Signal Proces"	"1053-587x"	"10.1109/Tsp.2020.3021525"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000572629000004"	""	""	"signal processing algorithms
robustness
force
stochastic processes
upper bound
numerical models
machine learning
action-manipulation attack
stochastic bandits
ucb
multiarmed bandit
algorithms
access"	"Due to the broad range of applications of stochastic multi-armed bandit model, understanding the effects of adversarial attacks and designing bandit algorithms robust to attacks are essential for the safe applications of this model. In this paper, we introduce a new class of attacks named action-manipulation attacks. In this class of attacks, an adversary can change the action signal selected by the user. We show that without knowledge of mean rewards of arms, our proposed attack can manipulate Upper Confidence Bound (UCB) algorithm, a widely used bandit algorithm, into pulling a target arm very frequently by spending only logarithmic cost. To defend against this class of attacks, we introduce a novel algorithm that is robust to action-manipulation attacks when an upper bound for the total attack cost is given. We prove that our algorithm has a pseudo-regret upper bounded by O(max{log T, A}) with a high probability, where T is the total number of rounds and A is the upper bound of the total attack cost."	"Nt0fv
Times Cited:2
Cited References Count:29"	""	"<Go to ISI>://WOS:000572629000004"	""	"Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. Wiafe; F. N. Koranteng; E. N. Obeng; N. Assyne; A. Wiafe; S. R. Gulliver"	"2020"	"Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature"	""	"Ieee Access"	""	""	"8"	""	""	"146598-146612"	""	""	""	""	""	""	""	"Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3013145"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000562284800001"	""	""	"systematics
machine learning
protocols
computer crime
artificial intelligence and cybersecurity
information security
machine learning
systematic reviews
intrusion detection system
malware detection
anomaly detection
neural-networks
cyber-security
algorithm
ensemble
privacy
game
ids"	"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets."	"Ne0jr
Times Cited:5
Cited References Count:171"	""	"<Go to ISI>://WOS:000562284800001"	""	"Univ Ghana, Dept Comp Sci, Accra, Ghana
Univ Educ, Dept Informat Technol Educ, Kumasi Campus, Winneba, Ghana
KPMG Ghana, Accra, Ghana
Univ Jyvaskyla, Fac Informat Technol, Jyvaskyla 40014, Finland
Univ Eastern Finland, Sch Comp, Kuopio 70211, Finland
Univ Reading, Henley Business Sch, Reading RG6 6UR, Berks, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. Kang; B. Song; X. J. Du; M. Guizani"	"2020"	"Adversarial Attacks for Image Segmentation on Multiple Lightweight Models"	""	"Ieee Access"	""	""	"8"	""	""	"31359-31370"	""	""	""	""	""	""	""	"Adversarial Attacks for Image Segmentation on Multiple Lightweight Models"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.2973069"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000527684600076"	""	""	"adversarial samples
image segmentation
joint learning
multi-model attack
perturbations"	"Due to the powerful ability of data fitting, deep neural networks have been applied in a wide range of applications in many key areas. However, in recent years, it was found that some adversarial samples easily fool the deep neural networks. These input samples are generated by adding a few small perturbations based on the original sample, making a very significant influence on the decision of the target model in the case of not being perceived. Image segmentation is one of the most important technologies in the medical image and automatic driving field. This paper mainly explores the security of deep neural network models based on the image segmentation tasks. Two lightweight image segmentation models on the embedded device suffered from the white-box attack by using local perturbations and universal perturbations. The perturbations are generated indirectly by a noise function and an intermediate variable so that the gradient of pixels can be propagated unlimitedly. Through experiments, we find that different models have different blind spots, and the adversarial samples trained for a single model have no transferability. In the end, multiple models are attacked by our joint learning. Finally, under the constraint of low perturbation, most of the pixels in the attacked area have been misclassified by both lightweight models. The experimental result shows that the proposed adversary is more likely to affect the performance of the segmentation model compared with the FGSM."	"Lf8sn
Times Cited:3
Cited References Count:41"	""	"<Go to ISI>://WOS:000527684600076"	""	"Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA
Qatar Univ, Dept Comp Sci & Engn, Doha 2713, Qatar"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Maiorca; A. Demontis; B. Biggio; F. Roli; G. Giacinto"	"2020"	"Adversarial Detection of Flash Malware: Limitations and Open Issues"	""	"Computers & Security"	""	""	"96"	""	""	"101901"	""	""	""	""	"Sep"	""	""	"Adversarial Detection of Flash Malware: Limitations and Open Issues"	"Comput Secur"	"0167-4048"	"ARTN 101901
10.1016/j.cose.2020.101901"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000566727400003"	""	""	"adobe flash
malware detection
secure machine learning
adversarial training
computer security
security"	"During the past four years, Flash malware has become one of the most insidious threats to detect, with almost 600 critical vulnerabilities targeting Adobe Flash Player disclosed in the wild. Research has shown that machine learning can be successfully used to detect Flash malware by leveraging static analysis to extract information from the structure of the file or its bytecode. However, the robustness of Flash malware detectors against well-crafted evasion attempts - also known as adversarial examples - has never been investigated. In this paper, we propose a security evaluation of a novel, representative Flash detector that embeds a combination of the prominent, static features employed by state-of-the-art tools. In particular, we discuss how to craft adversarial Flash malware examples, showing that it suffices to manipulate the corresponding source malware samples slightly to evade detection. We then empirically demonstrate that popular defense techniques proposed to mitigate evasion attempts, including re-training on adversarial examples, may not always be sufficient to ensure robustness. We argue that this occurs when the feature vectors extracted from adversarial examples become indistinguishable from those of benign data, meaning that the given feature representation is intrinsically vulnerable. In this respect, we are the first to formally define and quantitatively characterize this vulnerability, highlighting when an attack can be countered by solely improving the security of the learning algorithm, or when it requires also considering additional features. We conclude the paper by suggesting alternative research directions to improve the security of learning-based Flash malware detectors. (C) 2020 Elsevier Ltd. All rights reserved."	"Nk4uq
Times Cited:2
Cited References Count:64"	""	"<Go to ISI>://WOS:000566727400003"	""	"Univ Cagliari, Dept Elect & Elect Engn, Piazza dArmi Cagliari, I-09123 Cagliari, Italy
Pluribus One, Cagliari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Maiorca; D. Ariu; I. Corona; G. Giacinto"	"2015"	"A Structural and Content-based Approach for a Precise and Robust Detection of Malicious PDF Files"	""	"2015 International Conference on Information Systems Security and Privacy (Icissp)"	""	""	""	""	""	"27-36"	""	""	""	""	""	""	""	"A Structural and Content-based Approach for a Precise and Robust Detection of Malicious PDF Files"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000411700600003"	""	""	"pdf
evasion
adversarial machine learning
malware
javascript"	"During the past years, malicious PDF files have become a serious threat for the security of modern computer systems. They are characterized by a complex structure and their variety is considerably high. Several solutions have been academically developed to mitigate such attacks. However, they leveraged on information that were extracted from either only the structure or the content of the PDF file. This creates problems when trying to detect non-Javascript or targeted attacks. In this paper, we present a novel machine learning system for the automatic detection of malicious PDF documents. It extracts information from both the structure and the content of the PDF file, and it features an advanced parsing mechanism. In this way, it is possible to detect a wide variety of attacks, including non-Javascript and parsing-based ones. Moreover, with a careful choice of the learning algorithm, our approach provides a significantly higher accuracy compared to other static analysis techniques, especially in the presence of adversarial malware manipulation."	"Bi4ed
Times Cited:27
Cited References Count:39"	""	"<Go to ISI>://WOS:000411700600003"	""	"Univ Cagliari, Dept Elect & Elect Engn, Cagliari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Doku; D. B. Rawat"	"2021"	"Mitigating Data Poisoning Attacks On a Federated Learning-Edge Computing Network"	""	"2021 Ieee 18th Annual Consumer Communications & Networking Conference (Ccnc)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Mitigating Data Poisoning Attacks On a Federated Learning-Edge Computing Network"	"Consum Comm Network"	"2331-9852"	"10.1109/Ccnc49032.2021.9369581"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000668563500130"	""	""	""	"Edge Computing (EC) has seen a continuous rise in its popularity as it provides a solution to the latency and communication issues associated with edge devices transferring data to remote servers. EC achieves this by bringing the cloud closer to edge devices. Even though EC does an excellent job of solving the latency and communication issues, it does not solve the privacy issues associated with users transferring personal data to the nearby edge server. Federated Learning (FL) is an approach that was introduced to solve the privacy issues associated with data transfers to distant servers. FL attempts to resolve this issue by bringing the code to the data, which goes against the traditional way of sending the data to remote servers. In FL, the data stays on the source device, and a Machine Learning (ML) model used to train the local data is brought to the end device instead. End devices train the ML model using local data and then send the model updates back to the server for aggregation. However, this process of asking random devices to train a model using its local data has potential risks such as a participant poisoning the model using malicious data for training to produce bogus parameters. In this paper, an approach to mitigate data poisoning attacks in a federated learning setting is investigated. The application of the approach is highlighted, and the practical and secure nature of this approach is illustrated as well using numerical results."	"Br7ng
Times Cited:3
Cited References Count:26
IEEE Consumer Communications and Networking Conference"	""	"<Go to ISI>://WOS:000668563500130"	""	"Howard Univ, Dept Elect Engn & Comp Sci, Data Sci & Cybersecur Ctr DSC2, Washington, DC 20059 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. L. Zhang; B. Chen; X. Cheng; H. T. T. Binh; S. Yu"	"2021"	"PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems"	""	"Ieee Internet of Things Journal"	""	""	"8"	""	"5"	"3310-3322"	""	""	""	""	"Mar 1"	""	""	"PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems"	"Ieee Internet Things"	"2327-4662"	"10.1109/Jiot.2020.3023126"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000621420700024"	""	""	"data models
training
training data
computational modeling
gallium nitride
machine learning
servers
backdoor attack
federated learning
generative adversarial nets
label flipping
poisoning attacks"	"Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker's well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning."	"Ql9sn
Times Cited:8
Cited References Count:38"	""	"<Go to ISI>://WOS:000621420700024"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China
Nanjing Univ Aeronaut & Astronaut, Sci & Technol Avion Integrat Lab, Nanjing 211106, Peoples R China
Hanoi Univ Sci & Technol, Sch Informat & Commun Technol, Hanoi 100000, Vietnam
Univ Technol Sydney, Sch Comp Sci, Sydney, NSW 2007, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Caiazza; C. Cicconetti; V. Luconi; A. Vecchio"	"2021"	"Measurement-driven design and runtime optimization in edge computing: Methodology and tools"	""	"Computer Networks"	""	""	"194"	""	""	"108140"	""	""	""	""	"Jul 20"	""	""	"Measurement-driven design and runtime optimization in edge computing: Methodology and tools"	"Comput Netw"	"1389-1286"	"ARTN 108140
10.1016/j.comnet.2021.108140"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000659129400002"	""	""	"edge computing
etsi mec
network measurements
gsma platform operator
internet
simulation
things"	"Edge computing is projected to become the dominant form of cloud computing in the future because of the significant advantages it brings to both users (less latency, higher throughput) and telecom operators (less Internet traffic, more local management). However, to fully unlock its potential at scale, system designers and automated optimization systems alike will have to monitor closely the dynamics of both processing and communication facilities. Especially the latter is often neglected in current systems since network performance in cloud computing plays only a minor role. In this paper, we propose the architecture of MECPerf, which is a solution to collect network measurements in a live edge computing domain, to be collected for offline provisioning analysis and simulations, or to be provided in real-time for on-line system optimization. MECPerf has been validated in a realistic testbed funded by the European Commission (Fed4Fire+), and we describe here a summary of the results, which are fully available as open data and through a Python library to expedite their utilization. This is demonstrated via a use case involving the optimization of a system parameter for migrating clients in a federated edge computing system adopting the GSMA platform operator concept."	"So7bo
Times Cited:2
Cited References Count:49"	""	"<Go to ISI>://WOS:000659129400002"	""	"Univ Pisa, Dip Ing Informaz, Largo L Lazzarino 1, I-56122 Pisa, Italy
Univ Florence, Florence, Italy
CNR, Ist Informat & Telemat, Via G Moruzzi 1, I-56124 Pisa, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. R. Wang; S. Wen; A. Jolfaei; M. S. Haghighi; S. Nepal; Y. Xiang"	"2022"	"On the Neural Backdoor of Federated Generative Models in Edge Computing"	""	"Acm Transactions on Internet Technology"	""	""	"22"	""	"2"	"1 - 21"	""	""	""	""	"May"	""	""	"On the Neural Backdoor of Federated Generative Models in Edge Computing"	"Acm T Internet Techn"	"1533-5399"	"Artn 43
10.1145/3425662"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000798201400016"	""	""	"deep learning
neural backdoor
generative neural networks
federated learning
edge computing
cloud computing"	"Edge computing, as a relatively recent evolution of cloud computing architecture, is the newest way for enterprises to distribute computational power and lower repetitive referrals to central authorities. In the edge computing environment, Generative Models (GMs) have been found to be valuable and useful in machine learning tasks such as data augmentation and data pre-processing. Federated learning and distributed learning refer to training machine learning models in the edge computing network. However, federated learning and distributed learning also bring additional risks to GMs since all peers in the network have access to the model under training. In this article, we study the vulnerabilities of federated GMs to data-poisoning-based backdoor attacks via gradient uploading. We additionally enhance the attack to reduce the required poisonous data samples and cope with dynamic network environments. Last but not least, the attacks are formally proven to be stealthy and effective toward federated GMs. According to the experiments, neural backdoors can be successfully embedded by including merely 5% poisonous samples in the local training dataset of an attacker."	"1j8zi
Times Cited:0
Cited References Count:36"	""	"<Go to ISI>://WOS:000798201400016"	""	"Swinburne Univ Technol, John St, Hawthorn, Vic 3122, Australia
CSIRO, Data61, Corner Vimiera & Pembroke Rd, Marsfield, NSW 2122, Australia
Macquarie Univ, Balaclava Rd, Macquarie Pk, NSW 2109, Australia
Univ Tehran, North Kargar Ave, Tehran 1939957131, Iran"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Bian; L. B. Meng; D. R. Wu"	"2022"	"SSVEP-based brain-computer interfaces are vulnerable to square wave attacks"	""	"Science China-Information Sciences"	""	""	"65"	""	"4"	""	""	""	""	""	"Apr"	""	""	"SSVEP-based brain-computer interfaces are vulnerable to square wave attacks"	"Sci China Inform Sci"	"1674-733x"	"ARTN 140406
10.1007/s11432-022-3440-5"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000771626400005"	""	""	"electroencephalogram
brain-computer interface
steady-state visual evoked potential
adversarial attack"	"Electroencephalogram (EEG) based brain-computer interfaces (BCIs) have attracted wide attention in recent years. Steady-state visual evoked potential (SSVEP) is one of the most popular BCI paradigms, which has high information transmission rate and short user calibration time. Recent research has shown that EEG-based BCIs are under the threat of adversarial examples. However, existing attack approaches are very difficult to implement in a real-world system. Considering SSVEP's dependency on the frequency information, this paper proposes to use square wave signals as adversarial perturbations to attack SSVEP-based BCIs, which are easy to generate and apply in practice. EEG trials contaminated by the square wave perturbation can be classified into any target class specified by the attacker. Compared with previous approaches, our perturbation can be implemented much more easily. Experiments on two SSVEP datasets demonstrated the efficiency and robustness of the proposed approach in attacking two SSVEP classifiers based on canonical correlation analysis, exposing a critical security problem in SSVEP-based BCIs."	"Zx0ya
Times Cited:0
Cited References Count:39"	""	"<Go to ISI>://WOS:000771626400005"	""	"Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China
Zhejiang Lab, Hangzhou 311121, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. R. Liu; X. R. Xu; W. Wang"	"2022"	"Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives"	""	"Cybersecurity"	""	""	"5"	""	"1"	"1-19"	""	""	""	""	"Feb 2"	""	""	"Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives"	"Cybersecurity"	"2523-3246"	"ARTN 4
10.1186/s42400-021-00105-6"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000749534600001"	""	""	"federated learning
security and privacy threats
multi-phases
inference attacks
poisoning attacks
evasion attacks
defenses
trusted
privacy
networks
secure"	"Empirical attacks on Federated Learning (FL) systems indicate that FL is fraught with numerous attack surfaces throughout the FL execution. These attacks can not only cause models to fail in specific tasks, but also infer private information. While previous surveys have identified the risks, listed the attack methods available in the literature or provided a basic taxonomy to classify them, they mainly focused on the risks in the training phase of FL. In this work, we survey the threats, attacks and defenses to FL throughout the whole process of FL in three phases, including Data and Behavior Auditing Phase, Training Phase and Predicting Phase. We further provide a comprehensive analysis of these threats, attacks and defenses, and summarize their issues and taxonomy. Our work considers security and privacy of FL based on the viewpoint of the execution process of FL. We highlight that establishing a trusted FL requires adequate measures to mitigate security and privacy threats at each phase. Finally, we discuss the limitations of current attacks and defense approaches and provide an outlook on promising future research directions in FL."	"Yq8dl
Times Cited:0
Cited References Count:132"	""	"<Go to ISI>://WOS:000749534600001"	""	"Beijing Jiaotong Univ, Beijing Key Lab Secur & Privacy Intelligent Trans, Beijing 100044, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Zhang; B. Shen; A. Barnawi; S. Xi; N. Kumar; Y. Wu"	"2021"	"FedDPGAN: Federated Differentially Private Generative Adversarial Networks Framework for the Detection of COVID-19 Pneumonia"	""	"Inf Syst Front"	""	""	"23"	""	"6"	"1403-1415"	""	""	""	"2021/06/22"	""	""	""	"FedDPGAN: Federated Differentially Private Generative Adversarial Networks Framework for the Detection of COVID-19 Pneumonia"	""	"1387-3326 (Print)
1387-3326 (Linking)"	"10.1007/s10796-021-10144-6"	""	""	""	""	"PMC8204125"	""	""	""	""	""	""	"34149305"	""	""	"Covid-19
Differential privacy
Federated learning
Generative adversarial networks
Privacy protection"	"Existing deep learning technologies generally learn the features of chest X-ray data generated by Generative Adversarial Networks (GAN) to diagnose COVID-19 pneumonia. However, the above methods have a critical challenge: data privacy. GAN will leak the semantic information of the training data which can be used to reconstruct the training samples by attackers, thereby this method will leak the privacy of the patient. Furthermore, for this reason, that is the limitation of the training data sample, different hospitals jointly train the model through data sharing, which will also cause privacy leakage. To solve this problem, we adopt the Federated Learning (FL) framework, a new technique being used to protect data privacy. Under the FL framework and Differentially Private thinking, we propose a Federated Differentially Private Generative Adversarial Network (FedDPGAN) to detect COVID-19 pneumonia for sustainable smart cities. Specifically, we use DP-GAN to privately generate diverse patient data in which differential privacy technology is introduced to make sure the privacy protection of the semantic information of the training dataset. Furthermore, we leverage FL to allow hospitals to collaboratively train COVID-19 models without sharing the original data. Under Independent and Identically Distributed (IID) and non-IID settings, the evaluation of the proposed model is on three types of chest X-ray (CXR)images dataset (COVID-19, normal, and normal pneumonia). A large number of truthful reports make the verification of our model can effectively diagnose COVID-19 without compromising privacy."	"Zhang, Longling
Shen, Bochen
Barnawi, Ahmed
Xi, Shan
Kumar, Neeraj
Wu, Yi
eng
Inf Syst Front. 2021;23(6):1403-1415. doi: 10.1007/s10796-021-10144-6. Epub 2021 Jun 15."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34149305"	""	"School of Data Science and Technology, Heilongjiang University, Harbin, 150080 China.grid.412067.60000 0004 1760 1291
King Abdul Aziz University, Riyadh, 11543 Saudi Arabia.
Thapar Institute of Engineering and Technology, Pariala, India.grid.412436.60000 0004 0500 6866
School of Computer Science, University of Petroleum and Energy Studies, Dehradun Uttarakhand, India.grid.444415.40000 0004 1759 0860
Department of Computer Science and Information Engineering, Asia University, Taiwan, China.grid.252470.60000 0000 9263 9645"	""	""	""	""	""	""	""	""
"Journal Article"	"A. Richardson; G. Dozier; M. C. King; R. Chapman"	"2020"	"'Uh-oh Spaghetti-oh': When Successful Genetic and Evolutionary Feature Selection Makes You More Susceptible to Adversarial Authorship Attacks"	""	"2020 Ieee International Conference on Systems, Man, and Cybernetics (Smc)"	""	""	""	""	""	"567-571"	""	""	""	""	""	""	""	"'Uh-oh Spaghetti-oh': When Successful Genetic and Evolutionary Feature Selection Makes You More Susceptible to Adversarial Authorship Attacks"	"Ieee Sys Man Cybern"	"1062-922x"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000687430600088"	""	""	""	"Feature selection is a technique used to reduce an original set of features to a subset containing the most salient features. Reducing the feature set to the most significant subset of features typically results in an increase in the overall accuracy of a system. It has been shown that in some cases, the use of feature selection can make an underlying system susceptible to adversarial attacks. In this paper, we investigate the susceptibility of a feature selection-based Authorship Attribution System (AAS) to adversarial authorship attacks. The AAS studied is an instance of a Linear Support Vector Machine (LSVM). The feature selection algorithm used is an instance of Genetic & Evolutionary Feature Selection (GEFeS).
In order to evaluate the GEFeS+LSVM-based AAS, we use three adversarial authorship masking techniques to generate adversarial texts to attack the AAS. Our results show that in some cases the GEFeS+LSVM-based AAS is more susceptible to adversarial authorship attacks. We provide a simple measurement to determine whether the use of GEFeS is beneficial or detrimental to a LSVM-based AAS."	"Bs1dd
Times Cited:0
Cited References Count:36
IEEE International Conference on Systems Man and Cybernetics Conference Proceedings"	""	"<Go to ISI>://WOS:000687430600088"	""	"Auburn Univ, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA
Florida Inst Technol, Sch Comp, Melbourne, FL 32901 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Li; T. Li; H. Liu"	"2017"	"Recent advances in feature selection and its applications"	""	"Knowledge and Information Systems"	""	""	"53"	""	"3"	"551-577"	""	""	""	""	"Dec"	""	""	"Recent advances in feature selection and its applications"	"Knowl Inf Syst"	"0219-1377"	"10.1007/s10115-017-1059-8"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000412952200001"	""	""	"feature selection
survey
data mining
online feature-selection
gene selection
classification
regression
cancer
relevance
security"	"Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval."	"Fj7pw
Times Cited:123
Cited References Count:104"	""	"<Go to ISI>://WOS:000412952200001"	""	"Nanjing Univ Posts & Telecommun, Sch Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China
Nanjing Univ Posts & Telecommun, Jiangsu Key Lab Big Data Secur & Intelligent Proc, Nanjing, Jiangsu, Peoples R China
Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA
Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"U. M. Khaire; R. Dhanalakshmi"	"2022"	"Stability of feature selection algorithm: A review"	""	"Journal of King Saud University-Computer and Information Sciences"	""	""	"34"	""	"4"	"1060-1073"	""	""	""	""	"Apr"	""	""	"Stability of feature selection algorithm: A review"	"J King Saud Univ-Com"	"1319-1578"	"10.1016/j.jksuci.2019.06.012"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000782989300003"	""	""	"feature selection
knowledge discovery
stability
robustness
instability
perturbation
differential evolution
robust
dimensionality
optimization
extraction
smoothness
proteomics
diagnosis
sparsity
bias"	"Feature selection technique is a knowledge discovery tool which provides an understanding of the problem through the analysis of the most relevant features. Feature selection aims at building better classifier by listing significant features which also helps in reducing computational overload. Due to existing high throughput technologies and their recent advancements are resulting in high dimensional data due to which feature selection is being treated as handy and mandatory in such datasets. This actually questions the interpretability and stability of traditional feature selection algorithms. The high correlation in features frequently produces multiple equally optimal signatures, which makes traditional feature selection method unstable and thus leading to instability which reduces the confidence of selected features. Stability is the robustness of the feature preferences it produces to perturbation of training samples. Stability indicates the reproducibility power of the feature selection method. High stability of the feature selection algorithm is equally important as the high classification accuracy when evaluating feature selection performance. In this paper, we provide an overview of feature selection techniques and instability of the feature selection algorithm. We also present some of the solutions which can handle the different source of instability.(c) 2019 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an"	"0n7bm
Times Cited:61
Cited References Count:102"	""	"<Go to ISI>://WOS:000782989300003"	""	"Natl Inst Technol Nagaland, Dept Comp Sci & Engn, Dimapur, India
Natl Inst Technol Puducherry, Dept Comp Sci & Engn, Karaikal, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. X. Lai; X. L. Huang; X. Z. Gao; C. Xia; J. Y. Hua"	"2022"	"GAN-Based Information Leakage Attack Detection in Federated Learning"	""	"Security and Communication Networks"	""	""	"2022"	""	""	""	""	""	""	""	"Mar 23"	""	""	"GAN-Based Information Leakage Attack Detection in Federated Learning"	"Secur Commun Netw"	"1939-0114"	"Artn 4835776
10.1155/2022/4835776"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000802180800003"	""	""	"privacy"	"Federated learning (FL) has been a popular distributed learning framework to reduce privacy risks by keeping private data locally. However, recent work (Hitaj 2017) has demonstrated that sharing model's parameter updates still leaves FL vulnerable to internal attacks in its training phase. Existing works cannot detect such attacks well. To address this problem, we propose a novel and lightweight detection scheme which selects and analyzes just a few parameter updates of the last convolutional layer in the FL model. Extensive experiments demonstrate that our proposed detection scheme can accurately and efficiently detect the malicious participant in near real time for a scenario with a malicious participant."	"1p7jj
Times Cited:0
Cited References Count:31"	""	"<Go to ISI>://WOS:000802180800003"	""	"Nanjing Univ, Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China
Global EnergyInterconnect Res Inst, Nanjing Branch, State Grid Key Lab Informat & Network Secur, Nanjing, Jiangsu, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Li; Y. X. Fan; M. Tse; K. Y. Lin"	"2020"	"A review of applications in federated learning"	""	"Computers & Industrial Engineering"	""	""	"149"	""	""	"106854"	""	""	""	""	"Nov"	""	""	"A review of applications in federated learning"	"Comput Ind Eng"	"0360-8352"	"ARTN 106854
10.1016/j.cie.2020.106854"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000582320000070"	""	""	"federated learning
literature review
citation analysis
research front
design"	"Federated Learning (FL) is a collaboratively decentralized privacy-preserving technology to overcome challenges of data silos and data sensibility. Exactly what research is carrying the research momentum forward is a question of interest to research communities as well as industrial engineering. This study reviews FL and explores the main evolution path for issues exist in FL development process to advance the understanding of FL. This study aims to review prevailing application in industrial engineering to guide for the future landing application. This study also identifies six research fronts to address FL literature and help advance our understanding of FL for future optimization. This study contributes to conclude application in industrial engineering and computer science and summarize a review of applications in FL."	"Oh1gl
Times Cited:38
Cited References Count:113"	""	"<Go to ISI>://WOS:000582320000070"	""	"Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China
Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 201804, Peoples R China
Cardiff Univ, Cardiff Business Sch, Cardiff CF10 3AT, Wales"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Du; C. Wu; T. Yoshinaga; K. A. Yau; Y. Ji; J. Li"	"2020"	"Federated Learning for Vehicular Internet of Things: Recent Advances and Open Issues"	""	"IEEE Comput Graph Appl"	""	""	"1"	""	""	"45-61"	""	""	""	"2020/05/10"	"May 5"	""	""	"Federated Learning for Vehicular Internet of Things: Recent Advances and Open Issues"	""	"1558-1756 (Electronic)
0272-1716 (Linking)"	"10.1109/OJCS.2020.2992630"	""	""	""	""	""	""	""	""	""	""	""	"32386144"	""	""	""	"Federated learning (FL) is a distributed machine learning approach that can achieve the purpose of collaborative learning from a large amount of data that belong to different parties without sharing the raw data among the data owners. FL can sufficiently utilize the computing capabilities of multiple learning agents to improve the learning efficiency while providing a better privacy solution for the data owners. FL attracts tremendous interests from a large number of industries due to growing privacy concerns. Future vehicular Internet of Things (IoT) systems, such as cooperative autonomous driving and intelligent transport systems (ITS), feature a large number of devices and privacy-sensitive data where the communication, computing, and storage resources must be efficiently utilized. FL could be a promising approach to solve these existing challenges. In this paper, we first conduct a brief survey of existing studies on FL and its use in wireless IoT. Then we discuss the significance and technical challenges of applying FL in vehicular IoT, and point out future research directions."	"Du, Zhaoyang
Wu, Celimuge
Yoshinaga, Tsutomu
Yau, Kok-Lim Alvin
Ji, Yusheng
Li, Jie
eng
IEEE Comput Graph Appl. 2020 May 5. doi: 10.1109/OJCS.2020.2992630."	""	"https://www.ncbi.nlm.nih.gov/pubmed/32386144"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"V. Mothukuri; R. M. Parizi; S. Pouriyeh; Y. Huang; A. Dehghantanha; G. Srivastava"	"2021"	"A survey on security and privacy of federated learning"	""	"Future Generation Computer Systems-the International Journal of Escience"	""	""	"115"	""	""	"619-640"	""	""	""	""	"Feb"	""	""	"A survey on security and privacy of federated learning"	"Future Gener Comp Sy"	"0167-739x"	"10.1016/j.future.2020.10.007"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000592029600005"	""	""	"artificial intelligence
machine learning
distributed learning
federated learning
federated machine learning
security
privacy
training data
deep
attacks
system"	"Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL's security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios. (C) 2020 Elsevier B.V. All rights reserved."	"Ov2ek
Times Cited:83
Cited References Count:204"	""	"<Go to ISI>://WOS:000592029600005"	""	"Kennesaw State Univ, Dept Software Engn & Game Dev, Kennesaw, GA 30060 USA
Kennesaw State Univ, Dept Informat Technol, Kennesaw, GA 30060 USA
Univ Guelph, Cyber Sci Lab, Sch Comp Sci, Guelph, ON, Canada
Brandon Univ, Dept Math & Comp Sci, Brandon, MB, Canada
China Med Univ, Res Ctr Interneural Comp, Taichung 40402, Taiwan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Li; Y. X. Fan; K. Y. Lin"	"2020"	"A Survey on federated learning"	""	"2020 Ieee 16th International Conference on Control & Automation (Icca)"	""	""	""	""	""	"791-796"	""	""	""	""	""	""	""	"A Survey on federated learning"	"Ieee Int Conf Con Au"	"1948-3449"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000646357300135"	""	""	"federated learning
literature survey
citation analysis
research front"	"Federated learning (FL) is an emerging setting which implement machine learning in a distributed environment while protecting privacy. Research activities relating to FLhave grown at a fast rate recently in control. Exactly what activities have been carrying the research momentum forward is a question of interest to the research community. This study finds these research activities and optimization path of FL based on survey. Thus, this study aims to review related studies of FL to base on the baseline a universal definition gives a guiding for the future work. Besides, this study presents the prevailing FL applications and the evolution of federated learning. In the end, this study also identifies four research fronts to enrich the FL literature and help advance our understanding of the field. A comprehensive taxonomy of FL can also be developed through analyzing the results of this review."	"Br3hm
Times Cited:2
Cited References Count:63
IEEE International Conference on Control and Automation ICCA"	""	"<Go to ISI>://WOS:000646357300135"	""	"Tongji Univ, Shanghai 201804, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. L. Zhang; J. J. Chen; D. Wu; B. Chen; S. Yu"	"2019"	"Poisoning Attack in Federated Learning using Generative Adversarial Nets"	""	"2019 18th Ieee International Conference on Trust, Security and Privacy in Computing and Communications/13th Ieee International Conference on Big Data Science and Engineering (Trustcom/Bigdatase 2019)"	""	""	""	""	""	"374-380"	""	""	""	""	""	""	""	"Poisoning Attack in Federated Learning using Generative Adversarial Nets"	"Ieee Trust Big"	"2324-9013"	"10.1109/TrustCom/BigDataSE.2019.00057"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000567324100046"	""	""	"federated learning
poisoning attack
generative adversarial nets
security
privacy"	"Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80% accuracy on both poisoning tasks and main tasks."	"Bp8tq
Times Cited:27
Cited References Count:22
IEEE Trustcom BigDataSE ISPA"	""	"<Go to ISI>://WOS:000567324100046"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China
Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China
Univ Technol Sydney, Sch Software, Sydney, NSW 2007, Australia
Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW 2007, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Shen; T. Q. Zhu; D. Wu; W. Wang; W. L. Zhou"	"2020"	"From distributed machine learning to federated learning: In the view of data privacy and security"	""	"Concurrency and Computation-Practice & Experience"	""	""	"abs/2010.09258"	""	""	""	""	""	""	""	"Sep 23"	""	""	"From distributed machine learning to federated learning: In the view of data privacy and security"	"Concurr Comp-Pract E"	"1532-0626"	"10.1002/cpe.6002"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000571861400001"	""	""	"data privacy
distributed machine learning
federated learning
security
fully homomorphic encryption
computation
attacks"	"Federated learning is an improved version of distributed machine learning that further offloads operations which would usually be performed by a central server. The server becomes more like an assistant coordinating clients to work together rather than micromanaging the workforce as in traditional DML. One of the greatest advantages of federated learning is the additional privacy and security guarantees it affords. Federated learning architecture relies on smart devices, such as smartphones and IoT sensors, that collect and process their own data, so sensitive information never has to leave the client device. Rather, clients train a submodel locally and send an encrypted update to the central server for aggregation into the global model. These strong privacy guarantees make federated learning an attractive choice in a world where data breaches and information theft are common and serious threats. This survey outlines the landscape and latest developments in data privacy and security for federated learning. We identify the different mechanisms used to provide privacy and security, such as differential privacy, secure multiparty computation and secure aggregation. We also survey the current attack models, identifying the areas of vulnerability and the strategies adversaries use to penetrate federated systems. The survey concludes with a discussion on the open challenges and potential directions of future work in this increasingly popular learning paradigm."	"Nr9ct
Times Cited:3
Cited References Count:73"	""	"<Go to ISI>://WOS:000571861400001"	""	"Univ Technol Sydney, Ctr Cyber Secur & Privacy, Sch Comp Sci, Ultimo, NSW, Australia
Univ Technol Sydney, Sch Comp Sci, Ultimo, NSW, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. X. Zhou; Y. Sun; D. G. Wang; Q. Gao"	"2022"	"Fed-Fi: Federated Learning Malicious Model Detection Method Based on Feature Importance"	""	"Security and Communication Networks"	""	""	"2022"	""	""	""	""	""	""	""	"May 18"	""	""	"Fed-Fi: Federated Learning Malicious Model Detection Method Based on Feature Importance"	"Secur Commun Netw"	"1939-0114"	"Artn 7268347
10.1155/2022/7268347"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000805154100005"	""	""	"attacks"	"Federated learning, as a new security exchange paradigm, is widely used in medical care, driverless cars, finance, and other fields. However, federated learning still faces the problem of sybil attacks common in distributed frameworks. The existing schemes mainly defend against malicious model attacks with distance comparison, neural network, and confidence vote. But they are significantly limited in dealing with collusive sybil attacks. Therefore, we propose a federated learning malicious model detection method based on feature importance (Fed-Fi). Firstly, we screen important features by feature importance reasoning method based on LRP and compare the similarity based on Hamming distance between important features. Then, we adjust model learning rate adaptively to reduce the effect of collusive sybil attacks on the global model. The experimental results indicate that it can effectively resist the attack of collusive sybils in federated learning."	"1u1bk
Times Cited:0
Cited References Count:32"	""	"<Go to ISI>://WOS:000805154100005"	""	"Informat Engn Univ, Zhengzhou 450001, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Pope; Y. Balaji; S. Feizi"	"2020"	"Adversarial Robustness of Flow-Based Generative Models"	""	"International Conference on Artificial Intelligence and Statistics, Vol 108"	""	""	"108"	""	""	"3795-3804"	""	""	""	""	""	""	""	"Adversarial Robustness of Flow-Based Generative Models"	"Pr Mach Learn Res"	"2640-3498"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000559931302079"	""	""	""	"Flow-based generative models leverage invertible generator functions to fit a distribution to the training data using maximum likelihood. Despite their use in several application domains, robustness of these models to adversarial attacks has hardly been explored. In this paper, we study adversarial robustness of flow-based generative models both theoretically (for some simple models) and empirically (for more complex ones). First, we consider a linear flow-based generative model and compute optimal sample-specific and universal adversarial perturbations that maximally decrease the likelihood scores. Using this result, we study the robustness of the well-known adversarial training procedure, where we characterize the fundamental trade-off between model robustness and accuracy. Next, we empirically study the robustness of two prominent deep, nonlinear, flow-based generative models, namely GLOW and RealNVP. We design two types of adversarial attacks; one that minimizes the likelihood scores of in-distribution samples, while the other that maximizes the likelihood scores of out-of-distribution ones. We find that GLOW and RealNVP are extremely sensitive to both types of attacks. Finally, using a hybrid adversarial training procedure, we significantly boost the robustness of these generative models."	"Bp6lu
Times Cited:0
Cited References Count:28
Proceedings of Machine Learning Research"	""	"<Go to ISI>://WOS:000559931302079"	""	"Univ Maryland, College Pk, MD 20742 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Alman; T. Chu; A. Schild; Z. Song"	"2020"	"Algorithms and Hardness for Linear Algebra on Geometric Graphs"	""	"2020 Ieee 61st Annual Symposium on Foundations of Computer Science (Focs 2020)"	""	""	""	""	""	"541-552"	""	""	""	""	""	""	""	"Algorithms and Hardness for Linear Algebra on Geometric Graphs"	"Ann Ieee Symp Found"	"0272-5428"	"10.1109/Focs46700.2020.00057"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000652333400049"	""	""	"kernel
sparsification
integrals"	"For a function K : R-d x R-d -> R->= 0, and a set P = {x(1), ..., x(n)} subset of R-d of n points, the K graph G(P) of P is the complete graph on n nodes where the weight between nodes i and j is given by K(x(i), x(j)). In this paper, we initiate the study of when efficient spectral graph theory is possible on these graphs. We investigate whether or not it is possible to solve the following problems in n(1+o(1)) time for a K-graph G(P) when d < n(o(1)):
Multiply a given vector by the adjacency matrix or Laplacian matrix of G(P)
Find a spectral sparsifier of G(P)
Solve a Laplacian system in G(P)'s Laplacian matrix
For each of these problems, we consider all functions of the form K(u, v) = f(parallel to u - v parallel to(2)(2)) for a function f : R -> R. We provide algorithms and comparable hardness results for many such K, including the Gaussian kernel, Neural tangent kernels, and more. For example, in dimension d = Omega(log n), we show that there is a parameter associated with the function f for which low parameter values imply n(1+o(1)) time algorithms for all three of these problems and high parameter values imply the nonexistence of subquadratic time algorithms assuming Strong Exponential Time Hypothesis (SETH), given natural assumptions on f.
As part of our results, we also show that the exponential dependence on the dimension d in the celebrated fast multi-pole method of Greengard and Rokhlin cannot be improved, assuming SETH, for a broad class of functions f. To the best of our knowledge, this is the first formal limitation proven about fast multipole methods."	"Br4qy
Times Cited:0
Cited References Count:95
Annual IEEE Symposium on Foundations of Computer Science"	""	"<Go to ISI>://WOS:000652333400049"	""	"Harvard Univ, Cambridge, MA 02138 USA
Carnegie Mellon Univ, Pittsburgh, PA 15213 USA
Univ Washington, Seattle, WA 98195 USA
IAS, Columbia, MO USA
IAS, Princeton, NJ USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. M. Wu; X. Y. Li; Y. J. Deng"	"2020"	"Deep learning-driven wireless communication for edge-cloud computing: opportunities and challenges"	""	"Journal of Cloud Computing-Advances Systems and Applications"	""	""	"9"	""	"1"	"1-14"	""	""	""	""	"Apr 10"	""	""	"Deep learning-driven wireless communication for edge-cloud computing: opportunities and challenges"	"J Cloud Comput-Adv S"	""	"ARTN 21
10.1186/s13677-020-00168-9"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000525197000001"	""	""	"wireless communication
future network
security
edge-cloud computing
internet of things
high-reliability
csi feedback
low-latency
autoencoder"	"Future wireless communications are becoming increasingly complex with different radio access technologies, transmission backhauls, and network slices, and they play an important role in the emerging edge computing paradigm, which aims to reduce the wireless transmission latency between end-users and edge clouds. Deep learning techniques, which have already demonstrated overwhelming advantages in a wide range of internet of things (IoT) applications, show significant promise for solving such complicated real-world scenarios. Although the convergence of radio access networks and deep learning is still in the preliminary exploration stage, it has already attracted tremendous concern from both academia and industry. To address emerging theoretical and practical issues, ranging from basic concepts to research directions in future wireless networking applications and architectures, this paper mainly reviews the latest research progress and major technological deployment of deep learning in the development of wireless communications. We highlight the intuitions and key technologies of deep learning-driven wireless communication from the aspects of end-to-end communication, signal detection, channel estimation and compression sensing, encoding and decoding, and security and privacy. Main challenges, potential opportunities and future trends in incorporating deep learning schemes in wireless communications environments are further illustrated."	"Lc3af
Times Cited:9
Cited References Count:61"	""	"<Go to ISI>://WOS:000525197000001"	""	"Tianjin Univ, Ctr Appl Math, Weijin Rd, Tianjin, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. X. Jia; I. C. Konstantakopoulos; B. Li; C. Spanos"	"2018"	"Poisoning Attacks on Data-Driven Utility Learning in Games"	""	"2018 Annual American Control Conference (Acc)"	""	""	""	""	""	"5774-5780"	""	""	""	""	""	""	""	"Poisoning Attacks on Data-Driven Utility Learning in Games"	"P Amer Contr Conf"	"0743-1619"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000591256605137"	""	""	""	"Game theory has been employed for modeling agents' decisions in various science and engineering fields. Game theoretic analysis often assumes that the utility function of each agent is known a priori, and yet this assumption does not hold for many real-world applications. The combination of Internet of Things (IoT) and advanced data analysis techniques has stimulated fruitful research on learning agents' utility functions from data. Just as many other data-driven methods, utility learning also suffers from potential security risks. Due to the great economic value of accurate forecasting of agents' behaviors, there are huge incentives for adversaries to attack utility learning methods by poisoning training datasets and mislead predictions to achieve malicious goals. In this paper, we introduce and analyze optimal poisoning attack strategies in order to understand adversarial actions and further encourage potential defenses. Moreover, we study how an adversary might disguise the attacks by mimicking normal actions. The proposed attack strategies are evaluated on both synthetic and real-world social energy game data, and the results show that the root mean squared error in predicting agents' actions increases by up to 67% by adding only 5% well-crafted poisoning training instances."	"Bq4nj
Times Cited:3
Cited References Count:23
Proceedings of the American Control Conference"	""	"<Go to ISI>://WOS:000591256605137"	""	"Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA
Alexander S Onassis Publ Benefit Fdn, Athens, Greece"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Q. Pan; W. J. Yu; X. K. Yi; A. Khan; F. Yuan; Y. H. Zheng"	"2019"	"Recent Progress on Generative Adversarial Networks (GANs): A Survey"	""	"Ieee Access"	""	""	"7"	""	""	"36322-36333"	""	""	""	""	""	""	""	"Recent Progress on Generative Adversarial Networks (GANs): A Survey"	"Ieee Access"	"2169-3536"	"10.1109/Access.2019.2905015"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000464565100001"	""	""	"deep learning
machine learning
unsupervised learning
generative adversarial networks"	"Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed."	"Ht4wx
Times Cited:159
Cited References Count:82"	""	"<Go to ISI>://WOS:000464565100001"	""	"Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Jiangsu Collaborat Innovat Ctr Atmospher, Jiangsu Engn Ctr Network Monitoring, Nanjing 210044, Jiangsu, Peoples R China
Pakistan Inst Engn & Appl Sci, Dept Comp & Informat Sci, Nilore 45650, Pakistan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. T. Hu; R. J. Xie; Z. G. Lu; A. Q. Hu; M. H. Xue"	"2021"	"TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"2096-2112"	""	""	""	""	""	""	""	"TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing"	""	""	"10.1145/3460120.3485251"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478302008"	""	""	"membership privacy
differential privacy
generative adversarial networks (gans)
synthetic data releasing
privacy
noise"	"Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator.
Our experimental evaluations on TableGAN-MCA have five main findings. First, Tab/eGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA."	"Bs7xr
Times Cited:1
Cited References Count:47"	""	"<Go to ISI>://WOS:000768478302008"	""	"Southeast Univ, Nanjing, Peoples R China
Macquarie Univ, Sydney, NSW, Australia
Univ Adelaide, Adelaide, SA, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"E. Ayanoglu; K. Davaslioglu; Y. E. Sagduyu"	"2022"	"Machine Learning in NextG Networks via Generative Adversarial Networks"	""	"Ieee Transactions on Cognitive Communications and Networking"	""	""	"8"	""	"2"	"480-501"	""	""	""	""	"Jun"	""	""	"Machine Learning in NextG Networks via Generative Adversarial Networks"	"Ieee T Cogn Commun"	"2332-7731"	"10.1109/Tccn.2022.3153004"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000808086800007"	""	""	"generative adversarial networks (gans)
conditional gans
generative modeling
spectrum sharing
anomaly detection
outlier detection
wireless security
unsupervised learning
cognitive radio
anomaly detection
wireless networks
attacks
layer"	"Generative Adversarial Networks (GANs) are Machine Learning (ML) algorithms that have the ability to address competitive resource allocation problems together with detection and mitigation of anomalous behavior. In this paper, we investigate their use in next-generation (NextG) communications within the context of cognitive networks to address i) spectrum sharing, ii) detecting anomalies, and iii) mitigating security attacks. GANs have the following advantages. First, they can learn and synthesize field data, which can be costly, time consuming, and nonrepeatable. Second, they enable pre-training classifiers by using semi-supervised data. Third, they facilitate increased resolution. Fourth, they enable the recovery of corrupted bits in the spectrum. The paper provides the basics of GANs, a comparative discussion on different kinds of GANs, performance measures for GANs in computer vision and image processing as well as wireless applications, a number of datasets for wireless applications, performance measures for general classifiers, a survey of the literature on GANs for i)-iii) above, and future research directions. As a use case of GAN for NextG communications, we show that a GAN can be effectively applied for anomaly detection in signal classification (e.g., user authentication) outperforming another state-of-the-art ML technique, an autoencoder."	"1y4bn
Times Cited:0
Cited References Count:183"	""	"<Go to ISI>://WOS:000808086800007"	""	"Univ Calif Irvine, Ctr Pervas Commun & Comp CPCC, Dept EECS, Irvine, CA 92695 USA
UTS Inc, Greenbelt, MD 20770 USA
BlueHalo Co, Intelligent Automat, Rockville, MD 20855 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. L. Hu; J. Pang"	"2021"	"Membership Inference Attacks against GANs by Leveraging Over-representation Regions"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"2387-2389"	""	""	""	""	""	""	""	"Membership Inference Attacks against GANs by Leveraging Over-representation Regions"	""	""	"10.1145/3460120.3485338"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478302028"	""	""	"membership inference attacks
generative adversarial networks
human face generation
over-representation"	"Generative adversarial networks (GANs) have made unprecedented performance in image synthesis and play a key role in various downstream applications of computer vision. However, GAN models trained on sensitive data also pose a distinct threat to privacy. In this poster, we present a novel over-representation based membership inference attack. Unlike prior attacks against GANs which focus on the overall metrics, such as the attack accuracy, our attack aims to make inference from the high-precision perspective, which allows the adversary to concentrate on inferring a sample as a member confidently. Initial experimental results demonstrate that the adversary can achieve a high precision attack even if the overall attack accuracy is about 50% for a well-trained GAN model. Our work will raise awareness of the importance of precision when GAN owners evaluate the privacy risks of their models."	"Bs7xr
Times Cited:0
Cited References Count:16"	""	"<Go to ISI>://WOS:000768478302028"	""	"Univ Luxembourg, SnT, Esch Sur Alzette, Luxembourg
Univ Luxembourg, FSTM, Esch Sur Alzette, Luxembourg"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. X. Deng; Q. Sang"	"2020"	"Harnessing the Adversarial Perturbation to Enhance Security in the Autoencoder-Based Communication System"	""	"Electronics"	""	""	"9"	""	"2"	"294"	""	""	""	""	"Feb"	""	""	"Harnessing the Adversarial Perturbation to Enhance Security in the Autoencoder-Based Communication System"	"Electronics-Switz"	""	"ARTN 294
10.3390/electronics9020294"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000518412200094"	""	""	"deep learning
physical layer security
autoencoder communication system
adversarial attacks
adversarial training
physical layer security
channel estimation
wireless networks
attacks
power"	"Given the vulnerability of deep neural network to adversarial attacks, the application of deep learning in the wireless physical layer arouses comprehensive security concerns. In this paper, we consider an autoencoder-based communication system with a full-duplex (FD) legitimate receiver and an external eavesdropper. It is assumed that the system is trained from end-to-end based on the concepts of autoencoder. The FD legitimate receiver transmits a well-designed adversary perturbation signal to jam the eavesdropper while receiving information simultaneously. To defend the self-perturbation from the loop-back channel, the legitimate receiver is re-trained with the adversarial training method. The simulation results show that with the scheme proposed in this paper, the block-error-rate (BLER) of the legitimate receiver almost remains unaffected while the BLER of the eavesdropper is increased by orders of magnitude. This ensures reliable and secure transmission between the transmitter and the legitimate receiver."	"Ks6ja
Times Cited:0
Cited References Count:27"	""	"<Go to ISI>://WOS:000518412200094"	""	"Hohai Univ, Coll Internet Things Engn, Changzhou 213022, Jiangsu, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. D. Gao; R. Hu; Y. M. Gong"	"2020"	"Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing"	""	"2020 Ieee Global Communications Conference (Globecom)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing"	"Ieee Glob Comm Conf"	"2334-0983"	"10.1109/Globecom42002.2020.9322576"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000668970503035"	""	""	""	"Graph classification has practical applications in diverse fields. Recent studies show that graph-based machine learning models are especially vulnerable to adversarial perturbations due to the non i.i.d nature of graph data. By adding or deleting a small number of edges in the graph, adversaries could greatly change the graph label predicted by a graph classification model. In this work, we propose to build a smoothed graph classification model with certified robustness guarantee. We have proven that the resulting graph classification model would output the same prediction for a graph under l(0) bounded adversarial perturbation. We also evaluate the effectiveness of our approach under graph convolutional network (GCN) based multi-class graph classification model."	"Br7qi
Times Cited:3
Cited References Count:32
IEEE Global Communications Conference"	""	"<Go to ISI>://WOS:000668970503035"	""	"Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Z. Chen; Y. Nadji; A. Kountouras; F. Monrose; R. Perdisci; M. Antonakakis; N. Vasiloglou"	"2017"	"Practical Attacks Against Graph-based Clustering"	""	"Ccs'17: Proceedings of the 2017 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"1125-1142"	""	""	""	""	""	""	""	"Practical Attacks Against Graph-based Clustering"	""	""	"10.1145/3133956.3134083"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000440307700071"	""	""	"adversarial machine learning
unsupervised learning
dga
network security
hierarchical organization"	"Graph modeling allows numerous security problems to be tackled in a general way, however, little work has been done to understand their ability to withstand adversarial attacks. We design and evaluate two novel graph attacks against a state-of-the-art network level, graph-based detection system. Our work highlights areas in adversarial machine learning that have not yet been addressed, specifically: graph-based clustering techniques, and a global feature space where realistic attackers without perfect knowledge must be accounted for (by the defenders) in order to be practical. Even though less informed attackers can evade graph clustering with low cost, we show that some practical defenses are possible."	"Bk6gz
Times Cited:24
Cited References Count:55"	""	"<Go to ISI>://WOS:000440307700071"	""	"Georgia Inst Technol, Coll Comp, Sch Comp Sci, Atlanta, GA 30332 USA
Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA
Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA
Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA
Symantec CAML Grp, Mountain View, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"W. Jin; Y. Ma; X. R. Liu; X. F. Tang; S. H. Wang; J. L. Tang"	"2020"	"Graph Structure Learning for Robust Graph Neural Networks"	""	"Kdd '20: Proceedings of the 26th Acm Sigkdd International Conference on Knowledge Discovery & Data Mining"	""	""	""	""	""	"66-74"	""	""	""	""	""	""	""	"Graph Structure Learning for Robust Graph Neural Networks"	""	""	"10.1145/3394486.3403049"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000749552300008"	""	""	""	"Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses(1)."	"Bs6ls
Times Cited:39
Cited References Count:41"	""	"<Go to ISI>://WOS:000749552300008"	""	"Michigan State Univ, E Lansing, MI 48824 USA
Penn State Univ, University Pk, PA 16802 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. M. Mu; B. H. Wang; Q. Li; K. Sun; M. W. Xu; Z. T. Liu"	"2021"	"A Hard Label Black-box Adversarial Attack Against Graph Neural Networks"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"108-125"	""	""	""	""	""	""	""	"A Hard Label Black-box Adversarial Attack Against Graph Neural Networks"	""	""	"10.1145/3460120.3484796"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478300010"	""	""	"black-box adversarial attack
structural perturbation
graph neural networks
graph classification"	"Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.
In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model. To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses."	"Bs7xr
Times Cited:0
Cited References Count:63"	""	"<Go to ISI>://WOS:000768478300010"	""	"Tsinghua Univ, Inst Network Sci & Cyberspace, Dept Comp Sci, Beijing, Peoples R China
Tsinghua Univ, BNRist, Beijing, Peoples R China
Illinois Inst Technol, Chicago, IL USA
George Mason Univ, Fairfax, VA 22030 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"U. S. Shanthamallu; J. J. Thiagarajan; A. Spanias"	"2021"	"Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks"	""	"Thirty-Fifth Aaai Conference on Artificial Intelligence, Thirty-Third Conference on Innovative Applications of Artificial Intelligence and the Eleventh Symposium on Educational Advances in Artificial Intelligence"	""	""	"35"	""	""	"9524-9532"	""	""	""	""	""	""	""	"Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks"	"Aaai Conf Artif Inte"	"2159-5399"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000681269801022"	""	""	""	"Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN."	"Bs0dr
Times Cited:1
Cited References Count:32
AAAI Conference on Artificial Intelligence"	""	"<Go to ISI>://WOS:000681269801022"	""	"Arizona State Univ, Tempe, AZ 85281 USA
Lawrence Livermore Natl Lab, Livermore, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. H. Wang; N. Z. Gong"	"2019"	"Attacking Graph-based Classification via Manipulating the Graph Structure"	""	"Proceedings of the 2019 Acm Sigsac Conference on Computer and Communications Security (Ccs'19)"	""	""	""	""	""	"2023-2040"	""	""	""	""	""	""	""	"Attacking Graph-based Classification via Manipulating the Graph Structure"	""	""	"10.1145/3319535.3354206"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000509760700122"	""	""	"adversarial machine learning
adversarial graph-based classification
adversarial graph neural network"	"Graph-based classification methods are widely used for security analytics. Roughly speaking, graph-based classification methods include collective classification and graph neural network. Attacking a graph-based classification method enables an attacker to evade detection in security analytics. However, existing adversarial machine learning studies mainly focused on machine learning for non-graph data. Only a few recent studies touched adversarial graph-based classification methods. However, they focused on graph neural network, leaving collective classification largely unexplored. We aim to bridge this gap in this work. We consider an attacker's goal is to evade detection via manipulating the graph structure. We formulate our attack as a graph-based optimization problem, solving which produces the edges that an attacker needs to manipulate to achieve its attack goal. However, it is computationally challenging to solve the optimization problem exactly. To address the challenge, we propose several approximation techniques to solve the optimization problem. We evaluate our attacks and compare them with a recent attack designed for graph neural networks using four graph datasets. Our results show that our attacks can effectively evade graph-based classification methods. Moreover, our attacks outperform the existing attack for evading collective classification methods and some graph neural network methods."	"Bo2zy
Times Cited:19
Cited References Count:77"	""	"<Go to ISI>://WOS:000509760700122"	""	"Duke Univ, ECE Dept, Durham, NC 27706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Ongun; A. Oprea; C. Nita-Rotaru; M. Christodorescu; N. Salajegheh"	"2018"	"The House That Knows You: User Authentication Based on IoT Data"	""	"Proceedings of the 2018 Acm Sigsac Conference on Computer and Communications Security (Ccs'18)"	""	""	""	""	""	"2255-2257"	""	""	""	""	""	""	""	"The House That Knows You: User Authentication Based on IoT Data"	""	""	"10.1145/3243734.3278523"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000461315900175"	""	""	"behavioral authentication
internet of things
machine learning"	"Home-based Internet of Things (IoT) devices have gained in popularity and many households became "smart" by using devices such as smart sensors, locks, and voice-based assistants. Given the limitations of existing authentication techniques, we explore new opportunities for user authentication in smart home environments. Specifically, we design a novel authentication method in IoT-enabled smart homes. We perform initial experiments and a user study leveraging network traffic collected from 8 IoT devices in our university lab. Preliminary results show that our LSTM model achieves a maximum accuracy of 75% in identifying users."	"Bm2qn
Times Cited:2
Cited References Count:7"	""	"<Go to ISI>://WOS:000461315900175"	""	"Northeastern Univ, Boston, MA 02115 USA
Visa Res, Palo Alto, CA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"E. Nowroozi; A. Dehghantanha; R. M. Parizi; K. K. R. Choo"	"2021"	"A survey of machine learning techniques in adversarial image forensics"	""	"Computers & Security"	""	""	"100"	""	""	"102092"	""	""	""	""	"Jan"	""	""	"A survey of machine learning techniques in adversarial image forensics"	"Comput Secur"	"0167-4048"	"ARTN 102092
10.1016/j.cose.2020.102092"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000602828800002"	""	""	"image forensics
adversarial machine learning
adversarial learning
adversarial setting
image manipulation detection
cyber security
feature-selection
manipulation
examples"	"Image forensic plays a crucial role in both criminal investigations (e.g., dissemination of fake images to spread racial hate or false narratives about specific ethnicity groups or political campaigns) and civil litigation (e.g., defamation). Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches (e.g., how to detect adversarial (image) examples), and there are associated real-world consequences (e.g., inadmissible evidence, or wrongful conviction). Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios. (C) 2020 Elsevier Ltd. All rights reserved."	"Pl0nc
Times Cited:6
Cited References Count:96"	""	"<Go to ISI>://WOS:000602828800002"	""	"Univ Siena, Dept Informat Engn & Math, Siena, Italy
Univ Guelph, Sch Comp Sci, Guelph, ON, Canada
Kennesaw State Univ, Coll Comp & Software Engn, Kennesaw, GA 30144 USA
Univ Texas San Antonio, Dept Informat Syst & Cyber Secur, San Antonio, TX 78249 USA
Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA
Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Li; Q. Liu; W. T. Zhao; D. X. Wang; S. Q. Wang"	"2018"	"Chronic Poisoning Against Machine Learning Based IDSs Using Edge Pattern Detection"	""	"2018 Ieee International Conference on Communications (Icc)"	""	""	""	""	""	"1-7"	""	""	""	""	""	""	""	"Chronic Poisoning Against Machine Learning Based IDSs Using Edge Pattern Detection"	"Ieee Icc"	"1550-3607"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000519271301100"	""	""	"chronic poisoning
intrusion detection system
machine learning
data drifting"	"In big data era, machine learning is one of fundamental techniques in intrusion detection systems (IDSs). Poisoning attack, which is one of the most recognized security threats towards machine learning-based IDSs, injects some adversarial samples into the training phase, inducing data drifting of training data and a significant performance decrease of target IDSs over testing data. In this paper, we adopt the Edge Pattern Detection (EPD) algorithm to design a novel poisoning method that attack against several machine learning algorithms used in IDSs. Specifically, we propose a boundary pattern detection algorithm to efficiently generate the points that are near to abnormal data but considered to be normal ones by current classifiers. Then, we introduce a Batch-EPD Boundary Pattern (BEBP) detection algorithm to overcome the limitation of the number of edge pattern points generated by EPD and to obtain more useful adversarial samples. Based on BEBP, we further present a moderate but effective poisoning method called chronic poisoning attack. Extensive experiments on synthetic and three real network data sets demonstrate the performance of the proposed poisoning method against several well-known machine learning algorithms and a practical intrusion detection method named FMIFS-LSSVM-IDS."	"Bo5zi
Times Cited:0
Cited References Count:20
IEEE International Conference on Communications"	""	"<Go to ISI>://WOS:000519271301100"	""	"Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Wu; X. W. Yang; S. R. Pan; X. L. Yuan"	"2021"	"Adapting Membership Inference Attacks to GNN for Graph Classification: Approaches and Implications"	""	"2021 21st Ieee International Conference on Data Mining (Icdm 2021)"	""	""	""	""	""	"1421-1426"	""	""	""	""	""	""	""	"Adapting Membership Inference Attacks to GNN for Graph Classification: Approaches and Implications"	"Ieee Data Mining"	"1550-4786"	"10.1109/Icdm51629.2021.00182"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000780454100173"	""	""	"membership inference attacks
graph classification
graph neural networks"	"In light of the wide application of Graph Neural Networks (GNNs), Membership Inference Attack (MIA) against GNNs raises severe privacy concerns, where training data can be leaked from trained GNN models. However, prior studies focus on inferring the membership of only the components in a graph, e.g., an individual node or edge. In this paper, we take the first step in MIA against GNNs for graph-level classification. Our objective is to infer whether a graph sample has been used for training a GNN model. We present and implement two types of attacks, i.e., training-based attacks and threshold-based attacks from different adversarial capabilities. We perform comprehensive experiments to evaluate our attacks in seven real-world datasets using five representative GNN models. Both our attacks are shown effective and can achieve high performance, i.e., reaching over 0.7 attack F1 scores in most cases(1). Our findings also confirm that, unlike the node-level classifier, MIAs on graph-level classification tasks are more co-related with the overfitting level of GNNs rather than the statistic property of their training graphs."	"Bs9gc
Times Cited:0
Cited References Count:23
IEEE International Conference on Data Mining"	""	"<Go to ISI>://WOS:000780454100173"	""	"Monash Univ, Melbourne, Vic, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Jang; H. J. Yang"	"2020"	"Deep Reinforcement Learning-Based Resource Allocation and Power Control in Small Cells With Limited Information Exchange"	""	"Ieee Transactions on Vehicular Technology"	""	""	"69"	""	"11"	"13768-13783"	""	""	""	""	"Nov"	""	""	"Deep Reinforcement Learning-Based Resource Allocation and Power Control in Small Cells With Limited Information Exchange"	"Ieee T Veh Technol"	"0018-9545"	"10.1109/Tvt.2020.3027013"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000589655300014"	""	""	"interference
information exchange
resource management
time-varying channels
array signal processing
signal to noise ratio
machine learning
orthogonal frequency division multiplexing (ofdm) downlink
sum-rate maximization
distributed resource allocation
reinforcement learning (rl)
limited information exchange
rate maximization
networks
autoencoder
game"	"In multi-user downlink small cell networks, cooperative resource allocation (RA) within a small cell cluster is a key technique to enhance network capacity. However, capacity-maximizing RA in frequency-selective fading channels requires global channel state information (CSI) of users within a small cell cluster, which makes it infeasible in practical networks with limited direct link capacity. To circumvent this global CSI assumption, most of the existing studies on RA have been based on several CSI assumptions such as local CSI and local CSI at the transmitters (CSIT). Nevertheless, cost functions with local CSI or local CSIT in the literature rely on heuristic formulations, because the sum-rate cannot be computed if without global CSI. In this paper, we propose a deep reinforcement learning-based RA algorithm to maximize the sum-rate for any given limited information on instantaneous CSI or sum-rate at the previous period. The proposed scheme is not restricted to certain CSI assumptions, but attempts to find the best RA for any given information such as quantized local CSI and quantized local CSIT; thus, it is applicable to any given direct link capacity. The proposed algorithm is self-adaptive in time-varying channels, since it is not divided into training and test phases. We modify the target neural network (TNN) scheme to enhance the sum-rate and the convergence speed. Numerical simulations confirm that: i) the proposed algorithm outperforms the conventional algorithms even under the same CSI assumption such as local CSI and local CSIT; ii) a flexible trade-off between the amount of CSI and the sum-rate is realizable in practical systems."	"Or7hl
Times Cited:5
Cited References Count:48"	""	"<Go to ISI>://WOS:000589655300014"	""	"Ulsan Natl Inst Sci & Technol UNIST, Dept Elect Engn, Ulsan 44919, South Korea
Pohang Univ Sci & Technol POSTECH, Dept Elect Engn, Pohang 37673, South Korea"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Truong; K. Sun; S. Y. Wang; F. Guitton; Y. K. Guo"	"2021"	"Privacy preservation in federated learning: An insightful survey from the GDPR perspective"	""	"Computers & Security"	""	""	"110"	""	""	"102402"	""	""	""	""	"Nov"	""	""	"Privacy preservation in federated learning: An insightful survey from the GDPR perspective"	"Comput Secur"	"0167-4048"	"ARTN 102402
10.1016/j.cose.2021.102402"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000703432300009"	""	""	"federated learning
data protection regulation
gdpr
personal data
privacy
privacy preservation
secure 2-party computation
differential privacy
gradient descent
challenges
attacks
ideal"	"In recent years, along with the blooming of Machine Leaming (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML -based service providers not only confront with difficulties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Fur-thermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning with-out disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not sufficient for privacy-guarantee because model parameters ex-changed among participants conceal sensitive information that can be exploited in pri-vacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing chal-lenges are examined along with the prospective approaches following the GDPR reg-ulatory guidelines that FL-based systems shall implement to fully comply with the GDPR. (c) 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ )"	"Wb2uj
Times Cited:9
Cited References Count:128"	""	"<Go to ISI>://WOS:000703432300009"	""	"Imperial Coll London, Data Sci Inst, South Kensington Campus, London SW7 2AZ, England
Hong Kong Baptist Univ, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Z. Alom; T. M. Taha; C. Yakopcic; S. Westberg; P. Sidike; M. S. Nasrin; M. Hasan; B. C. Van Essen; A. A. S. Awwal; V. K. Asari"	"2019"	"A State-of-the-Art Survey on Deep Learning Theory and Architectures"	""	"Electronics"	""	""	"8"	""	"3"	""	""	""	""	""	"Mar 5"	""	""	"A State-of-the-Art Survey on Deep Learning Theory and Architectures"	"Electronics-Switz"	""	"ARTN 292
10.3390/electronics8030292"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000464239600001"	""	""	"deep learning
convolutional neural network (cnn)
recurrent neural network (rnn)
auto-encoder (ae)
restricted boltzmann machine (rbm)
deep belief network (dbn)
generative adversarial network (gan)
deep reinforcement learning (drl)
transfer learning
neural-networks
image classification
representations
segmentation
challenges
algorithm
game
go"	"In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models."	"Ht0ey
Times Cited:394
Cited References Count:329"	""	"<Go to ISI>://WOS:000464239600001"	""	"Univ Dayton, Dept Elect & Comp Engn, Dayton, OH 45469 USA
St Louis Univ, Dept Earth & Atmospher Sci, St Louis, MO 63108 USA
Comcast Labs, Washington, DC 20005 USA
LLNL, Livermore, CA 94550 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Danu; C. I. Nita; A. Vizitiu; C. Suciu; L. M. Itu"	"2019"	"Deep learning based generation of synthetic blood vessel surfaces"	""	"2019 23rd International Conference on System Theory, Control and Computing (Icstcc)"	""	""	""	""	""	"662-667"	""	""	""	""	""	""	""	"Deep learning based generation of synthetic blood vessel surfaces"	"Int Conf Syst Theo"	"2372-1618"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000590181100112"	""	""	"generative models
deep convolutional neural networks
synthetic data generation
three-dimensional surfaces"	"In recent years, the medical imaging area showed a notably increased interest in Deep Learning (DL) based applications. Deep learning is a machine learning (ML) technique which learns features and tasks directly from data, trying to model human abstract thinking. Since deep learning can create features without a human intervention, it allows data scientists to use more complex sets of features in comparison with traditional machine learning approaches. In addition to this, the robustness to natural variations in the data is automatically learned and the deep learning architecture is flexible, so that the same neural network based approach can be applied to many different applications and data types. Our goal is to apply DL based techniques in the context of medical imaging with the purpose of developing a workflow for diagnosing cardiovascular pathologies or cerebral aneurysms. Since a major challenge of this approach is the lack of large training databases, in this paper we are focusing on performing data augmentation by generating realistic synthetic anatomical models of blood vessels. For this task we used geometries describing vessel-like structures and also real anatomies extracted from patients. We chose to experiment with two state of the art models: Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). We address the problem of employing neural network based models on three dimensional surfaces. Such surfaces typically have an unstructured representation consisting of points and polygons and are not compatible with typical neural network architectures. We propose a technique based on surface voxelization which consists on representing the unstructured surface mesh as a threedimensional image, therefore becoming inherently compatible with a standard convolutional neural network. We performed experiments on three datasets containing both two and three dimensional surfaces representing blood vessel-like structures. We show that state of the art, deep learning based generative models, are capable of generating voxelized three dimensional surfaces of high quality that are visually indistinguishable from the training samples."	"Bq4lr
Times Cited:0
Cited References Count:21
International Conference on System Theory Control and Computing"	""	"<Go to ISI>://WOS:000590181100112"	""	"Transilvania Univ Brasov, Automat & Informat Technol, 5 Mihai Viteazu, Brasov 500174, Romania
Siemens SRL, Corp Technol, 3A Eroilor, Brasov 500007, Romania"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Li; P. J. Xu; Q. Ruan; W. S. Xu"	"2021"	"Text Adversarial Examples Generation and Defense Based on Reinforcement Learning"	""	"Tehnicki Vjesnik-Technical Gazette"	""	""	"28"	""	"4"	"1306-1314"	""	""	""	""	"Jul"	""	""	"Text Adversarial Examples Generation and Defense Based on Reinforcement Learning"	"Teh Vjesn"	"1330-3651"	"10.17559/Tv-20200801053744"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000675891000031"	""	""	"adversarial examples
defense
neural networks
text classification"	"In recent years, the neural networks are widely used in image processing, natural language processing and other fields. But there are new security issues-the adversarial examples. Crafted adversarial examples can make a trouble for the neural network, which leads to the mis-classification. Text classification is one of the basic tasks of the natural language processing. This paper is concerned about the generation and defense of text adversarial examples. The main contributions of this research are as follows: This paper explores a new type of adversarial example and applies reinforcement learning to generate the adversarial examples; a training set composed of adversarial examples is constructed. To build a more robust classifier, a new defense framework is established. In order to eliminate the influence of noise, well-designed predetector and reformer were implemented, which helps the neural networks to resist adversarial examples and reduce coupling."	"Tm9ue
Times Cited:1
Cited References Count:23"	""	"<Go to ISI>://WOS:000675891000031"	""	"Donghua Univ, Coll Comp Sci & Technol, Shanghai 201600, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. J. Hathaliya; S. Tanwar; P. Sharma"	"2022"	"Adversarial learning techniques for security and privacy preservation: A comprehensive review"	""	"Security and Privacy"	""	""	"5"	""	"3"	""	""	""	""	""	"May"	""	""	"Adversarial learning techniques for security and privacy preservation: A comprehensive review"	"Secur Privacy"	"2475-6725"	"ARTN e209
10.1002/spy2.209"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000748947800001"	""	""	"adversarial attacks
adversarial defenses
adversarial learning
block chain
deep learning
federated learning
machine learning
privacy
reinforcement learning
security
distributed attack detection
health-care
anomaly detection
data aggregation
fog
internet
blockchain
things
intelligence
challenges"	"In recent years, the use of smart devices has increased exponentially, resulting in massive amounts of data. To handle this data, effective data storage and management has required. Cloud computing (CC) is a promising solution to deal with this huge amount of data. Electronic devices are collecting real-time data from sensors and applications through a wireless communication channel in the digital era. In some cases, CC cannot protect against various malicious attacks in the wireless communication channel. To address this issue, we have used machine learning (ML) and deep learning (DL) techniques for attack detection in a wireless channel on an early basis. It trains a model to predict malicious activities of attackers, which aids in the security of CC's sensitive data. We employed adversarial learning techniques (AL) to add fake data into the model to ensure that the trained model was correct. The trained model can distinguish between the fake and real data from the training samples and improve the training samples' performance. AL provides different defense mechanisms to preserve the privacy of ML- and DL-based model but does not ensure the system's robustness. To improve the system's robustness, we have used federated learning with blockchain technology to make a system more robust, reliable, accurate, and transparent. This integration aids in providing high-graded security against adversarial attacks. This paper presents a comprehensive review to highlight the recent improvements in AL techniques. Moreover, we explored the various AL applications in security and privacy preservation. Finally, open research issues and future directions are discussed to show future research avenues."	"0j1yl
Times Cited:0
Cited References Count:141"	""	"<Go to ISI>://WOS:000748947800001"	""	"Nirma Univ, Inst Technol, Dept Comp Sci & Engn, Ahmadabad 382481, Gujarat, India
Samyak Infotech, Ahmadabad, Gujarat, India
Orbit Pharma, Ahmadabad, Gujarat, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Khorshidpour; S. Hashemi; A. Hamzeh"	"2016"	"Learning a Secure Classifier against Evasion Attack"	""	"2016 Ieee 16th International Conference on Data Mining Workshops (Icdmw)"	""	""	""	""	""	"295-302"	""	""	""	""	""	""	""	"Learning a Secure Classifier against Evasion Attack"	"Int Conf Dat Min Wor"	"2375-9232"	"10.1109/Icdmw.2016.46"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000401906900041"	""	""	"machine learning
adversarial classification
adversary
evasion attack
robustness"	"In security sensitive applications, there is a crafty adversary component which intends to mislead the detection system. The presence of an adversary component conflicts with the stationary data assumption that is a common assumption in most machine learning methods. Since machine learning methods are not inherently adversary-aware, it necessitates to investigate security evaluation of machine learning based detection systems in the adversarial environment. Research in adversarial environment mostly focused on modeling adversarial attacks and evaluating impact of them on learning algorithms, only few studies have devised learning algorithms with improved security. In this paper we propose a secure learning model against evasion attacks on the application of PDF malware detection. The experimental results acknowledge that the proposed method significantly improves the robustness of the learning system against manipulating data and evasion attempts at test time."	"Bh6mb
Times Cited:3
Cited References Count:20
International Conference on Data Mining Workshops"	""	"<Go to ISI>://WOS:000401906900041"	""	"Shiraz Univ, Dept Elect & Comp Engn, Shiraz, Iran"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Yahya; M. Hassan; S. Younis; M. Shafique"	"2020"	"Probabilistic Analysis of Targeted Attacks Using Transform-Domain Adversarial Examples"	""	"Ieee Access"	""	""	"8"	""	""	"33855-33869"	""	""	""	""	""	""	""	"Probabilistic Analysis of Targeted Attacks Using Transform-Domain Adversarial Examples"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.2974525"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000567604800066"	""	""	"steganography
targeted attacks
dnn classifiers
perturbations
adversarial examples
imperceptibility
white-box-attacks
black-box-attacks"	"In the past decade, Deep Neural Networks (DNNs) have achieved breakthrough collaborations in developing smart intelligent systems within the field of computer vision, natural language processing, autonomous systems, etc. Recent research has revealed that stability of such smart systems is at greater risk when they come across to adversarial perturbations. Although, these perturbations may not be perceivable in nature when seen from naked eye, yet, they are capable enough to fool state-of-the-art DNN classifiers. Till now, much of the previous work related to fool such classifiers focuses on generating adversaries that directly change pixel values of an image in spatial-domain. In this paper, we propose a novel transform-domain imperceptible attack methodology "TDIAM'' to generate adversaries based on image steganography-approach using a "single carefully selected targeted watermark''. We use three different frequency-domain approaches, i.e., Discrete Wavelet Transform (DWT), Discrete Cosine Transform (DCT) and Fast Fourier Transform (FFT) to craft perturbations in selective frequency component which makes it robust and it requires less computational time as it is a non-gradient approach. We present our case study on MNIST handwritten digits dataset. Our results demonstrate that the generated perturbation vector successfully fool simple Convolutional Neural Network (CNN), LeNet-5 and AlexNet architectures by increasing probability of adversaria examples for the targeted class (to which the targeted watermark belongs) in both "black-box'' and "white-box'' adversarial attacks. The results have shown that among these three perturbation approaches, DWT based perturbation shown promising results by effectively fooling DNNs while ensuring the high imperceptibility as well."	"Nl7qg
Times Cited:4
Cited References Count:46"	""	"<Go to ISI>://WOS:000567604800066"	""	"Natl Univ Sci & Technol NUST, Sch Elect Engn & Comp Sci SEECS, Islamabad 44000, Pakistan
Vienna Univ Technol TU Wien, Inst Comp Engn, A-1040 Vienna, Austria"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Khorshidpour; J. Tahmoresnezhad; S. Hashemi; A. Hamzeh"	"2018"	"Domain invariant feature extraction against evasion attack"	""	"International Journal of Machine Learning and Cybernetics"	""	""	"9"	""	"12"	"2093-2104"	""	""	""	""	"Dec"	""	""	"Domain invariant feature extraction against evasion attack"	"Int J Mach Learn Cyb"	"1868-8071"	"10.1007/s13042-017-0692-6"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000450175600011"	""	""	"adversarial environment
evasion attack
domain shift
domain adaptation
spam filtering
algorithm
selection
security
svm"	"In the security application, an attacker might violate the data stationary assumption that is a common assumption in the most machine learning techniques. This problem named as the domain shift problem arises when training (source) and test (target) data follow different distributions. The inherent adversarial nature of the security applications considerably effects on the robustness of a learning system. For that, a classifier designer needs to evaluate the robustness of a learning system under potential attacks during the design phase. The previous studies investigate the effect of reduced feature vector on the security evaluation of a learning classifier. They demonstrate that traditional feature selection techniques lead to even worsen performance. Therefore, an adversary-aware feature selection algorithm is proposed to improve the robustness of the learning systems. However, prior studies in domain adaptation techniques which are fundamental in addressing domain shift problem demonstrate that original space may not be directly suitable for refining this distribution mismatch, because some features may have been distorted by the domain shift. In this paper, we propose domain invariant feature extraction model based on domain adaptation technique in order to address domain shift problem caused by an adversary. We conduct an experiment that graphically shows the effect of a successful attack on the MNIST handwritten digits classification task. After that, we design synthetic datasets to investigate the effect of reduced feature vector on the performance of a learning system under attack. Moreover, our proposed feature extraction model significantly outperforms the adversarial-aware feature selection and traditional feature selection models on the application of spam filtering"	"Ha3sn
Times Cited:1
Cited References Count:36"	""	"<Go to ISI>://WOS:000450175600011"	""	"Shiraz Univ, Sch Elect & Comp Engn, Dept Comp Sci Engn & Informat Technol, Shiraz, Iran"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. W. Li; L. F. Lai; S. G. Cui"	"2021"	"On the Adversarial Robustness of LASSO Based Feature Selection"	""	"Ieee Transactions on Signal Processing"	""	""	"69"	""	""	"5555-5567"	""	""	""	""	""	""	""	"On the Adversarial Robustness of LASSO Based Feature Selection"	"Ieee T Signal Proces"	"1053-587x"	"10.1109/Tsp.2021.3115943"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000709069900003"	""	""	"feature extraction
optimization
robustness
machine learning algorithms
design methodology
deep learning
sparse matrices
linear regression
feature selection
lasso
adversarial machine learning
bi-level optimization
regression
recovery"	"In this paper, we investigate the adversarial robustness of feature selection based on the l(1) regularized linear regression model, namely LASSO. In the considered model, there is a malicious adversary who can observe the whole dataset, and then will carefully modify the response values or the feature matrix in order to manipulate the selected features. We formulate the modification strategy of the adversary as a bi-level optimization problem. Due to the difficulty of the non-differentiability of the l(1) norm at the zero point, we reformulate the l(1) norm regularizer as linear inequality constraints. We employ the interior-point method to solve this reformulated LASSO problem and obtain the gradient information. Then we use the projected gradient descent method to design the modification strategy. In addition, we demonstrate that this method can be extended to other l(1) based feature selection methods, such as group LASSO and sparse group LASSO. Numerical examples with synthetic and real data illustrate that our method is efficient and effective."	"Wj5fg
Times Cited:0
Cited References Count:55"	""	"<Go to ISI>://WOS:000709069900003"	""	"Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA
Chinese Univ Hong Kong, Shenzhen Res Inst Big Data, Sch Sci & Engn, Shenzhen 518172, Peoples R China
Chinese Univ Hong Kong, Future Network Intelligence Inst, Shenzhen 518172, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. W. Li; L. F. Lai; S. G. Cui"	"2019"	"On the Adversarial Robustness of Subspace Learning"	""	"2019 Ieee International Conference on Acoustics, Speech and Signal Processing (Icassp)"	""	""	""	""	""	"2477-2481"	""	""	""	""	""	""	""	"On the Adversarial Robustness of Subspace Learning"	"Int Conf Acoust Spee"	"1520-6149"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000482554002141"	""	""	"principal component analysis
subspace learning
adversarial robustness"	"In this paper, we investigate the adversarial robustness of subspace learning problems. Different from the scenario addressed by classic robust algorithms that assume fractions of data are corrupted, we consider a more powerful adversary who can observe the whole data and modify all of them. The goal of the adversary is to maximize the distance between the subspace learned from the original data set and that learned from the modified data. We characterize the optimal rank-one attack strategy and show that the optimal strategy depends on the smallest singular value of the original data matrix and the adversary's energy budget."	"Bn4np
Times Cited:2
Cited References Count:16
International Conference on Acoustics Speech and Signal Processing ICASSP"	""	"<Go to ISI>://WOS:000482554002141"	""	"Univ Calif Davis, Dept ECE, Davis, CA 95616 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Xing; S. Huang; L. W. Huangfu; F. Y. Chen; Y. X. Ge"	"2020"	"Robust Bidirectional Generative Network for Generalized Zero-Shot Learning"	""	"2020 Ieee International Conference on Multimedia and Expo (Icme)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Robust Bidirectional Generative Network for Generalized Zero-Shot Learning"	"Ieee Int Con Multi"	"1945-7871"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000612843900227"	""	""	"generalized zero-shot learning
generative adversarial network
adversarial attack
object recognition
robustness analysis"	"In this work, we propose a novel generative approach named Robust Bidirectional Generative Network (RBGN) based on Conditional Generative Adversarial Network (CGAN) for Generalized Zero-shot Learning (GZSL). RBGN employs the adversarial attack to train a more rigorous discriminator, thus enhancing the generalizability and robustness of the feature generator under minimax strategy. Moreover, RBGN decodes the generated visual features back to their semantic representations to further improve the representational ability of generated visual features and alleviate the hubness problem. The experimental results of GZSL on four datasets, i.e. CUB, SUN, AWA1, AWA2, demonstrate that our model achieves competitive performance compared to state-of-the-art approaches and owns better generalizability to the unseen classes over conventional generative GZSL models. Further robustness analysis also validates the strong robustness of our model to the different types of semantic disturbance."	"Bq6of
Times Cited:4
Cited References Count:32
IEEE International Conference on Multimedia and Expo"	""	"<Go to ISI>://WOS:000612843900227"	""	"Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Sch Big Data & Software Engn, Minist Educ, Chongqing 400044, Peoples R China
San Diego State Univ, Fowler Coll Business, San Diego, CA 92182 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Bai; J. Zhao; J. L. Zhu; S. D. Han; J. F. Chen; B. Li; A. Kot"	"2022"	"Toward Efficiently Evaluating the Robustness of Deep Neural Networks in IoT Systems: A GAN-Based Method"	""	"Ieee Internet of Things Journal"	""	""	"9"	""	"3"	"1875-1884"	""	""	""	""	"Feb 1"	""	""	"Toward Efficiently Evaluating the Robustness of Deep Neural Networks in IoT Systems: A GAN-Based Method"	"Ieee Internet Things"	"2327-4662"	"10.1109/Jiot.2021.3091683"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000747462100023"	""	""	"perturbation methods
generative adversarial networks
generators
neural networks
internet of things
training
optimization
adversarial examples
deep learning
generative adversarial networks (gans)
internet
security
things"	"Intelligent Internet of Things (IoT) systems based on deep neural networks (DNNs) have been widely deployed in the real world. However, DNNs are found to be vulnerable to adversarial examples, which raises people's concerns about intelligent IoT systems' reliability and security. Testing and evaluating the robustness of IoT systems become necessary and essential. Recently, various attacks and strategies have been proposed, but the efficiency problem remains unsolved properly. Existing methods are either computationally extensive or time consuming, which is not applicable in practice. In this article, we propose a novel framework, called attack-inspired generative adversarial networks (AI-GAN) to generate adversarial examples conditionally. Once trained, it can generate adversarial perturbations efficiently given input images and target classes. We apply AI-GAN on different data sets in white-box settings, black-box settings, and targeted models protected by state-of-the-art defenses. Through extensive experiments, AI-GAN achieves high attack success rates, outperforming existing methods, and reduces generation time significantly. Moreover, for the first time, AI-GAN successfully scales to complex data sets, e.g., CIFAR-100 and ImageNet, with about 90% success rates among all classes."	"Yn7vn
Times Cited:0
Cited References Count:53"	""	"<Go to ISI>://WOS:000747462100023"	""	"Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore
Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore
Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China
Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China
Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA
Univ Illinois, Comp Sci Dept, Urbana, IL 61801 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. C. Shen; X. L. Zhu; D. Ma"	"2019"	"TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications"	""	"Ieee Access"	""	""	"7"	""	""	"41498-41506"	""	""	""	""	""	""	""	"TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications"	"Ieee Access"	"2169-3536"	"10.1109/Access.2019.2905915"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000464407500001"	""	""	"deep neural networks
poisoning attack
privacy
adversarial attack"	"Internet application providers now have more incentive than ever to collect user data, which greatly increases the risk of user privacy violations due to the emerging of deep neural networks. In this paper, we propose TensorClog-a poisoning attack technique that is designed for privacy protection against deep neural networks. TensorClog has three properties with each of them serving a privacy protection purpose: 1) training on TensorClog poisoned data results in lower inference accuracy, reducing the incentive of abusive data collection; 2) training on TensorClog poisoned data converges to a larger loss, which prevents the neural network from learning the privacy; and 3) TensorClog regularizes the perturbation to remain a high structure similarity, so that the poisoning does not affect the actual content in the data. Applying our TensorClog poisoning technique to CIFAR-10 dataset results in an increase in both converged training loss and test error by 300% and 272%, respectively. It manages to maintain data's human perception with a high SSIM index of 0.9905. More experiments including different limited information attack scenarios and a real-world application transferred from pre-trained ImageNet models are presented to further evaluate TensorClog's effectiveness in more complex situations."	"Ht2qm
Times Cited:7
Cited References Count:23"	""	"<Go to ISI>://WOS:000464407500001"	""	"Zhejiang Univ, Coll Elect Engn, Hangzhou 310007, Zhejiang, Peoples R China
Zhejiang Univ, Sch Microelect, Hangzhou 310007, Zhejiang, Peoples R China
Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310007, Zhejiang, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Angiulli; L. Argento; A. Furfaro"	"2018"	"Effectiveness of content spatial distribution analysis in securing IoT environments"	""	"2018 Ieee 6th International Conference on Future Internet of Things and Cloud (Ficloud 2018)"	""	""	""	""	""	"41-46"	""	""	""	""	""	""	""	"Effectiveness of content spatial distribution analysis in securing IoT environments"	""	""	"10.1109/FiCloud.2018.00014"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000482232200006"	""	""	"internet of things
intrusion detection systems
cyber threats
performance evaluation
intrusion detection
internet
things
model"	"Internet of things (IoT) is considered one of the most important technologies, due to its impact on both personal and enterprise domains. However, similarly to the traditional Internet, IoT is facing many security issues, part of which caused by the presence of an astounding number of vulnerable devices. Researchers have been struggling to improve the security of IoT in many different ways, one of which concerning the employment of intrusion detection systems (IDS) in IoT environments. This paper evaluates the effectiveness of an intrusion detection technique, named PCkAD, in IoT environments. The technique exploits the spatial distribution information of payload data to identify malicious contents. PCkAD was evaluated on two datasets, containing traffic related to sensors and actuators. The experimental results show that PCkAD can be effectively employed to increase the security of IoT environments."	"Bn4ly
Times Cited:1
Cited References Count:34"	""	"<Go to ISI>://WOS:000482232200006"	""	"Univ Calabria, DIMES, P Bucci 41C, I-87036 Arcavacata Di Rende, CS, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Pawlicki; M. Choras; R. Kozik"	"2020"	"Defending network intrusion detection systems against adversarial evasion attacks"	""	"Future Generation Computer Systems-the International Journal of Escience"	""	""	"110"	""	""	"148-154"	""	""	""	""	"Sep"	""	""	"Defending network intrusion detection systems against adversarial evasion attacks"	"Future Gener Comp Sy"	"0167-739x"	"10.1016/j.future.2020.04.013"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000541153400013"	""	""	""	"Intrusion Detection and the ability to detect attacks is a crucial aspect to ensure cybersecurity. However, what if an IDS (Intrusion Detection System) itself is attacked; in other words what defends the defender? In this work, the focus is on countering attacks on machine learning-based cyberattack detectors. In principle, we propose the adversarial machine learning detection solution. Indeed, contemporary machine learning algorithms have not been designed bearing in mind the adversarial nature of the environments they are deployed in. Thus, Machine Learning solutions are currently the target of a range of attacks. This paper evaluates the possibility of deteriorating the performance of a well-optimised intrusion detection algorithm at test time by crafting adversarial attacks with the four of the recently proposed methods and then offers a way to detect those attacks. The relevant background is provided for both artificial neural networks and four ways of crafting adversarial attacks. The new detection method is explained in detail, and the results of five different classifiers are compared. To the best of our knowledge, detecting adversarial attacks on artificial neural networks has not yet been widely researched in the context of intrusion detection systems. (C) 2020 Elsevier B.V. All rights reserved."	"Lz3uk
Times Cited:27
Cited References Count:45"	""	"<Go to ISI>://WOS:000541153400013"	""	"ITTI Sp Zoo, Poznan, Poland
UTP Univ Sci & Technol, Bydgoszcz, Poland"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. A. Ayub; W. A. Johnson; D. A. Talbert; A. Siraj"	"2020"	"Model Evasion Attack on Intrusion Detection Systems using Adversarial Machine Learning"	""	"2020 54th Annual Conference on Information Sciences and Systems (Ciss)"	""	""	""	""	""	"324-329"	""	""	""	""	""	""	""	"Model Evasion Attack on Intrusion Detection Systems using Adversarial Machine Learning"	""	""	"10.1109/Ciss48834.2020.1570617116"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000712170800057"	""	""	"adversarial machine learning
evasion attack
intrusion detection system
neural network"	"Intrusion Detection Systems (IDS) have a long history as an effective network defensive mechanism. The systems alert defenders of suspicious and / or malicious behavior detected on the network. With technological advances in AI over the past decade, machine learning (ML) has been assisting IDS to improve accuracy, perform better analysis, and discover variations of existing or new attacks. However, applications of ML algorithms have some reported weaknesses and in this research, we demonstrate how one of such weaknesses can be exploited against the workings of the IDS. The work presented in this paper is twofold: (1) we develop a ML approach for intrusion detection using Multilayer Perceptron (MLP) network and demonstrate the effectiveness of our model with two different network-based IDS datasets; and (2) we perform a model evasion attack against the built MLP network for IDS using an adversarial machine learning technique known as the Jacobian-based Saliency Map Attack (JSMA) method. Our experimental results show that the model evasion attack is capable of significantly reducing the accuracy of the IDS, i.e., detecting malicious traffic as benign. Our findings support that neural network-based IDS is susceptible to model evasion attack, and attackers can essentially use this technique to evade intrusion detection systems effectively."	"Bs3kn
Times Cited:10
Cited References Count:46"	""	"<Go to ISI>://WOS:000712170800057"	""	"Tennessee Technol Univ, Dept Comp Sci, Cookeville, TN 38505 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Marchal; M. Miettinen; T. D. Nguyen; A. R. Sadeghi; N. Asokan"	"2019"	"AuDI: Toward Autonomous IoT Device-Type Identification Using Periodic Communication"	""	"Ieee Journal on Selected Areas in Communications"	""	""	"37"	""	"6"	"1402-1412"	""	""	""	""	"Jun"	""	""	"AuDI: Toward Autonomous IoT Device-Type Identification Using Periodic Communication"	"Ieee J Sel Area Comm"	"0733-8716"	"10.1109/Jsac.2019.2904364"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000468234400017"	""	""	"internet of things
device-type identification
autonomous iot device identification
self-learning
physical device"	"IoT devices are being widely deployed. But the huge variance among them in the level of security and requirements for network resources makes it unfeasible to manage IoT networks using a common generic policy. One solution to this challenge is to define policies for classes of devices based on device type. In this paper, we present AuDI, a system for quickly and effectively identifying the type of a device in an IoT network by analyzing their network communications. AuDI models the periodic communication traffic of IoT devices using an unsupervised learning method to perform identification. In contrast to prior work, AuDI operates autonomously after initial setup, learning, without human intervention nor labeled data, to identify previously unseen device types. AuDI can identify the type of a device in any mode of operation or stage of lifecycle of the device. Via systematic experiments using 33 off-the-shelf IoT devices, we show that AuDI is effective (98.2% accuracy)."	"Hy6iq
Times Cited:32
Cited References Count:35"	""	"<Go to ISI>://WOS:000468234400017"	""	"Aalto Univ, Dept Comp Sci, Helsinki 00076, Finland
Tech Univ Darmstadt, Dept Comp Sci, D-64293 Darmstadt, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. D. Nguyen; S. Marchal; M. Miettinen; H. Fereidooni; N. Asokan; A. R. Sadeghi"	"2019"	"DIoT: A Federated Self-learning Anomaly Detection System for IoT"	""	"2019 39th Ieee International Conference on Distributed Computing Systems (Icdcs 2019)"	""	""	""	""	""	"756-767"	""	""	""	""	""	""	""	"DIoT: A Federated Self-learning Anomaly Detection System for IoT"	"Int Con Distr Comp S"	"1063-6927"	"10.1109/Icdcs.2019.00080"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000565234200071"	""	""	"internet of things
iot security
iot malware
anomaly detection
federated deep learning
self-learning"	"IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present DIoT, an autonomous self-learning distributed system for detecting compromised IoT devices. DIoT builds effectively on device-type-specific communication profiles without human intervention nor labeled data that are subsequently used to detect anomalous deviations in devices' communication behavior, potentially caused by malicious adversaries. DIoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to.. anomaly-detection-based intrusion detection. Consequently, DIoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that DIoT is highly effective (95.6% detection rate) and fast (approximate to 257ms) at detecting devices compromised by, for instance, the infamous Mirai malware. DIoT reported no false alarms when evaluated in a real-world smart home deployment setting."	"Bp8ga
Times Cited:106
Cited References Count:44
IEEE International Conference on Distributed Computing Systems"	""	"<Go to ISI>://WOS:000565234200071"	""	"Tech Univ Darmstadt, Darmstadt, Germany
Aalto Univ, Aalto, Finland"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Xu; Z. M. Gao; X. X. Fan; L. Chen; H. Kim; T. Suh; W. D. Shi"	"2020"	"Blockchain based End-to-end Tracking System for Distributed IoT Intelligence Application Security Enhancement"	""	"2020 Ieee 19th International Conference on Trust, Security and Privacy in Computing and Communications (Trustcom 2020)"	""	""	""	""	""	"1029-1036"	""	""	""	""	""	""	""	"Blockchain based End-to-end Tracking System for Distributed IoT Intelligence Application Security Enhancement"	"Ieee Int Conf Trust"	"2324-898x"	"10.1109/TrustCom50675.2020.00137"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000671077600123"	""	""	"blockchain
iot
dnn
security
framework
internet"	"IoT devices provide a rich data source that is not available in the past, which is valuable for a wide range of intelligence applications, especially deep neural network (DNN) applications that are data-thirsty. An established DNN model provides useful analysis results that can improve the operation of IoT systems in turn. The progress in distributed/federated DNN training further unleashes the potential of integration of IoT and intelligence applications. When a large number of IoT devices are deployed in different physical locations, distributed training allows training modules to be deployed to multiple edge data centers that are close to the IoT devices to reduce the latency and movement of large amounts of data. In practice, these IoT devices and edge data centers are usually owned and managed by different parties, who do not fully trust each other or have conflicting interests. It is hard to coordinate them to provide end-to-end integrity protection of the DNN construction and application with classical security enhancement tools. For example, one party may share an incomplete data set with others, or contribute a modified sub DNN model to manipulate the aggregated model and affect the decision-making process. To mitigate this risk, we propose a novel blockchain based end-to-end integrity protection scheme for DNN applications integrated with an IoT system in the edge computing environment. The protection system leverages a set of cryptography primitives to build a blockchain adapted for edge computing that is scalable to handle a large number of IoT devices. The customized blockchain is integrated with a distributed/federated DNN to offer integrity and authenticity protection services."	"Br8du
Times Cited:1
Cited References Count:22
IEEE International Conference on Trust Security and Privacy in Computing and Communications"	""	"<Go to ISI>://WOS:000671077600123"	""	"Univ Texas Rio Grande Valley, Comp Sci Dept, Edinburg, TX 78539 USA
Auburn Univ, Comp Sci Dept, Montgomery, AL USA
IoTeX, San Francisco, CA USA
Texas Tech Univ, Comp Sci Dept, Lubbock, TX USA
Korea Univ, Comp Sci Dept, Seoul, South Korea
Univ Houston, Comp Sci Dept, Houston, TX USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Ali; Y. F. Hu; D. K. Luong; G. Oguntala; J. P. Li; K. Abdo"	"2020"	"Adversarial Attacks on AI based Intrusion Detection System for Heterogeneous Wireless Communications Networks"	""	"2020 Aiaa/Ieee 39th Digital Avionics Systems Conference (Dasc) Proceedings"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Adversarial Attacks on AI based Intrusion Detection System for Heterogeneous Wireless Communications Networks"	"Ieeeaaia Digit Avion"	"2155-7195"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000646035600082"	""	""	"ai security
deep neural network
sdn
intrusion detection
heterogeneous wireless access technologies"	"It has been recognized that artificial intelligence (AI) will play an important role in future societies. AI has already been incorporated in many industries to improve business processes and automation. Although the aviation industry has successfully implemented flight management systems or autopilot to automate flight operations, it is expected that full embracement of AI remains a challenge. Given the rigorous validation process and the requirements for the highest level of safety standards and risk management, AI needs to prove itself being safe to operate.
This paper addresses the safety issues of AI deployment in an aviation network compatible with the Future Communication Infrastructure that utilizes heterogeneous wireless access technologies for communications between the aircraft and the ground networks. It further considers the exploitation of software defined networking (SDN) technologies in the ground network while the adoption of SDN in the airborne network can be optional. Due to the nature of centralized management in SDN-based network, the SDN controller can become a single point of failure or a target for cyber attacks. To countermeasure such attacks, an intrusion detection system utilises AI techniques, more specifically deep neural network (DNN), is considered. However, an adversary can target the AI-based intrusion detection system. This paper examines the impact of AI security attacks on the performance of the DNN algorithm. Poisoning attacks targeting the DSL-KDD datasets which were used to train the DNN algorithm were launched at the intrusion detection system. Results showed that the performance of the DNN algorithm has been significantly degraded in terms of the mean square error, accuracy rate, precision rate and the recall rate."	"Br3fo
Times Cited:0
Cited References Count:11
IEEE-AIAA Digital Avionics Systems Conference"	""	"<Go to ISI>://WOS:000646035600082"	""	"Univ Bradford, Fac Engn & Informat, Bradford, W Yorkshire, England
Altys Technol, Tolous, France"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Narayanan; M. Chandramohan; L. H. Chen; Y. Liu"	"2017"	"Context-Aware, Adaptive, and Scalable Android Malware Detection Through Online Learning"	""	"Ieee Transactions on Emerging Topics in Computational Intelligence"	""	""	"1"	""	"3"	"157-175"	""	""	""	""	"Jun"	""	""	"Context-Aware, Adaptive, and Scalable Android Malware Detection Through Online Learning"	"Ieee Tetci"	"2471-285x"	"10.1109/Tetci.2017.2699220"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000679683900003"	""	""	"concept drift
graph kernels
malware detection
online learning"	"It is well known that Android malware constantly evolves so as to evade detection. This causes the entire malware population to be nonstationary. Contrary to this fact, most of the prior works on machine learning based android malware detection have assumed that the distribution of the observed malware characteristics (i.e., features) does not change over time. In this paper, we address the problem of malware population drift and propose a novel online learning based framework to detect malware, named CASANDRA (Context-aware, Adaptive and Scalable ANDRoid mAlwa re detector). In order to perform accurate detection, a novel graph kernel that facilitates capturing apps security-sensitive behaviors along with their context information from dependence graphs is proposed. Besides being accurate and scalable, CASANDRA has specific advantages: first, being adaptive to the evolution in malware features over time; second, explaining the significant features that led to an apps classification as being malicious or benign. In a large-scale comparative analysis, CASANDRA outperforms two state-of-the-art techniques on a benchmark dataset achieving 99.23% F-measure. When evaluated with more than 87 000 apps collected in-the-wild, CASANDRA achieves 89.92% accuracy, outperforming existing techniques by more than 25% in their typical batch learning setting and more than 7% when they are continuously retained, while maintaining comparable efficiency."	"Vk3lv
Times Cited:24
Cited References Count:69"	""	"<Go to ISI>://WOS:000679683900003"	""	"Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore
Nanyang Technol Univ, Singapore 639798, Singapore
Nanyang Technol Univ, Sch Elect & Elect Engn, Ctr Infocomm Technol, Singapore 639798, Singapore"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Deldjoo; T. Di Noia; F. A. Merra"	"2021"	"A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"	""	"Acm Computing Surveys"	""	""	"54"	""	"2"	"1 - 38"	""	""	""	""	"Apr"	""	""	"A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"	"Acm Comput Surv"	"0360-0300"	"Artn 35
10.1145/3439729"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000644444900010"	""	""	"recommender systems
adversarial machine learning
security
adversarial perturbation
robustness
privacy
generative adversarial network
min-max game
matrix factorization
privacy
robustness
attacks"	"Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. However, success has been accompanied with a major new arising challenge: Many applications of machine learning (ML) are adversarial in nature [146]. In recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs.
The goal of this survey is two-fold: (i) to present recent advances on adversarial machine learning (AML) for the security of RS (i.e., attacking and defense recommendation models) and (ii) to show another successful application of AML in generative adversarial networks (GANs) for generative applications, thanks to their ability for learning (high-dimensional) data distributions. In this survey, we provide an exhaustive literature review of 76 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community working on the security of RS or on generative models using GANs to improve their quality."	"Rt4qd
Times Cited:20
Cited References Count:182"	""	"<Go to ISI>://WOS:000644444900010"	""	"Polytech Univ Bari, Via Orabona 4, I-70125 Bari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"K. Sadeghi; A. Banerjee; S. K. S. Gupta"	"2020"	"A System-Driven Taxonomy of Attacks and Defenses in Adversarial Machine Learning"	""	"IEEE Trans Emerg Top Comput Intell"	""	""	"4"	""	"4"	"450-467"	""	""	""	"2021/03/23"	"Aug"	""	""	"A System-Driven Taxonomy of Attacks and Defenses in Adversarial Machine Learning"	""	"2471-285X (Electronic)
2471-285X (Linking)"	"10.1109/tetci.2020.2968933"	""	""	""	""	"PMC7971418"	""	""	""	""	""	""	"33748635"	""	""	"Computational intelligence
adversarial machine learning
attack model
defense model
supervised learning"	"Machine Learning (ML) algorithms, specifically supervised learning, are widely used in modern real-world applications, which utilize Computational Intelligence (CI) as their core technology, such as autonomous vehicles, assistive robots, and biometric systems. Attacks that cause misclassifications or mispredictions can lead to erroneous decisions resulting in unreliable operations. Designing robust ML with the ability to provide reliable results in the presence of such attacks has become a top priority in the field of adversarial machine learning. An essential characteristic for rapid development of robust ML is an arms race between attack and defense strategists. However, an important prerequisite for the arms race is access to a well-defined system model so that experiments can be repeated by independent researchers. This paper proposes a fine-grained system-driven taxonomy to specify ML applications and adversarial system models in an unambiguous manner such that independent researchers can replicate experiments and escalate the arms race to develop more evolved and robust ML applications. The paper provides taxonomies for: 1) the dataset, 2) the ML architecture, 3) the adversary's knowledge, capability, and goal, 4) adversary's strategy, and 5) the defense response. In addition, the relationships among these models and taxonomies are analyzed by proposing an adversarial machine learning cycle. The provided models and taxonomies are merged to form a comprehensive system-driven taxonomy, which represents the arms race between the ML applications and adversaries in recent years. The taxonomies encode best practices in the field and help evaluate and compare the contributions of research works and reveals gaps in the field."	"Sadeghi, Koosha
Banerjee, Ayan
Gupta, Sandeep K S
eng
R21 EB019202/EB/NIBIB NIH HHS/
IEEE Trans Emerg Top Comput Intell. 2020 Aug;4(4):450-467. doi: 10.1109/tetci.2020.2968933. Epub 2020 May 25."	""	"https://www.ncbi.nlm.nih.gov/pubmed/33748635"	"internal-pdf://1883542391/Sadeghi-2020-A System-Driven Taxonomy of Attac.pdf"	"IMPACT lab (http://impact.asu.edu/), CIDSE, Arizona State University, Tempe, Arizona, USA, 85281."	""	""	""	""	""	""	""	""
"Journal Article"	"B. Qian; J. Su; Z. Y. Wen; D. N. Jha; Y. H. Li; Y. Guan; D. Puthal; P. James; R. Y. Yang; A. Y. Zomaya; O. Rana; L. Z. Wang; M. Koutny; R. Ranjan"	"2020"	"Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey"	""	"Acm Computing Surveys"	""	""	"53"	""	"4"	"1 - 47"	""	""	""	""	"Sep"	""	""	"Orchestrating the Development Lifecycle of Machine Learning-based IoT Applications: A Taxonomy and Survey"	"Acm Comput Surv"	"0360-0300"	"Artn 82
10.1145/3398020"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000575832400015"	""	""	"iot
machine learning
deep learning
orchestration
neural-networks
data fusion
optimization methods
resource-allocation
feature-extraction
fault-tolerance
demand response
internet
performance
recognition"	"Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages."	"Nx6qd
Times Cited:13
Cited References Count:325"	""	"<Go to ISI>://WOS:000575832400015"	""	"Newcastle Univ, Sch Comp, Newcastle Upon Tyne, Tyne & Wear, England
Univ Leeds, Sch Comp, Leeds, W Yorkshire, England
Univ Sydney, Sch Comp, Sydney, NSW, Australia
Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales
China Univ Geosci, Sch Comp Sci, Wuhan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Talpur; M. Gurusamy"	"2022"	"Machine Learning for Security in Vehicular Networks: A Comprehensive Survey"	""	"Ieee Communications Surveys and Tutorials"	""	""	"24"	""	"1"	"346-379"	""	""	""	""	""	""	""	"Machine Learning for Security in Vehicular Networks: A Comprehensive Survey"	"Ieee Commun Surv Tut"	""	"10.1109/Comst.2021.3129079"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000763336500013"	""	""	"security
wireless communication
vehicular ad hoc networks
computer architecture
privacy
cloud computing
5g mobile communication
vehicular networks
machine learning
security
privacy
trust
artificial neural-networks
ddos attack detection
intrusion detection
location privacy
wireless networks
communication-systems
driver identification
vehicle detection
next-generation
internet"	"Machine Learning (ML) has emerged as an attractive and viable technique to provide effective solutions for a wide range of application domains. An important application domain is vehicular networks wherein ML-based approaches are found to be very useful to address various problems. The use of wireless communication between vehicular nodes and/or infrastructure makes it vulnerable to different types of attacks. In this regard, ML and its variants are gaining popularity to detect attacks and deal with different kinds of security issues in vehicular communication. In this paper, we present a comprehensive survey of ML-based techniques for different security issues in vehicular networks. We first briefly introduce the basics of vehicular networks and different types of communications. Apart from the traditional vehicular networks, we also consider modern vehicular network architectures. We propose a taxonomy of security attacks in vehicular networks and discuss various security challenges and requirements. We classify the ML techniques developed in the literature according to their use in vehicular network applications. We explain the solution approaches and working principles of these ML techniques in addressing various security challenges and provide insightful discussion. The limitations and challenges in using ML-based methods in vehicular networks are discussed. Finally, we present observations and lessons learned before we conclude our work."	"Zk9yc
Times Cited:3
Cited References Count:220"	""	"<Go to ISI>://WOS:000763336500013"	""	"Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. G. Liu; J. Zhang; Y. Wang; W. L. Zhou; Y. Xiang; O. De Vel"	"2018"	"A Data-driven Attack against Support Vectors of SVM"	""	"Proceedings of the 2018 Acm Asia Conference on Computer and Communications Security (Asiaccs'18)"	""	""	""	""	""	"723-734"	""	""	""	""	""	""	""	"A Data-driven Attack against Support Vectors of SVM"	""	""	"10.1145/3196494.3196539"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000516620100058"	""	""	"adversarial learning
evasion attacks
support vector machines
machine
security"	"Machine learning (ML) is commonly used in multiple disciplines and real-world applications, such as information retrieval, financial systems, health, biometrics and online social networks. However, their security profiles against deliberate attacks have not often been considered. Sophisticated adversaries can exploit specific vulnerabilities exposed by classical ML algorithms to deceive intelligent systems. It is emerging to perform a thorough security evaluation as well as potential attacks against the machine learning techniques before developing novel methods to guarantee that machine learning can be securely applied in adversarial setting. In this paper, an effective attack strategy for crafting foreign support vectors in order to attack a classic ML algorithm, the Support Vector Machine (SVM) has been proposed with mathematical proof. The new attack can minimize the margin around the decision boundary and maximize the hinge loss simultaneously. We evaluate the new attack in different real-world applications including social spam detection, Internet traffic classification and image recognition. Experimental results highlight that the security of classifiers can be worsened by poisoning a small group of support vectors."	"Bo5ba
Times Cited:3
Cited References Count:50"	""	"<Go to ISI>://WOS:000516620100058"	""	"Swinburne Univ Technol, Hawthorn, Vic 3122, Australia
Guangzhou Univ, Guangzhou 510006, Peoples R China
Deakin Univ, Burwood, Vic 3125, Australia
Def Sci & Technol Grp, Edinburgh, SA 5111, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. M. Wang; J. Li; X. H. Kuang; Y. A. Tan; J. Li"	"2019"	"The security of machine learning in an adversarial setting: A survey"	""	"Journal of Parallel and Distributed Computing"	""	""	"130"	""	""	"12-23"	""	""	""	""	"Aug"	""	""	"The security of machine learning in an adversarial setting: A survey"	"J Parallel Distr Com"	"0743-7315"	"10.1016/j.jpdc.2019.03.003"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000470804100002"	""	""	"machine learning
adversarial setting
adversarial attack
adversarial example
security model
covert channel"	"Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models. (C) 2019 Elsevier Inc. All rights reserved."	"Ic2qf
Times Cited:49
Cited References Count:106"	""	"<Go to ISI>://WOS:000470804100002"	""	"Guangzhou Univ, Sch Comp Sci, Guangzhou 510006, Guangdong, Peoples R China
Natl Key Lab Sci & Technol Informat Syst Secur, Beijing, Peoples R China
Beijing Inst Technol Univ, Sch Comp Sci, Beijing, Peoples R China
Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Xiao; B. Biggio; B. Nelson; H. Xiao; C. Eckert; F. Roli"	"2015"	"Support vector machines under adversarial label contamination"	""	"Neurocomputing"	""	""	"160"	""	""	"53-62"	""	""	""	""	"Jul 21"	""	""	"Support vector machines under adversarial label contamination"	"Neurocomputing"	"0925-2312"	"10.1016/j.neucom.2014.08.081"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000354139100005"	""	""	"support vector machines
adversarial learning
label noise
label flip attacks
robustness
security"	"Machine learning algorithms are increasingly being applied in security-related tasks such as spam and malware detection, although their security properties against deliberate attacks have not yet been widely understood. Intelligent and adaptive attackers may indeed exploit specific vulnerabilities exposed by machine learning techniques to violate system security. Being robust to adversarial data manipulation is thus an important, additional requirement for machine learning algorithms to successfully operate in adversarial settings. In this work, we evaluate the security of Support Vector Machines (SVMs) to wellcrafted, adversarial label noise attacks. In particular, we consider an attacker that aims to maximize the SVM's classification error by flipping a number of labels in the training data. We formalize a corresponding optimal attack strategy, and solve it by means of heuristic approaches to keep the computational complexity tractable. We report an extensive experimental analysis on the effectiveness of the considered attacks against linear and non-linear SVMs, both on synthetic and real-world datasets. We finally argue that our approach can also provide useful insights for developing more secure SVM learning algorithms, and also novel techniques in a number of related research areas, such as semi-supervised and active learning. (C) 2015 Elsevier B.V. All rights reserved."	"Ch6ia
Times Cited:80
Cited References Count:45"	""	"<Go to ISI>://WOS:000354139100005"	""	"Tech Univ Munich, Dept Comp Sci, D-85748 Garching, Germany
Univ Cagliari, Dept Elect & Elect Engn, I-09123 Cagliari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. Ditzler; A. Prater"	"2017"	"Fine Tuning Lasso in an Adversarial Environment Against Gradient Attacks"	""	"2017 Ieee Symposium Series on Computational Intelligence (Ssci)"	""	""	""	""	""	"1828-1834"	""	""	""	""	""	""	""	"Fine Tuning Lasso in an Adversarial Environment Against Gradient Attacks"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000428251401126"	""	""	"feature selection
adversarial machine learning
supervised learning
feature-selection"	"Machine learning and data mining algorithms typically assume that the training and testing data are sampled from the same fixed probability distribution; however, this violation is often violated in practice. The field of domain adaptation addresses the situation where this assumption of a fixed probability between the two domains is violated; however, the difference between the two domains (training/source and testing/target) may not be known a priori. There has been a recent thrust in addressing the problem of learning in the presence of an adversary, which we formulate as a problem of domain adaption to build a more robust classifier. This is because the overall security of classifiers and their preprocessing stages have been called into question with the recent findings of adversaries in a learning setting. Adversarial training (and testing) data pose a serious threat to scenarios where an attacker has the opportunity to "poison" the training or "evade" on the testing data set(s) in order to achieve something that is not in the best interest of the classifier. Recent work has begun to show the impact of adversarial data on several classifiers; however, the impact of the adversary on aspects related to preprocessing of data (i.e., dimensionality reduction or feature selection) has widely been ignored in the revamp of adversarial learning research. Furthermore, variable selection, which is a vital component to any data analysis, has been shown to be particularly susceptible under an attacker that has knowledge of the task. In this work, we explore avenues for learning resilient classification models in the adversarial learning setting by considering the effects of adversarial data and how to mitigate its effects through optimization. Our model forms a single convex optimization problem that uses the labeled training data from the source domain and known weaknesses of the model for an adversarial component. We benchmark the proposed approach on synthetic data and show the trade-off between classification accuracy and skew-insensitive statistics."	"Bj8if
Times Cited:0
Cited References Count:30"	""	"<Go to ISI>://WOS:000428251401126"	""	"Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA
US Air Force, Res Lab, Informat Directorate, Rome, NY 13441 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. Rosenberg; E. Gudes"	"2017"	"Bypassing system calls-based intrusion detection systems"	""	"Concurrency and Computation-Practice & Experience"	""	""	"29"	""	"16"	""	""	""	""	""	"Aug 25"	""	""	"Bypassing system calls-based intrusion detection systems"	"Concurr Comp-Pract E"	"1532-0626"	"ARTN e4023
10.1002/cpe.4023"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000406232100010"	""	""	"behavior analysis
camouflage algorithm
decision trees
machine learning
malware detection
malware obfuscation"	"Machine learning augments today's intrusion detection system (IDS) capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS' classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS on the basis of various classifiers using system calls, executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code's functionality, for decision tree and random forest classifiers. We also present transformations to the classifier's input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates."	"Sp. Iss. SI
Fb6ds
Times Cited:7
Cited References Count:32"	""	"<Go to ISI>://WOS:000406232100010"	""	"Open Univ Israel, Raanana, Israel
Ben Gurion Univ Negev, Beer Sheva, Israel"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. Han; B. Olivier"	"2020"	"Interpretable and Adversarially-Resistant Behavioral Malware Signatures"	""	"Proceedings of the 35th Annual Acm Symposium on Applied Computing (Sac'20)"	""	""	""	""	""	"1668-1677"	""	""	""	""	""	""	""	"Interpretable and Adversarially-Resistant Behavioral Malware Signatures"	""	""	"10.1145/3341105.3373854"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000569720900234"	""	""	""	"Machine learning based techniques have been widely applied to dynamic malware analysis. However, such techniques largely complicate the understanding of predicted results due to their algorithm complexity. The situation becomes even worse with the application of deep learning techniques, which usually include complex architectures with multiple layers of transformations. In addition, most learning-based approaches are potentially vulnerable to behavior transformation attacks.
We propose a novel design of behavior-based malware signature, which achieves both the resistance against behavioral transformation and the ease of behavior interpretation. Our design mainly relies on the construction of behavioral signatures, obtained from unsupervised machine learning algorithms, and without requiring any expert knowledge. The behavioral signatures are then used as features for classification tasks. In contrast with prior learning-based works, our signatures provide straightforwardly interpretable information about the decision of classification. Analyzing several real-life malware samples with our signatures, we highlight very characteristic behaviors of some well-known malware families. In standard classification tasks, experiments show that we obtain comparable performances with respect to state-of-the-art techniques. Different to other classification techniques, our experiments demonstrate that our signature representation is resistant against behavioral transformations without affecting the interpretability of the results."	"Bp9nd
Times Cited:3
Cited References Count:38"	""	"<Go to ISI>://WOS:000569720900234"	""	"Orange Labs, Paris, France
Orange, Paris, France"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Calleja; A. Martin; H. D. Menendez; J. Tapiador; D. Clark"	"2018"	"Picking on the family: Disrupting android malware triage by forcing misclassification"	""	"Expert Systems with Applications"	""	""	"95"	""	""	"113-126"	""	""	""	""	"Apr 1"	""	""	"Picking on the family: Disrupting android malware triage by forcing misclassification"	"Expert Syst Appl"	"0957-4174"	"10.1016/j.eswa.2017.11.032"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000423635700009"	""	""	"malware classification
adversarial learning
genetic algorithms
iagodroid
classifiers
security"	"Machine learning classification algorithms are widely applied to different malware analysis problems because of their proven abilities to learn from examples and perform relatively well with little human input. Use cases include the labelling of malicious samples according to families during triage of suspected malware. However, automated algorithms are vulnerable to attacks. An attacker could carefully manipulate the sample to force the algorithm to produce a particular output. In this paper we discuss one such attack on Android malware classifiers. We design and implement a prototype tool, called lagoDroid, that takes as input a malware sample and a target family, and modifies the sample to cause it to be classified as belonging to this family while preserving its original semantics. Our technique relies on a search process that generates variants of the original sample without modifying their semantics. We tested lagoDroid against RevealDroid, a recent, open source, Android malware classifier based on a variety of static features. IagoDroid successfully forces misclassification for 28 of the 29 representative malware families present in the DREBIN dataset. Remarkably, it does so by modifying just a single feature of the original malware. On average, it finds the first evasive sample in the first search iteration, and converges to a 100% evasive population within 4 iterations. Finally, we introduce RevealDroid*, a more robust classifier that implements several techniques proposed in other adversarial learning domains. Our experiments suggest that RevealDroid* can correctly detect up to 99% of the variants generated by lagoDroid. (C) 2017 The Authors. Published by Elsevier Ltd."	"Fu1ut
Times Cited:25
Cited References Count:57"	""	"<Go to ISI>://WOS:000423635700009"	""	"Univ Carlos III Madrid, Dept Comp Sci, Madrid, Spain
Univ Autonoma Madrid, Dept Informat, Madrid, Spain
UCL, Gower St, London WCIE 6BT, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. F. Xue; C. X. Yuan; H. Y. Wu; Y. S. Zhang; W. Q. Liu"	"2020"	"Machine Learning Security: Threats, Countermeasures, and Evaluations"	""	"Ieee Access"	""	""	"8"	""	""	"74720-74742"	""	""	""	""	""	""	""	"Machine Learning Security: Threats, Countermeasures, and Evaluations"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.2987435"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000530830800100"	""	""	"machine learning
security
data models
machine learning algorithms
training
training data
prediction algorithms
artificial intelligence security
poisoning attacks
backdoor attacks
adversarial examples
privacy-preserving machine learning
poisoning attacks
defenses"	"Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented."	"Lk4kw
Times Cited:26
Cited References Count:123"	""	"<Go to ISI>://WOS:000530830800100"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 210016, Peoples R China
Nanjing Upsec Network Secur Technol Res Inst Co L, Nanjing 211100, Peoples R China
Nanjing Univ Aeronaut & Astronaut, Coll Elect & Informat Engn, Nanjing 210016, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"W. B. Jiang; H. W. Li; L. Gong; H. M. Yang; R. X. Lu"	"2020"	"Accelerating Poisoning Attack Through Momentum and Adam Algorithms"	""	"2020 Ieee 92nd Vehicular Technology Conference (Vtc2020-Fall)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Accelerating Poisoning Attack Through Momentum and Adam Algorithms"	"Veh Technol Confe"	""	"10.1109/VTC2020-Fall49728.2020.9348449"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000662218600019"	""	""	"poisoning attack
machine learning
defense algorithm"	"Machine learning has demonstrated promising application prospects in the field of vehicular technology during the past decade, for instance, it effectively propelled the development of autonomous vehicles and intelligent transportation systems. However, machine learning is still vulnerable to numerous malicious attacks. Amongst them, poisoning attack is one of the most severe security threats to the training process of machine learning, where the attacker injects some poisoned samples to the training dataset to make the learned model unavailable. As the crucial part of poisoning attack is generating poisoned samples, most proposals for poisoning attack have employed traditional gradient-based optimization algorithms to optimize the poisoned samples. Nevertheless, conventional gradient-based optimization algorithms are liable to get trapped in local optimums or saddle points and have a slow rate of convergence. As a result, these problems may lead to a reduction of the poisoned samples' effect. To address these issues, we propose two improved gradientbased poisoning attack algorithms. Specifically, in order to accelerate the convergence speed, we propose the first poisoning attack algorithm by employing momentum algorithm. Also, we propose the second poisoning attack algorithm by utilizing adam algorithm, which can get rid of some local optimums and has a faster convergence speed simultaneously. After that, support vector machines (SVM), linear regression and logistics regression are chosen as exemplary algorithms to conduct our attack algorithms and the effectiveness and computational overhead of the two attack algorithms are evaluated. Finally, we propose a countermeasure algorithm, which can detect suspicious samples using mahalanobis distance."	"Br6nn
Times Cited:0
Cited References Count:24
IEEE Vehicular Technology Conference VTC"	""	"<Go to ISI>://WOS:000662218600019"	""	"Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China
Peng Cheng Lab, Cyberspace Secur Res Ctr, Shenzhen 518000, Peoples R China
Univ New Brunswick, Fac Comp Sci, Fredericton, NB, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. W. Chow; W. Susilo; J. F. Wang; R. Buckland; J. Baek; J. Kim; N. Li"	"2021"	"Utilizing QR codes to verify the visual fidelity of image datasets for machine learning"	""	"Journal of Network and Computer Applications"	""	""	"173"	""	""	"102834"	""	""	""	""	"Jan 1"	""	""	"Utilizing QR codes to verify the visual fidelity of image datasets for machine learning"	"J Netw Comput Appl"	"1084-8045"	"ARTN 102834
10.1016/j.jnca.2020.102834"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000598596700001"	""	""	"adversarial machine learning
cyber security
qr code
visual fidelity
watermarking"	"Machine learning is becoming increasingly popular in modern technology and has been adopted in various application areas. However, researchers have demonstrated that machine learning models are vulnerable to adversarial examples in their inputs, which has given rise to a field of research known as adversarial machine learning. Potential adversarial attacks include methods of poisoning datasets by perturbing input samples to mislead machine learning models into producing undesirable results. While such perturbations are often subtle and imperceptible from the perspective of a human, they can greatly affect the performance of machine learning models. This paper presents two methods of verifying the visual fidelity of image-based datasets by using QR codes to detect perturbations in the data. In the first method, a verification string is stored for each image in a dataset. These verification strings can be used to determine whether or not an image in the dataset has been perturbed. In the second method, only a single verification string is stored and can be used to verify whether an entire dataset is intact.y"	"Pe8fg
Times Cited:3
Cited References Count:28"	""	"<Go to ISI>://WOS:000598596700001"	""	"Univ Wollongong, Inst Cybersecur & Cryptol, Sch Comp & Informat Technol, Wollongong, NSW, Australia
Xidian Univ, State Key Lab Integrated Serv Networks ISN, Xian, Peoples R China
Univ New South Wales, Sch Comp Sci & Engn, Cybercrime Cyberwar & Cyberterror, Sydney, NSW, Australia
Univ Newcastle, Sch Elect Engn & Comp, Callaghan, NSW, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Q. Liu; P. Li; W. T. Zhao; W. Cai; S. Yu; V. C. M. Leung"	"2018"	"A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View"	""	"Ieee Access"	""	""	"6"	""	""	"12103-12117"	""	""	""	""	""	""	""	"A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View"	"Ieee Access"	"2169-3536"	"10.1109/Access.2018.2805680"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000428582200001"	""	""	"machine learning
adversarial samples
security threats
defensive techniques
attacks"	"Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future."	"Ga8ic
Times Cited:131
Cited References Count:111"	""	"<Go to ISI>://WOS:000428582200001"	""	"Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China
Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada
Deakin Univ Melbourne, Sch Informat Technol, Burwood Campus, Burwood, Vic 3125, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Russu; A. Demontis; B. Biggio; G. Fumera; F. Roli"	"2016"	"Secure Kernel Machines against Evasion Attacks"	""	"Aisec'16: Proceedings of the 2016 Acm Workshop on Artificial Intelligence and Security"	""	""	""	""	""	"59-69"	""	""	""	""	""	""	""	"Secure Kernel Machines against Evasion Attacks"	""	""	"10.1145/2996758.2996771"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000391051600006"	""	""	"adversarial machine learning
evasion attacks
secure learning
kernel methods"	"Machine learning is widely used in security-sensitive settings like spam and malware detection, although it has been shown that malicious data can be carefully modified at test time to evade detection. To overcome this limitation, adversary-aware learning algorithms have been developed, exploiting robust optimization and game-theoretical models to incorporate knowledge of potential adversarial data manipulations into the learning algorithm. Despite these techniques have been shown to be effective in some adversarial learning tasks, their adoption in practice is hindered by different factors, including the difficulty of meeting specific theoretical requirements, the complexity of implementation, and scalability issues, in terms of computational time and space required during training. In this work, we aim to develop secure kernel machines against evasion attacks that are not computationally more demanding than their non-secure counterparts. In particular, leveraging recent work on robustness and regularization, we show that the security of a linear classifier can be drastically improved by selecting a proper regularizer, depending on the kind of evasion attack, as well as unbalancing the cost of classification errors. We then discuss the security of nonlinear kernel machines, and show that a proper choice of the kernel function is crucial. We also show that unbalancing the cost of classification errors and varying some kernel parameters can further improve classifier security, yielding decision functions that better enclose the legitimate data. Our results on spam and PDF malware detection corroborate our analysis."	"Bg6zd
Times Cited:21
Cited References Count:36"	""	"<Go to ISI>://WOS:000391051600006"	""	"Univ Cagliari, Piazza Armi, I-09123 Cagliari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Drews; A. Albarghouthi; L. D'Antoni"	"2020"	"Proving Data-Poisoning Robustness in Decision Trees"	""	"Proceedings of the 41st Acm Sigplan Conference on Programming Language Design and Implementation (Pldi '20)"	""	""	""	""	""	"1083-1097"	""	""	""	""	""	""	""	"Proving Data-Poisoning Robustness in Decision Trees"	""	""	"10.1145/3385412.3385975"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000614622300072"	""	""	"abstract interpretation
adversarial machine learning
decision trees
poisoning
robustness"	"Machine learning models are brittle, and small changes in the training data can result in different predictions. We study the problem of proving that a prediction is robust to data poisoning, where an attacker can inject a number of malicious elements into the training set to influence the learned model. We target decision-tree models, a popular and simple class of machine learning models that underlies many complex learning techniques. We present a sound verification technique based on abstract interpretation and implement it in a tool called Antidote. Antidote abstractly trains decision trees for an intractably large space of possible poisoned datasets. Due to the soundness of our abstraction, Antidote can produce proofs that, for a given input, the corresponding prediction would not have changed had the training set been tampered with or not. We demonstrate the effectiveness of Antidote on a number of popular datasets."	"Bq7do
Times Cited:2
Cited References Count:37"	""	"<Go to ISI>://WOS:000614622300072"	""	"Univ Wisconsin, Madison, WI 53706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Bhushan; Z. Y. Yang; N. Virani; N. Iyer"	"2020"	"Variational Encoder-Based Reliable Classification"	""	"2020 Ieee International Conference on Image Processing (Icip)"	""	""	""	""	""	"1941-1945"	""	""	""	""	""	""	""	"Variational Encoder-Based Reliable Classification"	"Ieee Image Proc"	"1522-4880"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000646178502010"	""	""	"classification
justified belief
reliability
interpretability
adversarial attacks
algorithms"	"Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in l(2)-distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding."	"Br3fp
Times Cited:0
Cited References Count:19
IEEE International Conference on Image Processing ICIP"	""	"<Go to ISI>://WOS:000646178502010"	""	"GE Res, 1 Res Circle, Niskayuna, NY 12309 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Chen; F. Luo; T. Li; T. Xiang; Z. L. Liu; J. Li"	"2020"	"A training-integrity privacy-preserving federated learning scheme with trusted execution environment"	""	"Information Sciences"	""	""	"522"	""	""	"69-79"	""	""	""	""	"Jun"	""	""	"A training-integrity privacy-preserving federated learning scheme with trusted execution environment"	"Inform Sciences"	"0020-0255"	"10.1016/j.ins.2020.02.037"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000530094300019"	""	""	"federated learning
data integrity
privacy preserving
trusted execution environment"	"Machine learning models trained on sensitive real-world data promise improvements to everything from medical screening to disease outbreak discovery. In many application domains, learning participants would benefit from pooling their private datasets, training precise machine learning models on the aggregate data, and sharing the profits of using these models. Considering privacy and security concerns often prevent participants from contributing sensitive data for training, researchers proposed several techniques to achieve data privacy in federated learning systems. However, such techniques are susceptible to causative attacks, whereby malicious participants can inject false training results with the aim of corrupting the well-learned model. To end this, in this paper, we propose a new privacy-preserving federated learning scheme that guarantees the integrity of deep learning processes. Based on the Trusted Execution Environment (TEE), we design a training-integrity protocol for this scheme, in which causative attacks can be detected. Thus, each participant is compelled to execute the privacy-preserving learning algorithm of the scheme correctly. We evaluate the performance of our scheme by prototype implementations. The experimental result shows that the scheme is training-integrity and practical. (C) 2020 Elsevier Inc. All rights reserved."	"Lj3ux
Times Cited:24
Cited References Count:54"	""	"<Go to ISI>://WOS:000530094300019"	""	"Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou, Peoples R China
Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China
Nankai Univ, Coll Cyber Sci, Tianjin, Peoples R China
Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Jagielski; G. Severi; N. P. Harger; M. Oprea"	"2021"	"Subpopulation Data Poisoning Attacks"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"3104-3122"	""	""	""	""	""	""	""	"Subpopulation Data Poisoning Attacks"	""	""	"10.1145/3460120.3485368"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478303009"	""	""	"adversarial machine learning
poisoning attacks
fairness"	"Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a subpopulation attack, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat."	"Bs7xr
Times Cited:1
Cited References Count:80"	""	"<Go to ISI>://WOS:000768478303009"	""	"Northeastern Univ, Boston, MA 02115 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Amsaleg; J. Bailey; A. Barbe; S. M. Erfani; T. Furon; M. E. Houle; M. Radovanovic; X. V. Nguyen"	"2021"	"High Intrinsic Dimensionality Facilitates Adversarial Attack: Theoretical Evidence"	""	"Ieee Transactions on Information Forensics and Security"	""	""	"16"	""	""	"854-865"	""	""	""	""	""	""	""	"High Intrinsic Dimensionality Facilitates Adversarial Attack: Theoretical Evidence"	"Ieee T Inf Foren Sec"	"1556-6013"	"10.1109/Tifs.2020.3023274"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000576264500012"	""	""	"adversarial attack
intrinsic dimensionality
nearest neighbor"	"Machine learning systems are vulnerable to adversarial attack. By applying to the input object a small, carefully-designed perturbation, a classifier can be tricked into making an incorrect prediction. This phenomenon has drawn wide interest, with many attempts made to explain it. However, a complete understanding is yet to emerge. In this paper we adopt a slightly different perspective, still relevant to classification. We consider retrieval, where the output is a set of objects most similar to a user-supplied query object, corresponding to the set of k-nearest neighbors. We investigate the effect of adversarial perturbation on the ranking of objects with respect to a query. Through theoretical analysis, supported by experiments, we demonstrate that as the intrinsic dimensionality of the data domain rises, the amount of perturbation required to subvert neighborhood rankings diminishes, and the vulnerability to adversarial attack rises. We examine two modes of perturbation of the query: either 'closer' to the target point, or 'farther' from it. We also consider two perspectives: 'query-centric', examining the effect of perturbation on the query's own neighborhood ranking, and 'target-centric', considering the ranking of the query point in the target's neighborhood set. All four cases correspond to practical scenarios involving classification and retrieval."	"Ny2zo
Times Cited:3
Cited References Count:51"	""	"<Go to ISI>://WOS:000576264500012"	""	"Univ Rennes, INRIA, CNRS, IRISA, Campus Beaulieu, F-35042 Rennes, France
Univ Melbourne, Sch Comp & Informat Syst, Parkville, Vic 3010, Australia
Ecole Normale Super Lyon, Lab Phys, F-69364 Lyon, France
Natl Inst Informat, Tokyo 1018430, Japan
Univ Novi Sad, Fac Sci, Novi Sad 21000, Serbia
NVIDIA Corp, Santa Clara, CA 95051 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Steinhardt; P. W. Koh; P. Liang"	"2017"	"Certified Defenses for Data Poisoning Attacks"	""	"Advances in Neural Information Processing Systems 30 (Nips 2017)"	""	""	"30"	""	""	""	""	""	""	""	""	""	""	"Certified Defenses for Data Poisoning Attacks"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000452649403057"	""	""	"security"	"Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (nonpoisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data."	"Bl5st
Times Cited:2
Cited References Count:65
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000452649403057"	""	"Stanford Univ, Stanford, CA 94305 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. P. K. Chan; Z. M. He; H. J. Li; C. C. Hsu"	"2018"	"Data sanitization against adversarial label contamination based on data complexity"	""	"International Journal of Machine Learning and Cybernetics"	""	""	"9"	""	"6"	"1039-1052"	""	""	""	""	"Jun"	""	""	"Data sanitization against adversarial label contamination based on data complexity"	"Int J Mach Learn Cyb"	"1868-8071"	"10.1007/s13042-016-0629-5"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000432817200012"	""	""	"adversarial learning
poisoning attack
data sanitization
data complexity
classifiers
security
attacks"	"Machine learning techniques may suffer from adversarial attack in which an attacker misleads a learning process by manipulating training samples. Data sanitization is one of countermeasures against poisoning attack. It is a data pre-processing method which filters suspect samples before learning. Recently, a number of data sanitization methods are devised for label flip attack, but their flexibility is limited due to specific assumptions. It is observed that abrupt label flip caused by attack changes complexity of classification. A data sanitization method based on data complexity, which is a measure of the difficulty of classification on a dataset, is proposed in this paper. Our method measures the data complexity of a training set after removing a sample and its nearest samples. Contaminated samples are then distinguished from untainted samples according to their data complexity values. Experimental results support the idea that data complexity can be used to identify attack samples. The proposed method achieves a better result than the current sanitization method in terms of detection accuracy for well known security application problems."	"Gg6ou
Times Cited:16
Cited References Count:47"	""	"<Go to ISI>://WOS:000432817200012"	""	"South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China
Foshan Univ, Sch Elect & Informat Engn, Foshan 528000, Peoples R China
Fu Jen Catholic Univ, Comp Sci & Informat Engn, New Taipei, Taiwan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. Apruzzese; M. Colajanni; L. Ferretti; M. Marchetti"	"2019"	"Addressing Adversarial Attacks Against Security Systems Based on Machine Learning"	""	"2019 11th International Conference on Cyber Conflict (Cycon): Silent Battle"	""	""	"900"	""	""	"383-400"	""	""	""	""	""	""	""	"Addressing Adversarial Attacks Against Security Systems Based on Machine Learning"	"Int Conf Cyber Confl"	"2325-5366"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000484938000021"	""	""	"adversarial attacks
machine learning
deep learning
poisoning attacks
evasion attacks
intrusion detection"	"Machine-learning solutions are successfully adopted in multiple contexts but the application of these techniques to the cyber security domain is complex and still immature. Among the many open issues that affect security systems based on machine learning, we concentrate on adversarial attacks that aim to affect the detection and prediction capabilities of machine-learning models. We consider realistic types of poisoning and evasion attacks targeting security solutions devoted to malware, spam and network intrusion detection. We explore the possible damages that an attacker can cause to a cyber detector and present some existing and original defensive techniques in the context of intrusion detection systems. This paper contains several performance evaluations that are based on extensive experiments using large traffic datasets. The results highlight that modern adversarial attacks are highly effective against machine-learning classifiers for cyber detection, and that existing solutions require improvements in several directions. The paper paves the way for more robust machine-learning-based techniques that can be integrated into cyber security platforms."	"Bn6at
Times Cited:14
Cited References Count:55
International Conference on Cyber Conflict"	""	"<Go to ISI>://WOS:000484938000021"	""	"Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Modena, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Y. Liu; L. Sun; X. Q. Mao"	"2022"	"Output-correlated adversarial attack for image translation network"	""	"Journal of Electronic Imaging"	""	""	"31"	""	"2"	"023030 - 023030"	""	""	""	""	"Mar 1"	""	""	"Output-correlated adversarial attack for image translation network"	"J Electron Imaging"	"1017-9909"	"Artn 023030
10.1117/1.Jei.31.2.023030"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000789627500030"	""	""	"generative adversarial network
image translation
adversarial attack
output diversification initialization"	"Maliciously forged images generated by image translation networks can cause significant security threats to personal privacy and national security. An emerging solution for forged images is preventing image forgery models from tampering with user images through adversarial attacks. Currently, conventional adversarial generation algorithms use random noise as the starting point, which makes the final adversarial output similar to the original output but does not prevent image tampering. The output-correlated initialization is applied to improve the adversarial attack algorithm for the image translation network and improve the visual effect of adversarial attacks. Moreover, a comparative experiment is performed on multiple loss functions, and the loss function with the best performance is selected as the adversarial loss function to complete the adversarial attack on the image translation network. The selected initialization method makes the search process of adversarial examples more comprehensive and makes the generation results of adversarial examples more diverse. The analysis of the visual effects of the attack reveals how the proposed adversarial attack methods affect the forgery results of different image translation frameworks and generate more chaotic images. Comparison of multiple indicators demonstrates that the proposed method has a high attack success rate and expands the image distance between the adversarial output and the original output, thereby improving the attack efficiency, preventing malicious tampering with the image, and protecting the user image. (C) 2022 SPIE and IS&T"	"0x3rh
Times Cited:0
Cited References Count:40"	""	"<Go to ISI>://WOS:000789627500030"	""	"Informat Engn Univ, Zhengzhou, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"K. F. Zhao; H. Zhou; Y. L. Zhu; X. Zhan; K. Zhou; J. F. Li; L. Yu; W. Yuan; X. P. Luo"	"2021"	"Structural Attack against Graph Based Android Malware Detection"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"3218-3235"	""	""	""	""	""	""	""	"Structural Attack against Graph Based Android Malware Detection"	""	""	"10.1145/3460120.3485387"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478303015"	""	""	"structural attack
android malware detection
function call graph"	"Malware detection techniques achieve great success with deeper insight into the semantics of malware. Among existing detection techniques, function call graph (FCG) based methods achieve promising performance due to their prominent representations of malware's functionalities. Meanwhile, recent adversarial attacks not only perturb feature vectors to deceive classifiers (i.e., feature-space attacks) but also investigate how to generate real evasive malware (i.e., problem-space attacks). However, existing problem-space attacks are limited due to their inconsistent transformations between feature space and problem space.
In this paper, we propose the first structural attack against graph-based Android malware detection techniques, which addresses the inverse-transformation problem [1] between feature-space attacks and problem-space attacks. We design a Heuristic optimization model integrated with Reinforcement learning framework to optimize our structural ATtack (HRAT). HRAT includes four types of graph modifications (i.e., inserting and deleting nodes, adding edges and rewiring) that correspond to four manipulations on apps (i.e., inserting and deleting methods, adding call relation, rewiring). Through extensive experiments on over 30k Android apps, HRAT demonstrates outstanding attack performance on both feature space (over 90% attack success rate) and problem space (up to 100% attack success rate in most cases). Besides, the experiment results show that combing multiple attack behaviors strategically makes the attack more effective and efficient."	"Bs7xr
Times Cited:0
Cited References Count:68"	""	"<Go to ISI>://WOS:000768478303015"	""	"Hong Kong Polytech Univ, Hong Kong, Peoples R China
Huazhong Univ Sci & Technol, Wuhan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. K. Ng; F. Jiang; L. Y. Zhang; W. L. Zhou"	"2019"	"Static malware clustering using enhanced deep embedding method"	""	"Concurrency and Computation-Practice & Experience"	""	""	"31"	""	"19"	""	""	""	""	""	"Oct 10"	""	""	"Static malware clustering using enhanced deep embedding method"	"Concurr Comp-Pract E"	"1532-0626"	"ARTN e5234
10.1002/cpe.5234"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000486203400010"	""	""	"deep learning
malware clustering
machine learning
network
classification"	"Malware refers to any software, programs, or files that are intentionally utilised to compromise the system and cause unexpected losses to end-users such as economical losses or privacy breaches. The rapid growth of malware makes it impossible to keep up with its progress merely via human interventions or manual analysis. One of the challenges for the human-oriented approaches is they will cause backlog and inability to keep up with the development traces of the malware. Hence, an efficient method is needed urgently to analyse effectively and identify accurately the malware in their domain. Malware clustering has been extensively studied in the machine learning area with regards to distance functions, grouping algorithm and cluster validation. A large number of research studies have been done via behavioral analysis for clustering to achieve high performance of malware detections. However, there is a trade-off for better detection performance between behaviorial approaches and high computational forces. Up to date, little work focuses on the deep learning representations for malware clustering. Therefore, in this paper, we propose an enhanced deep embedded clustering method to facilitate an effective and efficient malware clustering process. The new method takes advantage of linear dimensionality reduction and a customised deep neural network to learn malware representations in an orthogonal space and performs cluster assignments. Our experimental results demonstrate that the proposed clustering model outperforms the traditional K-means method with regards to the enhanced features using various auto-encoder, pre-trained weight and principle component analysis (PCA)."	"Sp. Iss. SI
Iy2eo
Times Cited:5
Cited References Count:44"	""	"<Go to ISI>://WOS:000486203400010"	""	"Deakin Univ, Fac Sci Engn & Built Environm, Geelong, Vic 3220, Australia
Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. D. Champneys; A. Green; J. Morales; M. Silva; D. Mascarenas"	"2021"	"On the vulnerability of data-driven structural health monitoring models to adversarial attack"	""	"Structural Health Monitoring-an International Journal"	""	""	"20"	""	"4"	"1476-1493"	""	""	""	""	"Jul"	""	""	"On the vulnerability of data-driven structural health monitoring models to adversarial attack"	"Struct Health Monit"	"1475-9217"	"Artn 1475921720920233
10.1177/1475921720920233"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000537540100001"	""	""	"structural health monitoring
adversarial attack
threat model"	"Many approaches at the forefront of structural health monitoring rely on cutting-edge techniques from the field of machine learning. Recently, much interest has been directed towards the study of so-called adversarial examples; deliberate input perturbations that deceive machine learning models while remaining semantically identical. This article demonstrates that data-driven approaches to structural health monitoring are vulnerable to attacks of this kind. In the perfect information or 'white-box' scenario, a transformation is found that maps every example in the Los Alamos National Laboratory three-storey structure dataset to an adversarial example. Also presented is an adversarial threat model specific to structural health monitoring. The threat model is proposed with a view to motivate discussion into ways in which structural health monitoring approaches might be made more robust to the threat of adversarial attack."	"Sp. Iss. SI
Tb2ry
Times Cited:3
Cited References Count:44"	""	"<Go to ISI>://WOS:000537540100001"	""	"Univ Sheffield, Adv Mfg Res Ctr Boeing AMRC, Ind Doctorate Ctr Machining Sci, Rotherham S60 5TZ, S Yorkshire, England
Univ Sheffield, Dynam Res Grp, Sheffield, S Yorkshire, England
Los Alamos Natl Lab, Los Alamos, NM USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Liu; G. Ditzler"	"2019"	"Data Poisoning Attacks against Mrmr"	""	"2019 Ieee International Conference on Acoustics, Speech and Signal Processing (Icassp)"	""	""	""	""	""	"2517-2521"	""	""	""	""	""	""	""	"Data Poisoning Attacks against Mrmr"	"Int Conf Acoust Spee"	"1520-6149"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000482554002149"	""	""	"feature selection
information theory
adversarial learning
feature-selection"	"Many machine learning models lack the consideration that an adversary can alter data at the time of training or testing. Over the past decade, the machine learning models' vulnerability has been a concern and more secure algorithms are needed. Unfortunately, the security of feature selection (FS) remains an under-explored area. There are only a few works that address data poisoning algorithms that are targeted at embedded FS; however, data poisoning techniques targeted at information-theoretic FS do not exist. In this contribution, a novel data poisoning algorithm is proposed that targets failures in minimum Redundancy Maximum Relevance (mRMR). We demonstrate that mRMR can be easily poisoned to select features that would not normally have been selected."	"Bn4np
Times Cited:1
Cited References Count:19
International Conference on Acoustics Speech and Signal Processing ICASSP"	""	"<Go to ISI>://WOS:000482554002149"	""	"Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. J. Ji; X. Y. Zhang; S. L. Ji; X. P. Luo; T. Wang"	"2018"	"Model-Reuse Attacks on Deep Learning Systems"	""	"Proceedings of the 2018 Acm Sigsac Conference on Computer and Communications Security (Ccs'18)"	""	""	""	""	""	"349-363"	""	""	""	""	""	""	""	"Model-Reuse Attacks on Deep Learning Systems"	""	""	"10.1145/3243734.3243757"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000461315900023"	""	""	"deep learning systems
third-party model
model-reuse attack"	"Many of today's machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e. g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far.
In this paper, we demonstrate that malicious primitive models pose immense threats to the security of ML systems. We present a broad class of model-reuse attacks wherein maliciously crafted models trigger host ML systems to misbehave on targeted inputs in a highly predictable manner. By empirically studying four deep learning systems (including both individual and ensemble systems) used in skin cancer screening, speech recognition, face verification, and autonomous steering, we show that such attacks are (i) effective -the host systems misbehave on the targeted inputs as desired by the adversary with high probability, (ii) evasive - the malicious models function indistinguishably from their benign counterparts on non-targeted inputs, (iii) elastic - the malicious models remain effective regardless of various system design choices and tuning strategies, and (iv) easy - the adversary needs little prior knowledge about the data used for system tuning or inference. We provide analytical justification for the effectiveness of model-reuse attacks, which points to the unprecedented complexity of today's primitive models. This issue thus seems fundamental to many ML systems. We further discuss potential countermeasures and their challenges, which lead to several promising research directions."	"Bm2qn
Times Cited:36
Cited References Count:63"	""	"<Go to ISI>://WOS:000461315900023"	""	"Lehigh Univ, Bethlehem, PA 18015 USA
Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China
Alibaba ZJU Joint Res Inst Frontier Technol, Hangzhou, Zhejiang, Peoples R China
Hong Kong Polytech Univ, Hong Kong, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Inkawhich; W. Wen; H. Li; Y. R. Chen"	"2019"	"Feature Space Perturbations Yield More Transferable Adversarial Examples"	""	"2019 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr 2019)"	""	""	""	""	""	"7059-7067"	""	""	""	""	""	""	""	"Feature Space Perturbations Yield More Transferable Adversarial Examples"	"Proc Cvpr Ieee"	"1063-6919"	"10.1109/Cvpr.2019.00723"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000542649300053"	""	""	""	"Many recent works have shown that deep learning models are vulnerable to quasi-imperceptible input perturbations, yet practitioners cannot fully explain this behavior. This work describes a transfer-based blackbox targeted adversarial attack of deep feature space representations that also provides insights into cross-model class representations of deep CNNs. The attack is explicitly designed for transferability and drives feature space representation of a source image at layer L towards the representation of a target image at L. The attack yields highly transferable targeted examples, which outperform competition winning methods by over 30% in targeted attack metrics. We also show the choice of L to generate examples from is important, transferability characteristics are blackbox model agnostic, and indicate that well trained deep models have similar highly-abstract representations."	"Bp2ib
Times Cited:25
Cited References Count:25
IEEE Conference on Computer Vision and Pattern Recognition"	""	"<Go to ISI>://WOS:000542649300053"	""	"Duke Univ, Elect & Comp Engn Dept, Durham, NC 27708 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Y. Chen; C. T. Chen; C. Y. Sang; Y. C. Yang; S. H. Huang"	"2021"	"Adversarial Attacks Against Reinforcement Learning-Based Portfolio Management Strategy"	""	"Ieee Access"	""	""	"9"	""	""	"50667-50685"	""	""	""	""	""	""	""	"Adversarial Attacks Against Reinforcement Learning-Based Portfolio Management Strategy"	"Ieee Access"	"2169-3536"	"10.1109/Access.2021.3068768"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000638387700001"	""	""	"task analysis
portfolios
perturbation methods
deep learning
training
information management
solid modeling
reinforcement learning
adversarial attack"	"Many researchers have incorporated deep neural networks (DNNs) with reinforcement learning (RL) in automatic trading systems. However, such methods result in complicated algorithmic trading models with several defects, especially when a DNN model is vulnerable to malicious adversarial samples. Researches have rarely focused on planning for long-term attacks against RL-based trading systems. To neutralize these attacks, researchers must consider generating imperceptible perturbations while simultaneously reducing the number of modified steps. In this research, an adversary is used to attack an RL-based trading agent. First, we propose an extension of the ensemble of the identical independent evaluators (EIIE) method, called enhanced EIIE, in which information on the best bids and asks is incorporated. Enhanced EIIE was demonstrated to produce an authoritative trading agent that yields better portfolio performance relative to that of an EIIE agent. Enhanced EIIE was then applied to the adversarial agent for the agent to learn when and how much to attack (in the form of introducing perturbations).In our experiments, our proposed adversarial attack mechanisms were > 30% more effective at reducing accumulated portfolio value relative to the conventional attack mechanisms of the fast gradient sign method (FSGM) and iterative FSGM, which are currently more commonly researched and adapted to compare and improve."	"Rk6es
Times Cited:3
Cited References Count:40"	""	"<Go to ISI>://WOS:000638387700001"	""	"Natl Chiao Tung Univ, Inst Informat Management, Hsinchu 30010, Taiwan
Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan
Natl Chiao Tung Univ, Dept Informat Management & Finance, Hsinchu 30010, Taiwan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Lin; K. Q. Xiong"	"2021"	"Mahalanobis distance-based robust approaches against false data injection attacks on dynamic power state estimation"	""	"Computers & Security"	""	""	"108"	""	""	"102326"	""	""	""	""	"Sep"	""	""	"Mahalanobis distance-based robust approaches against false data injection attacks on dynamic power state estimation"	"Comput Secur"	"0167-4048"	"ARTN 102326
10.1016/j.cose.2021.102326"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000681264400020"	""	""	"cyber-physical systems
power grids
kalman filters
state estimation
cyber-physical systems
kalman filter
security"	"Many researchers have studied false data injection (FDI) attacks in power state estimation, but existing state estimation approaches are still highly vulnerable to FDI attacks. Currently, most existing studies on FDI attacks focus on static state estimation (SSE), where power system states are not changed with time, and one of them includes the discovery of three efficient FDI attacks that can introduce arbitrary large errors into certain state variables without being detected by existing bad measurement detection algorithms. In reality, however, power states are varied with time in real-world power systems. In this paper, we investigate the problem of the above three FDI attacks against dynamic power state estimation (DSE). Although the three attacks were discovered in SSE several years ago, none of them has been well addressed in static power state systems. In this research, we propose two robust defense approaches against the above three efficient FDI attacks on DSE. Compared to existing approaches, our proposed approaches have three major differences and significant strengths: (1) they defend against the three FDI attacks on dynamic power state estimation rather than static power state estimation, (2) they give a robust estimator that can accurately extract a subset of attack-free sensors for power state estimation, and (3) they adopt the little-known Mahalanobis distance in the consistency check of power sensor measurements, which is different from the Euclidean distance used in all the existing studies on power state estimation. We mathematically prove that the Mahalanobis distance is not only useful but also much better than the Euclidean distance in the consistency check of power sensor measurements. Our time complexity analysis shows that the two proposed robust defense approaches are efficient. Moreover, in order to demonstrate the effectiveness of the proposed approaches, we compare them with the three well-known approaches: the least square approach, the Imhotep-SMT approach, and the MEE-UKF approach. Our extensive xperiments show that the proposed approaches further reduce the estimation error by two orders of magnitude and four orders of magnitude compared to the Imhotep-SMT approach and the least square approach, respectively. Moreover, our approach is more stable than the MEE-UKF approach. (c) 2021 Elsevier Ltd. All rights reserved."	"Tu8er
Times Cited:0
Cited References Count:45"	""	"<Go to ISI>://WOS:000681264400020"	""	"Univ S Florida, ICNS Lab & Cyber Florida, Tampa, FL 33620 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. F. Qi; J. J. Hu; Z. Yi"	"2021"	"Missed diagnoses detection by adversarial learning"	""	"Knowledge-Based Systems"	""	""	"220"	""	""	"106903"	""	""	""	""	"May 23"	""	""	"Missed diagnoses detection by adversarial learning"	"Knowl-Based Syst"	"0950-7051"	"10.1016/j.knosys.2021.106903"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000637680300013"	""	""	"missed diagnoses detection
medical image
deep neural network"	"Missed diagnosis has been a serious public health issue in clinical diagnosis and treatment, it may cause disease deterioration and reduce cure rate. In recent years, many deep learning approaches have been proposed for automated medical image classification. In this study, we propose a new methodology to detect the images missed diagnosed by deep learning classifiers. Based on the intermediate feature maps of deep learning classifiers, the proposed model can detect missed diagnosed sample and reduce the missed diagnoses rate, with no obvious decrease in the accuracy. The proposed model is constructed using generative adversarial networks and autoencoders to learn consistent mapping from data space to latent space, and is trained with adversarial examples. After training, the output of the discriminator is used to recognize missed diagnosed samples. The method is evaluated on different network architectures and various types of medical image datasets and achieves promising results. Compared with other state-of-the-art approaches, the proposed method shows superior performance on most datasets. (c) 2021 Elsevier B.V. All rights reserved."	"Rj5zk
Times Cited:1
Cited References Count:34"	""	"<Go to ISI>://WOS:000637680300013"	""	"Sichuan Univ, Coll Comp Sci, Machine Intelligence Lab, Chengdu 610065, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. A. Bello; H. Chiroma; A. Y. Gital; L. A. Gabralla; S. M. Abdulhamid; L. Shuib"	"2020"	"Machine learning algorithms for improving security on touch screen devices: a survey, challenges and new perspectives"	""	"Neural Computing & Applications"	""	""	"32"	""	"17"	"13651-13678"	""	""	""	""	"Sep"	""	""	"Machine learning algorithms for improving security on touch screen devices: a survey, challenges and new perspectives"	"Neural Comput Appl"	"0941-0643"	"10.1007/s00521-020-04775-0"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000516248500003"	""	""	"machine learning algorithms
deep learning
mobile phone touch screen
android
support vector machine
command attention
security
support vector machine
continuous authentication
random forest
classification
input"	"Mobile phone touch screen devices are equipped with high processing power and high memory. This led to users not only storing photos or videos but stored sensitive application such as banking applications. As a result of that the security system of the mobile phone touch screen devices becomes sacrosanct. The application of machine learning algorithms in enhancing security on mobile phone touch screen devices is gaining a tremendous popularity in both academia and the industry. However, notwithstanding the growing popularity, up to date no comprehensive survey has been conducted on machine learning algorithms solutions to improve the security of mobile phone touch screen devices. This survey aims to connect this gap by conducting a comprehensive survey on the solutions of machine learning algorithms to improve the security of mobile phone touch screen devices including the analysis and synthesis of the algorithms and methodologies provided for those solutions. This article presents a comprehensive survey and a new taxonomy of the state-of-the-art literature on machine learning algorithms in improving the security of mobile phone touch screen devices. The limitation of the methodology in each article reviewed is pointed out. Challenges of the existing approaches and new perspective of future research directions for developing more accurate and robust solutions to mobile phone touch screen security are discussed. In particular, the survey found that exploring of different aspects of deep learning solutions to improve the security of mobile phone touch screen devices is under-explored."	"Nb5mu
Times Cited:6
Cited References Count:90"	""	"<Go to ISI>://WOS:000516248500003"	""	"Abubakar Tafawa Balewa Univ, Dept Math Sci, Bauchi, Nigeria
Fed Coll Educ Tech, Dept Comp Sci, Gombe, Gombe, Nigeria
Princess Nourah Bint Abdulrahman Univ, Community Coll, Dept Comp Sci & Informat Technol, Riyadh, Saudi Arabia
Fed Univ Technol, Dept Cyber Secur Sci, Minna, Minna, Nigeria
Univ Malaya, Fac Comp Sci & Informat Technol, Dept Informat Syst, Kuala Lumpur, Malaysia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. H. Chen; M. Tang; J. H. Li"	"2022"	"Inversion Attacks against CNN Models Based on Timing Attack"	""	"Security and Communication Networks"	""	""	"2022"	""	""	""	""	""	""	""	"Feb 26"	""	""	"Inversion Attacks against CNN Models Based on Timing Attack"	"Secur Commun Netw"	"1939-0114"	"Artn 6285909
10.1155/2022/6285909"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000773607300003"	""	""	""	"Model confidentiality attacks on convolutional neural networks (CNN) are becoming more and more common. At present, model reverse attack is an important means of model confidentiality attacks, but all of these attacks require strong attack ability, meanwhile, the success rates of these attacks are low. We study the time leakage of CNN running on the SoC (system on-chip) system and propose a reverse method based on side-channel attack. It uses the SDK tool-profiler to collect the time leakage of different networks of various CNNs. According to the linear relationship between time leakage, calculation, and memory usage parameters, we take the profiling attack to establish a mapping library of time and the different networks. After that, the smallest difference between the measured time of unknown models and the theoretical time in the mapping library is considered to be the real parameters of the unknown models. Finally, we can reverse other layers even the entire model. Based on the experiments, the reverse success rate of common convolutional layers is above 78.5%, and the reverse success rates of different CNNs (such as AlexNet, ConvNet, LeNet, etc.) are all above 67.67%. Moreover, the results show that the success rate of our method is 10% higher than the traditional methods on average. In the adversarial sample attack, the success rate reached 97%."	"Zz9ua
Times Cited:0
Cited References Count:34"	""	"<Go to ISI>://WOS:000773607300003"	""	"Wuhan Univ, Sch Cyber Sci & Engn, Key Lab Aerosp Informat Secur & Trusted Comp, Minist Educ, Wuhan 430072, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Jha; U. Jang; S. Jha; B. Jalaian"	"2018"	"Detecting Adversarial Examples Using Data Manifolds"	""	"2018 Ieee Military Communications Conference (Milcom 2018)"	""	""	""	""	""	"547-552"	""	""	""	""	""	""	""	"Detecting Adversarial Examples Using Data Manifolds"	"Ieee Milit Commun C"	"2155-7578"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000459819200091"	""	""	"trusted machine learning
robustness
adversarial examples
manifolds"	"Models produced by machine learning, particularly deep neural networks, are state-of-the-art for many machine learning tasks and demonstrate very high prediction accuracy. Unfortunately, these models are also very brittle and vulnerable to specially crafted adversarial examples. Recent results have shown that accuracy of these models can be reduced from close to hundred percent to below 5% using adversarial examples. This brittleness of deep neural networks makes it challenging to deploy these learning models in security-critical areas where adversarial activity is expected, and cannot be ignored. A number of methods have been recently proposed to craft more effective and generalizable attacks on neural networks along with competing efforts to improve robustness of these learning models. But the current approaches to make machine learning techniques more resilient fall short of their goal. Further, the succession of new adversarial attacks against proposed methods to increase neural network robustness raises doubts about a foolproof approach to robustify machine learning models against all possible adversarial attacks. In this paper, we consider the problem of detecting adversarial examples. This would help identify when the learning models cannot be trusted without attempting to repair the models or make them robust to adversarial attacks. This goal of finding limitations of the learning model presents a more tractable approach to protecting against adversarial attacks. Our approach is based on identifying a low dimensional manifold in which the training samples lie, and then using the distance of a new observation from this manifold to identify whether this data point is adversarial or not. Our empirical study demonstrates that adversarial examples not only lie farther away from the data manifold, but this distance from manifold of the adversarial examples increases with the attack confidence. Thus, adversarial examples that are likely to result into incorrect prediction by the machine learning model is also easier to detect by our approach. This is a first step towards formulating a novel approach based on computational geometry that can identify the limiting boundaries of a machine learning model, and detect adversarial attacks."	"Bm1ht
Times Cited:3
Cited References Count:31
IEEE Military Communications Conference"	""	"<Go to ISI>://WOS:000459819200091"	""	"SRI Int, Comp Sci Lab, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA
Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA
US Army, Res Lab ARL, Computat & Informat Sci Directorate, Adelphi, MD USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Hayase; W. H. Kong; R. Somani; S. Oh"	"2021"	"SPECTRE Defending Against Backdoor Attacks Using Robust Statistics"	""	"International Conference on Machine Learning, Vol 139"	""	""	"139"	""	""	""	""	""	""	""	""	""	""	"SPECTRE Defending Against Backdoor Attacks Using Robust Statistics"	"Pr Mach Learn Res"	"2640-3498"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000683104604014"	""	""	""	"Modern machine learning increasingly requires training on a large collection of data from multiple sources, not all of which can be trusted. A particularly concerning scenario is when a small fraction of poisoned data changes the behavior of the trained model when triggered by an attacker-specified watermark. Such a compromised model will be deployed unnoticed as the model is accurate otherwise. There have been promising attempts to use the intermediate representations of such a model to separate corrupted examples from clean ones. However, these defenses work only when a certain spectral signature of the poisoned examples is large enough for detection. There is a wide range of attacks that cannot be protected against by the existing defenses. We propose a novel defense algorithm using robust covariance estimation to amplify the spectral signature of corrupted data. This defense provides a clean model, completely removing the backdoor, even in regimes where previous methods have no hope of detecting the poisoned examples."	"Bs0jp
Times Cited:1
Cited References Count:55
Proceedings of Machine Learning Research"	""	"<Go to ISI>://WOS:000683104604014"	""	"Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Ahmadi; D. Ulyanov; S. Semenov; M. Trofimov; G. Giacinto"	"2016"	"Novel Feature Extraction, Selection and Fusion for Effective Malware Family Classification"	""	"Codaspy'16: Proceedings of the Sixth Acm Conference on Data and Application Security and Privacy"	""	""	""	""	""	"183-194"	""	""	""	""	""	""	""	"Novel Feature Extraction, Selection and Fusion for Effective Malware Family Classification"	""	""	"10.1145/2857705.2857713"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000387921800027"	""	""	"windows malware
machine learning
malware family
computer security
classification
microsoft malware classification challenge
entropy"	"Modern malware is designed with mutation characteristics, namely polymorphism and metamorphism, which causes an enormous growth in the number of variants of malware samples. Categorization of malware samples on the basis of their behaviors is essential for the computer security community, because they receive huge number of malware everyday, and the signature extraction process is usually based on malicious parts characterizing malware families. Microsoft released a malware classification challenge in 2015 with a huge dataset of near 0.5 terabytes of data, containing more than 20K malware samples. The analysis of this dataset inspired the development of a novel paradigm that is effective in categorizing malware variants into their actual family groups. This paradigm is presented and discussed in the present paper, where emphasis has been given to the phases related to the extraction, and selection of a set of novel features for the effective representation of malware samples. Features can be grouped according to different characteristics of malware behavior, and their fusion is performed according to a per-class weighting paradigm. The proposed method achieved a very high accuracy (approximate to 0.998) on the Microsoft Malware Challenge dataset."	"Bg3eo
Times Cited:134
Cited References Count:45"	""	"<Go to ISI>://WOS:000387921800027"	""	"Univ Cagliari, Dept Elect & Elect Engn, I-09124 Cagliari, Italy
Skolkovo Inst Sci & Technol, Moskovsky, Russia
Natl Res Univ, Higher Sch Econ, Moscow, Russia
Moscow Inst Phys & Technol, Dolgoprudnyi, Moskovskaya Obl, Russia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"O. B. Botocan; G. Czibula"	"2017"	"HACGA: An artifacts-based clustering approach for malware classification"	""	"2017 13th Ieee International Conference on Intelligent Computer Communication and Processing (Iccp)"	""	""	""	""	""	"5-12"	""	""	""	""	""	""	""	"HACGA: An artifacts-based clustering approach for malware classification"	"Int C Intell Comp Co"	"2065-9946"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000417426600001"	""	""	"malicious software
behavioral analysis
machine learning
hierarchical clustering
genetic algorithms"	"More and more sophisticated malware attacks are developed nowadays and new variants of existing malicious software are released daily. Malware clustering is often applied to identify patterns of malicious software, with similar samples being grouped together and considered variants of the same malware family. In this paper we propose an automated technique based on agglomerative hierarchical clustering combined with a supervised learning method for parameters optimization which helps determining samples that exhibit the same behavior, allowing malware analysts to uncover new and interesting threats. The proposed method relies on behavioral and attack pattern analysis. Despite the complexity of nowadays malicious software, the attacks of same malware families are very similar in terms of actions performed on the infected system. The experimental evaluation is carried out on a real case study and the results are analyzed, interpreted and compared to the ones of similar existing approaches. Our experiments demonstrate the capability of the proposed clustering method to accurately identify groups of similar malware samples, which are very likely to represent malware families."	"Bj1dj
Times Cited:1
Cited References Count:21
IEEE International Conference on Intelligent Computer Communication and Processing ICCP"	""	"<Go to ISI>://WOS:000417426600001"	""	"Babes Bolyai Univ, Fac Math & Comp Sci, 1,M Kogalniceanu St, Cluj Napoca 400084, Romania"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. F. Rajotte; R. T. Ng"	"2020"	"Private data sharing between decentralized users through the privGAN architecture"	""	"2020 Ieee 24th International Enterprise Distributed Object Computing Workshop (Edocw 2020)"	""	""	""	""	""	"37-42"	""	""	""	""	""	""	""	"Private data sharing between decentralized users through the privGAN architecture"	"Ieee Int Enterp"	"2325-6583"	"10.1109/Edocw49879.2020.00018"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000630251700005"	""	""	"synthetic data
gan
privacy
distributed data
federated learning"	"More data is almost always beneficial for analysis and machine learning tasks. In many realistic situations however, an enterprise cannot share its data, either to keep a competitive advantage or to protect the privacy of the data sources, the enterprise's clients for example. We propose a method for data owners to share synthetic or fake versions of their data without sharing the actual data, nor the parameters of models that have direct access to the data. The method proposed is based on the privGAN architecture where local GANs are trained on their respective data subsets with an extra penalty from a central discriminator aiming to discriminate the origin of a given fake sample. We demonstrate that this approach, when applied to subsets of various sizes, leads to better utility for the owners than the utility from their real small datasets. The only shared pieces of information are the parameter updates of the central discriminator. The privacy is demonstrated with white-box attacks on the most vulnerable elments of the architecture and the results are close to random guessing. This method would apply naturally in a federated learning setting."	"Br0rf
Times Cited:0
Cited References Count:21
IEEE International Enterprise Distributed Object Computing Conference Workshops-EDOCW"	""	"<Go to ISI>://WOS:000630251700005"	""	"Univ British Columbia, Data Sci Inst, Vancouver, BC, Canada
Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Assion; P. Schlicht; F. Gressner; W. Gunther; F. Huger; N. Schmidt; U. Rasheed"	"2019"	"The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks"	""	"2019 Ieee/Cvf Conference on Computer Vision and Pattern Recognition Workshops (Cvprw 2019)"	""	""	""	""	""	"1370-1379"	""	""	""	""	""	""	""	"The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks"	"Ieee Comput Soc Conf"	"2160-7508"	"10.1109/Cvprw.2019.00177"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000569983600171"	""	""	""	"Most state-of-the-art machine learning (ML) classification systems are vulnerable to adversarial perturbations. As a consequence, adversarial robustness poses a significant challenge for the deployment of ML-based systems in safety- and security-critical environments like autonomous driving, disease detection or unmanned aerial vehicles. In the past years we have seen an impressive amount of publications presenting more and more new adversarial attacks. However, the attack research seems to be rather unstructured and new attacks often appear to be random selections from the unlimited set of possible adversarial attacks. With this publication, we present a structured analysis of the adversarial attack creation process. By detecting different building blocks of adversarial attacks, we outline the road to new sets of adversarial attacks. We call this the "attack generator". In the pursuit of this objective, we summarize and extend existing adversarial perturbation taxonomies. The resulting taxonomy is then linked to the application context of computer vision systems for autonomous vehicles, i.e. semantic segmentation and object detection. Finally, in order to prove the usefulness of the attack generator, we investigate existing semantic segmentation attacks with respect to the detected defining components of adversarial attacks."	"Bp9nt
Times Cited:1
Cited References Count:44
IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"	""	"<Go to ISI>://WOS:000569983600171"	""	"Neurocat GmbH, Berlin, Germany
Volkswagen AG, Wolfsburg, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Hayes; G. Danezis"	"2018"	"Learning Universal Adversarial Perturbations with Generative Models"	""	"2018 Ieee Symposium on Security and Privacy Workshops (Spw 2018)"	""	""	""	""	""	"43-49"	""	""	""	""	""	""	""	"Learning Universal Adversarial Perturbations with Generative Models"	""	""	"10.1109/Spw.2018.00015"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000674762400007"	""	""	"machine"	"Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks."	"Br8zo
Times Cited:26
Cited References Count:39"	""	"<Go to ISI>://WOS:000674762400007"	""	"UCL, London, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. Q. Gao; T. L. Cai; H. C. Li; L. W. Wang; C. J. Hsieh; J. D. Lee"	"2019"	"Convergence of Adversarial Training in Overparametrized Neural Networks"	""	"Advances in Neural Information Processing Systems 32 (Nips 2019)"	""	""	"32"	""	""	""	""	""	""	""	""	""	""	"Convergence of Adversarial Training in Overparametrized Neural Networks"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000535866904064"	""	""	""	"Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network. Adversarial training [31], a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within epsilon of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the l(infinity)-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks."	"Bp0qe
Times Cited:6
Cited References Count:54
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000535866904064"	""	"Peking Univ, Sch Math Sci, Beijing, Peoples R China
MIT, Dept EECS, Cambridge, MA 02139 USA
Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China
Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA
Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Z. Liang; G. Ditzler"	"2018"	"The Impact of an Adversary in a Language Model"	""	"2018 Ieee Symposium Series on Computational Intelligence (Ieee Ssci)"	""	""	""	""	""	"658-665"	""	""	""	""	""	""	""	"The Impact of an Adversary in a Language Model"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000459238800090"	""	""	""	"Neural networks have been quite successful at complex classification tasks. Furthermore, they have the ability to learn information from a large volume of data. Unfortunately, not all of the sources available are secure and there is a possibility that an adversary in the environment has the malicious intention to poison a training dataset to cause the neural network to have a poor generalization error. Therefore, it is important to observe how susceptible a neural network is to the free parameters (i.e., gradient thresholds, hidden layer size, etc.) and the availability of adversarial data. In this work, we study the impact of an adversary for language models with Long Short-Tenn Memory (LSTM) networks and its configurations. We experimented with the Penn Tree Bank (PTR) dataset and adversarial text that was sampled from works in a different era. Our results show that there are several effective ways to poison such an LSTM language model. Furthermore, from our experiments, we are able to provide suggestions about the steps that can be taken to reduce the impact of such attacks."	"Bm0wb
Times Cited:0
Cited References Count:24"	""	"<Go to ISI>://WOS:000459238800090"	""	"Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 87521 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. H. Huang; M. Pan; Y. M. Gong"	"2019"	"Robust Truth Discovery Against Data Poisoning in Mobile Crowdsensing"	""	"2019 Ieee Global Communications Conference (Globecom)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Robust Truth Discovery Against Data Poisoning in Mobile Crowdsensing"	"Ieee Glob Comm Conf"	"2334-0983"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000552238604015"	""	""	""	"Nowadays most mobile devices are equipped with advanced sensors, enabling the measurement of information about surrounding environment or social settings. The ubiquity of mobile devices makes them the perfect platform for massive data collection, which motivates the emergence of mobile crowd-sensing paradigm. However, due to the inherent noisy nature of the sensing process and the limited capability of low-cost commodity sensors, crowdsensed information tends to be less reliable compared with sensing results through dedicated sensing hardware, and multiple crowdsensing sources may conflict with each other. Thus, it is important to resolve conflicts in the collected data and discover the underlying truth. Traditional truth discovery approaches usually estimate the reliability of data sources and predict the truth value based on source reliability. However, recent data poisoning attacks greatly degrade the performance of existing truth discovery algorithms, where attackers aim to maximize the utility loss. In this paper, we investigate the data poisoning attacks on truth discovery and propose a robust approach against such attacks through additional source estimation and source filtering before data aggregation. Based on real-world data, we simulate our approach and evaluate its performance under data poisoning attacks, demonstrating the robustness of our approach."	"Bp4gg
Times Cited:2
Cited References Count:41
IEEE Global Communications Conference"	""	"<Go to ISI>://WOS:000552238604015"	""	"Oklahoma State Univ, Stillwater, OK 74078 USA
Univ Houston, Houston, TX 77204 USA
Univ Texas San Antonio, San Antonio, TX 78249 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Wang; M. L. Qin; M. Q. Chen; C. F. Jia; Y. Ma"	"2018"	"A Learning Evasive Email-Based P2P-Like Botnet"	""	"China Communications"	""	""	"15"	""	"2"	"15-24"	""	""	""	""	"Feb"	""	""	"A Learning Evasive Email-Based P2P-Like Botnet"	"China Commun"	"1673-5447"	"Doi 10.1109/Cc.2018.8300268"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000424738100002"	""	""	"malware
botnet
learning evasion
command and control
command"	"Nowadays, machine learning is widely used in malware detection system as a core component. The machine learning algorithm is designed under the assumption that all datasets follow the same underlying data distribution. But the real-world malware data distribution is not stable and changes with time. By exploiting the knowledge of the machine learning algorithm and malware data concept drift problem, we show a novel learning evasive botnet architecture and a stealthy and secure C&C mechanism. Based on the email communication channel, we construct a stealthy email-based P2P-like botnet that exploit the excellent reputation of email servers and a huge amount of benign email communication in the same channel. The experiment results show horizontal correlation learning algorithm is difficult to separate malicious email traffic from normal email traffic based on the volume features and time-related features with enough confidence. We discuss the malware data concept drift and possible defense strategies."	"Sp. Iss. SI
Fv7ca
Times Cited:7
Cited References Count:26"	""	"<Go to ISI>://WOS:000424738100002"	""	"Nankai Univ, Coll Comp & Control Engn, Tianjin 300350, Peoples R China
Civil Aviat Univ China, Informat Secur Evaluat Ctr Civil Aviat, Tianjin 300300, Peoples R China
Key Lab High Trusted Informat Syst Hebei Prov, Baoding 071002, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. C. He; T. W. Zhang; R. Lee"	"2019"	"Sensitive-Sample Fingerprinting of Deep Neural Networks"	""	"2019 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr 2019)"	""	""	""	""	""	"4724-4732"	""	""	""	""	""	""	""	"Sensitive-Sample Fingerprinting of Deep Neural Networks"	"Proc Cvpr Ieee"	"1063-6919"	"10.1109/Cvpr.2019.00486"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000529484004092"	""	""	""	"Numerous cloud-based services are provided to help customers develop and deploy deep learning applications. When a customer deploys a deep learning model in the cloud and serves it to end-users, it is important to be able to verify that the deployed model has not been tampered with.
In this paper; we propose a novel and practical methodology to verify the integrity of remote deep learning models, with only black-box access to the target models. Specifically, we define Sensitive-Sample fingerprints, which are a small set of human unnoticeable transformed inputs that make the model outputs sensitive to the model's parameters. Even small model changes can be clearly reflected in the model outputs. Experimental results on different types of model integrity attacks show that the proposed approach is both effective and efficient. It can detect model integrity breaches with high accuracy (>99.95%) and guaranteed zero false positives on all evaluated attacks. Meanwhile, it only requires up to 103x fewer model inferences, compared to non-sensitive samples."	"Bo8xd
Times Cited:11
Cited References Count:30
IEEE Conference on Computer Vision and Pattern Recognition"	""	"<Go to ISI>://WOS:000529484004092"	""	"Princeton Univ, Princeton, NJ 08544 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Stutz; M. Hein; B. Schiele"	"2019"	"Disentangling Adversarial Robustness and Generalization"	""	"2019 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr 2019)"	""	""	""	""	""	"6969-6980"	""	""	""	""	""	""	""	"Disentangling Adversarial Robustness and Generalization"	"Proc Cvpr Ieee"	"1063-6919"	"10.1109/Cvpr.2019.00714"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000542649300044"	""	""	""	"Obtaining deep networks that are robust against adversarial examples and generalize well is an open problem. A recent hypothesis [102, 95] even states that both robust and accurate models are impossible, i.e., adversarial robustness and generalization are conflicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that: 1. regular adversarial examples leave the manifold; 2. adversarial examples constrained to the manifold, i.e., on-manifold adversarial examples, exist; 3. on-manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization; 4. regular robustness and generalization are not necessarily contradicting goals. These assumptions imply that both robust and accurate models are possible. However different models (architectures,training strategies etc.) can exhibit different robustness and generalization characteristics. To confirm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on EMNIST [19], Fashion-MNIST [106] and CelebA [58]."	"Bp2ib
Times Cited:21
Cited References Count:109
IEEE Conference on Computer Vision and Pattern Recognition"	""	"<Go to ISI>://WOS:000542649300044"	""	"Saarland Informat Campus, Max Planck Inst Informat, Saarbrucken, Germany
Univ Tubingen, Tubingen, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Gardiner; S. Nagaraja"	"2016"	"On the Security of Machine Learning in Malware C&C Detection: A Survey"	""	"Acm Computing Surveys"	""	""	"49"	""	"3"	"1 - 39"	""	""	""	""	"Dec"	""	""	"On the Security of Machine Learning in Malware C&C Detection: A Survey"	"Acm Comput Surv"	"0360-0300"	"Artn 59
10.1145/3003816"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000391639400019"	""	""	"security
algorithms
performance
command and control channels
botnets
network intrusion
data mining
machine learning
networks"	"One of the main challenges in security today is defending against malware attacks. As trends and anecdotal evidence show, preventing these attacks, regardless of their indiscriminate or targeted nature, has proven difficult: intrusions happen and devices get compromised, even at security-conscious organizations. As a consequence, an alternative line of work has focused on detecting and disrupting the individual steps that follow an initial compromise and are essential for the successful progression of the attack. In particular, several approaches and techniques have been proposed to identify the command and control (C&C) channel that a compromised system establishes to communicate with its controller.
A major oversight of many of these detection techniques is the design's resilience to evasion attempts by the well-motivated attacker. C&C detection techniques make widespread use of a machine learning (ML) component. Therefore, to analyze the evasion resilience of these detection techniques, we first systematize works in the field of C&C detection and then, using existing models from the literature, go on to systematize attacks against the ML components used in these approaches."	"Eh3ah
Times Cited:56
Cited References Count:105"	""	"<Go to ISI>://WOS:000391639400019"	""	"Univ Lancaster, Secur Lancaster Res Ctr, InfoLab21, Lancaster LA1 4WA, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Chen; D. J. Kempton; A. Ahmadzadeh; J. Z. Wen; A. L. Ji; R. A. Angryk"	"2022"	"CGAN-based synthetic multivariate time-series generation: a solution to data scarcity in solar flare forecasting"	""	"Neural Computing & Applications"	""	""	""	""	""	""	""	""	""	""	"May 30"	""	""	"CGAN-based synthetic multivariate time-series generation: a solution to data scarcity in solar flare forecasting"	"Neural Comput Appl"	"0941-0643"	"10.1007/s00521-022-07361-8"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000802897500003"	""	""	"multivariate time series
class imbalance
generative adversarial network
flare forecasting"	"One of the major bottlenecks in refining supervised algorithms is data scarcity. This might be caused by a number of reasons often rooted in extremely expensive and lengthy data collection processes. In natural domains such as Heliophysics, it may take decades for sufficiently large samples for machine learning purposes. Inspired by the massive success of generative adversarial networks (GANs) in generating synthetic images, in this study we employed the conditional GAN (CGAN) on a recently released benchmark dataset tailored for solar flare forecasting. Our goal is to generate synthetic multivariate time-series data that (1) are statistically similar to the real data and (2) improve the performance of flare prediction when used to remedy the scarcity of strong flares. To evaluate the generated samples, first, we used the Kullback-Leibler divergence and adversarial accuracy measures to quantify the similarity between the real and synthetic data in terms of their descriptive statistics. Second, we evaluated the impact of the generated samples by training a predictive model on their descriptive statistics, which resulted in a significant improvement (over 1100% in TSS and 350% in HSS). Third, we used the generated time series to examine their high-dimensional contribution to mitigating the scarcity of the strong flares, which we also observed a significant improvement in terms of TSS (4%, 7%, and 31%) and HSS (75%, 35%, and 72%), compared to oversampling, undersampling, and synthetic oversampling methods, respectively. We believe our findings can open new doors toward more robust and accurate flare forecasting models."	"1q7xq
Times Cited:0
Cited References Count:52"	""	"<Go to ISI>://WOS:000802897500003"	""	"Georgia State Univ, Atlanta, GA 30302 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. B. Wang; S. Y. Zheng; M. K. Song; Q. Wang; A. Rahimpour; H. R. Qi"	"2019"	"advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns"	""	"2019 Ieee/Cvf International Conference on Computer Vision (Iccv 2019)"	""	""	""	""	""	"8340-8349"	""	""	""	""	""	""	""	"advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns"	"Ieee I Conf Comp Vis"	"1550-5499"	"10.1109/Iccv.2019.00843"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000548549203046"	""	""	""	"Person re-identification (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks.
In this paper, we take the first attempt to implement robust physical-world attacks against deep re-ID. We propose a novel attack algorithm, called advPattern, for generating adversarial patterns on clothes, which learns the variations of image pairs across cameras to pull closer the image features from the same camera, while pushing features from different cameras farther. By wearing our crafted "invisible cloak", an adversary can evade person search, or impersonate a target person to fool deep re-ID models in physical world. We evaluate the effectiveness of our transformable patterns on adversaries' clothes with Market1501 and our established PRCS dataset. The experimental results show that the rank-1 accuracy of re-ID models for matching the adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore, the adversary can impersonate a target person with 47.1% rank-1 accuracy and 67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID systems are vulnerable to our physical attacks."	"Bp3pr
Times Cited:8
Cited References Count:38
IEEE International Conference on Computer Vision"	""	"<Go to ISI>://WOS:000548549203046"	""	"Wuhan Univ, Sch Cyber Sci & Engn, Minist Educ, Key Lab Aerosp Informat Secur & Trusted Comp, Wuhan, Peoples R China
Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Shirazi; B. Bezawada; I. Ray; C. Anderson"	"2021"	"Directed adversarial sampling attacks on phishing detection"	""	"Journal of Computer Security"	""	""	"29"	""	"1"	"1-23"	""	""	""	""	""	""	""	"Directed adversarial sampling attacks on phishing detection"	"J Comput Secur"	"0926-227x"	"10.3233/Jcs-191411"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000618528800001"	""	""	"phishing
machine learning
adversarial sampling
classifiers
features"	"Phishing websites trick honest users into believing that they interact with a legitimate website and capture sensitive information, such as user names, passwords, credit card numbers, and other personal information. Machine learning is a promising technique to distinguish between phishing and legitimate websites. However, machine learning approaches are susceptible to adversarial learning attacks where a phishing sample can bypass classifiers. Our experiments on publicly available datasets reveal that the phishing detection mechanisms are vulnerable to adversarial learning attacks. We investigate the robustness of machine learning-based phishing detection in the face of adversarial learning attacks.
We propose a practical approach to simulate such attacks by generating adversarial samples through direct feature manipulation. To enhance the sample's success probability, we describe a clustering approach that guides an attacker to select the best possible phishing samples that can bypass the classifier by appearing as legitimate samples. We define the notion of vulnerability level for each dataset that measures the number of features that can be manipulated and the cost for such manipulation. Further, we clustered phishing samples and showed that some clusters of samples are more likely to exhibit higher vulnerability levels than others. This helps an adversary identify the best candidates of phishing samples to generate adversarial samples at a lower cost. Our finding can be used to refine the dataset and develop better learning models to compensate for the weak samples in the training dataset."	"Qh8nl
Times Cited:1
Cited References Count:26"	""	"<Go to ISI>://WOS:000618528800001"	""	"Colorado State Univ, Ft Collins, CO 80523 USA
Indian Inst Technol Jammu, Jammu, Jammu & Kashmir, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. H. Xie; P. S. Yu"	"2018"	"Next Generation Trustworthy Fraud Detection"	""	"2018 4th Ieee International Conference on Collaboration and Internet Computing (Cic 2018)"	""	""	""	""	""	"279-282"	""	""	""	""	""	""	""	"Next Generation Trustworthy Fraud Detection"	""	""	"10.1109/Cic.2018.00045"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000519942300033"	""	""	""	"Popular web applications, such as e-commerce, social networks and online ad auction, are providing valuable services to web users but have also been plagued by prevalent and diverse frauds. Many detection methodologies have been devised but detection trustworthiness is still one important and yet missing desideratum: a user will not trust a detector that has uncertain accuracy, can malfunction under unexpected situations, or can't explain its behaviors and interal working. Previous efforts mostly focused on detection accuracy, and our goal is to chart a path towards a more comprehensive definition of trustworthy detection, that consists of accuracy, transparency, and proactivity. To achieve the goal, we identify key challenges rooting at the specific settings of the above applications: the evolving nature and unexpectedness of the fraudsters' strategies, the ever-growing large amount of data, and the increasing complexity of effective detectors. We hope spark a large volume of research questions and solutions with respect to the above challenges."	"Bo6gw
Times Cited:1
Cited References Count:48"	""	"<Go to ISI>://WOS:000519942300033"	""	"Lehigh Univ, Bethlehem, PA 18015 USA
Univ Illinois, Chicago, IL USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Caswell; J. D. Gans; N. Generous; C. M. Hudson; E. Merkley; C. Johnson; C. Oehmen; K. Omberg; E. Purvine; K. Taylor; C. L. Ting; M. Wolinsky; G. Xie"	"2019"	"Defending Our Public Biological Databases as a Global Critical Infrastructure"	""	"Frontiers in Bioengineering and Biotechnology"	""	""	"7"	""	""	""	""	""	""	""	"Apr 5"	""	""	"Defending Our Public Biological Databases as a Global Critical Infrastructure"	"Front Bioeng Biotech"	"2296-4185"	"ARTN 58
10.3389/fbioe.2019.00058"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000464602400001"	""	""	"cyberbiosecurity
biosecurity
cybersecurity
biological databases
machine learning
bioeconomy
genome
taxonomy
contamination
similarity
quality
DNA"	"Progress in modern biology is being driven, in part, by the large amounts of freely available data in public resources such as the International Nucleotide Sequence Database Collaboration (INSDC), the world's primary database of biological sequence (and related) information. INSDC and similar databases have dramatically increased the pace of fundamental biological discovery and enabled a host of innovative therapeutic, diagnostic, and forensic applications. However, as high-value, openly shared resources with a high degree of assumed trust, these repositories share compelling similarities to the early days of the Internet. Consequently, as public biological databases continue to increase in size and importance, we expect that they will face the same threats as undefended cyberspace. There is a unique opportunity, before a significant breach and loss of trust occurs, to ensure they evolve with quality and security as a design philosophy rather than costly "retrofitted" mitigations. This Perspective surveys some potential quality assurance and security weaknesses in existing open genomic and proteomic repositories, describes methods to mitigate the likelihood of both intentional and unintentional errors, and offers recommendations for risk mitigation based on lessons learned from cybersecurity."	"Ht5kl
Times Cited:5
Cited References Count:55"	""	"<Go to ISI>://WOS:000464602400001"	""	"Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA
Los Alamos Natl Lab, Biosci Div, Los Alamos, NM USA
Los Alamos Natl Lab, Global Secur Directorate, Los Alamos, NM USA
Sandia Natl Labs, Livermore, CA USA
Pacific Northwest Natl Lab, Richland, WA 99352 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. L. Yao; S. K. Moon; G. J. Bi"	"2017"	"A hybrid machine learning approach for additive manufacturing design feature recommendation"	""	"Rapid Prototyping Journal"	""	""	"23"	""	"6"	"983-997"	""	""	""	""	""	""	""	"A hybrid machine learning approach for additive manufacturing design feature recommendation"	"Rapid Prototyping J"	"1355-2546"	"10.1108/Rpj-03-2016-0041"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000416572700003"	""	""	"additive manufacturing
selective laser melting
design feature recommendation
hybrid machine learning
product
fabrication
knowledge
form"	"Purpose - This paper aims to present a hybrid machine learning algorithm for additive manufacturing (AM) design feature recommendation during the conceptual design phase.
Design/methodology/approach - In the proposed hybrid machine learning algorithm, hierarchical clustering is performed on coded AM design features and target components, resulting in a dendrogram. Existing industrial application examples are used to train a supervised classifier that determines the final sub-cluster within the dendrogram containing the recommended AM design features.
Findings - Through a case study of designing additive manufactured R/C car components, the proposed hybrid machine learning method was proven useful in providing feasible conceptual design solutions for inexperienced designers by recommending appropriate AM design features.
Originality/value - The proposed method helps inexperienced designers who are newly exposed to AM capabilities explore and utilize AM design knowledge computationally."	"Fo2an
Times Cited:33
Cited References Count:32"	""	"<Go to ISI>://WOS:000416572700003"	""	"Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore
Singapore Inst Mfg Technol, Joining Technol Grp, Singapore, Singapore"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Cohen; T. Gluck; Y. Elovici; A. Shabtai"	"2019"	"Security Analysis of Radar Systems"	""	"Cps-Spc'19: Proceedings of the Acm Workshop on Cyber-Physical Systems Security & Privacy"	""	""	""	""	""	"3-14"	""	""	""	""	""	""	""	"Security Analysis of Radar Systems"	""	""	"10.1145/3338499.3357363"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000557870300002"	""	""	"cyber security
radar system
machine learning
adversarial learning
electronic warfare"	"Radar systems are used to detect the existence, location, and trajectory of objects in space using electromagnetic waves. They are mainly used for tracking aircraft, missiles, satellites, and watercraft. In recent years, as technology has evolved, the use of radar systems has increased, along with reliance on their correct and reliable operation. Given this, the reliability and availability of information provided by radar systems is growing in importance. Although the field of cyber security has been continuously evolving, most studies conducted on protecting radar systems have focused on electronic warfare. Radar systems also include a wide variety of components, such as a communications system or SCADA system, which are also vulnerable to cyber attacks. In this study, we present a risk analysis for radar systems. First, we present an in depth review of the existing literature on related topics. Then, we describe the threats we identified and their possible outcome. Finally, we demonstrate the realization of an attack on a radar system created for this task. We conclude by identifying future research directions aiming at protecting advanced radar systems from cyber attacks."	"Bp5qf
Times Cited:2
Cited References Count:59"	""	"<Go to ISI>://WOS:000557870300002"	""	"Ben Gurion Univ Negev, Dept Software & Informat Syst Engn, Beer Sheva, Israel"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. D. Rouhani; M. Samragh; M. Javaheripi; T. Javidi; F. Koushanfar"	"2018"	"DeepFense: Online Accelerated Defense Against Adversarial Deep Learning"	""	"2018 Ieee/Acm International Conference on Computer-Aided Design (Iccad) Digest of Technical Papers"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"DeepFense: Online Accelerated Defense Against Adversarial Deep Learning"	"Iccad-Ieee Acm Int"	"1933-7760"	"10.1145/3240765.3240791"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000494640800132"	""	""	"adversarial attacks
deep learning
model reliability
fpga acceleration
real-time computing"	"Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide -spread usage of DL in critical and time -sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource -constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection."	"Bo1bf
Times Cited:11
Cited References Count:25
ICCAD-IEEE ACM International Conference on Computer-Aided Design"	""	"<Go to ISI>://WOS:000494640800132"	""	"Univ Calif San Diego, La Jolla, CA 92093 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Javaheripi; M. Samragh; B. D. Rouhani; T. Javidi; F. Koushanfar"	"2021"	"CuRTAIL: ChaRacterizing and Thwarting AdversarIal Deep Learning"	""	"Ieee Transactions on Dependable and Secure Computing"	""	""	"18"	""	"2"	"736-752"	""	""	""	""	"Mar-Apr"	""	""	"CuRTAIL: ChaRacterizing and Thwarting AdversarIal Deep Learning"	"Ieee T Depend Secure"	"1545-5971"	"10.1109/Tdsc.2020.3024191"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000628912100017"	""	""	"machine learning
robustness
computational modeling
redundancy
hardware
machine learning algorithms
deep learning
model reliability
adversarial samples
white-box attacks"	"Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. This article introduces CuRTAIL, a novel end-to-end computing framework to characterize and thwart potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model. We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. This, in turn, ensures the effectiveness of the defense in the face of generic attacks. We evaluate the robustness of our proposed methodology against the state-of-the-art adaptive attacks in a white-box setting considering that the adversary knows everything about the victim model and its defenders. Extensive evaluations for analyzing MNIST, CIFAR10, and ImageNet data corroborate the effectiveness of CuRTAIL framework against adversarial samples. The computations in each modular redundancy can be performed independently of the other redundancy modules. As such, CuRTAIL detection algorithm can be completely parallelized among multiple hardware settings to achieve maximum throughput. We further provide an open-source Application Programming Interface (API) to facilitate the adoption of the proposed framework for various applications."	"Qw8qe
Times Cited:3
Cited References Count:48"	""	"<Go to ISI>://WOS:000628912100017"	""	"Univ Calif San Diego, La Jolla, CA 92093 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. C. Tian; K. X. Pei; S. Jana; B. Ray"	"2018"	"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars"	""	"Proceedings 2018 Ieee/Acm 40th International Conference on Software Engineering (Icse)"	""	""	""	""	""	"303-314"	""	""	""	""	""	""	""	"DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars"	""	""	"10.1145/3180155.3180220"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000454843300035"	""	""	"deep learning
testing
self-driving cars
deep neural networks
autonomous vehicle
neuron coverage"	"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.
However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.
In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge."	"Bl7cs
Times Cited:373
Cited References Count:83"	""	"<Go to ISI>://WOS:000454843300035"	""	"Univ Virginia, Charlottesville, VA 22903 USA
Columbia Univ, New York, NY 10027 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. K. Sah; H. Ghasemzadeh"	"2019"	"Adar: Adversarial Activity Recognition in Wearables"	""	"2019 Ieee/Acm International Conference on Computer-Aided Design (Iccad)"	""	""	""	""	""	"1-8"	""	""	""	""	""	""	""	"Adar: Adversarial Activity Recognition in Wearables"	"Iccad-Ieee Acm Int"	"1933-7760"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000524676400082"	""	""	""	"Recent advances in machine learning and deep neural networks have led to the realization of many important applications in the area of personalized medicine. Whether it is detecting activities of daily living or analyzing images for cancerous cells, machine learning algorithms have become the dominant choice for such emerging applications. In particular, the state-of-the-art algorithms used for human activity recognition (HAR) using wearable inertial sensors utilize machine learning algorithms to detect health events and to make predictions from sensor data. Currently, however, there remains a gap in research on whether or not and how activity recognition algorithms may become the subject of adversarial attacks. In this paper, we take the first strides on (1) investigating methods of generating adversarial example in the context of HAR systems; (2) studying the vulnerability of activity recognition models to adversarial examples in feature and signal domain; and (3) investigating the effects of adversarial training on HAR systems. We introduce Adar1, a novel computational framework for optimization-driven creation of adversarial examples in sensor-based activity recognition systems. Through extensive analysis based on real sensor data collected with human subjects, we found that simple evasion attacks are able to decrease the accuracy of a deep neural network from 95.1% to 3.4% and from 93.1% to 16.8% in the case of a convolutional neural network. With adversarial training, the robustness of the deep neural network increased on the adversarial examples by 49.1% in the worst case while the accuracy on clean samples decreased by 13.2%."	"Bo7kj
Times Cited:0
Cited References Count:28
ICCAD-IEEE ACM International Conference on Computer-Aided Design"	""	"<Go to ISI>://WOS:000524676400082"	""	"Washington State Univ, Pullman, WA 99164 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Zhou; J. X. Ren; D. J. Dou; R. M. Jin; J. Y. Zheng; K. Lee"	"2020"	"Robust Meta Network Embedding against Adversarial Attacks"	""	"20th Ieee International Conference on Data Mining (Icdm 2020)"	""	""	""	""	""	"1448-1453"	""	""	""	""	""	""	""	"Robust Meta Network Embedding against Adversarial Attacks"	"Ieee Data Mining"	"1550-4786"	"10.1109/Icdm50108.2020.00192"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000630177700180"	""	""	"meta learning
multiple network embedding
adversarial attacks
dynamic adversarial training"	"Recent studies have shown that graph mining models are vulnerable to adversarial attacks. This paper proposes a robust meta network embedding framework, RoMNE, which improves the robustness of multiple network embedding on adversarial noisy networks while preserving the utility on original clean ones. First, we propose a generic meta learning based multiple network embedding model that can quickly adapt it to new embedding tasks on a variety of network data with only a small number of parameter and training updates. Second, Gumbel estimator and Gaussian smoothing techniques are introduced to implement differentiable approximation for optimizing non-differential objective of effective adversarial attacks. Last but not least, the adversarial attack and defense models are integrated into a dynamic adversarial training model. The competition of two models helps the latter be robust to adversarial attacks."	"Br0ql
Times Cited:4
Cited References Count:63
IEEE International Conference on Data Mining"	""	"<Go to ISI>://WOS:000630177700180"	""	"Auburn Univ, Auburn, AL 36849 USA
Univ Oregon, Eugene, OR 97403 USA
Kent State Univ, Kent, OH 44242 USA
Louisiana State Univ, Baton Rouge, LA 70803 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. T. Zhang; C. X. Tian; Y. L. Li; L. Su; N. Yang; W. X. Zhao; J. Gao"	"2021"	"Data Poisoning Attack against Recommender System Using Incomplete and Perturbed Data"	""	"Kdd '21: Proceedings of the 27th Acm Sigkdd Conference on Knowledge Discovery & Data Mining"	""	""	""	""	""	"2154-2164"	""	""	""	""	""	""	""	"Data Poisoning Attack against Recommender System Using Incomplete and Perturbed Data"	""	""	"10.1145/3447548.3467233"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000749556802020"	""	""	"adversarial learning
recommender system
data poisoning"	"Recent studies reveal that recommender systems are vulnerable to data poisoning attack due to their openness nature. In data poisoning attack, the attacker typically recruits a group of controlled users to inject well-crafted user-item interaction data into the recommendation model's training set to modify the model parameters as desired. Thus, existing attack approaches usually rewire kill access to the training data to infer items' characteristics and craft the fake interactions for controlled users. However, such attack approaches may not be feasible in practice due to the attacker's limited data collection capability and the restricted access to the training data, which sometiines are even perturbed by the privacy preserving mechanism of the service providers. Such design-reality gap may cause failure of attacks. In this paper, we fill the gap by proposing two novel adversarial attack approaches to handle the incompleteness and perturbations in user-item interaction data. First, we propose a bi-level optimization framework that incorporates a probabilistic generative model to find the users and items whose interaction data is sufficient and has riot been significantly perturbed, and leverage these users and items' data to craft fake user-item interactions. Moreover, we reverse the learning process of recommendation models and develop a simple yet effective approach that can incorporate context-specific heuristic rules to handle data incompleteness and perturbations. Extensive experiments on two datasets against three representative recommendation models show that the proposed approaches can achieve better attack performance than existing approaches."	"Bs6lu
Times Cited:0
Cited References Count:29"	""	"<Go to ISI>://WOS:000749556802020"	""	"Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA
Renmin Univ China, Sch Informat, Beijing, Peoples R China
Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing, Peoples R China
Beijing Key Lab Big Data Management & Anal Method, Beijing, Peoples R China
SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY USA
Alibaba Grp, Hangzhou, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"K. Eykholt; I. Evtimov; E. Fernandes; B. Li; A. Rahmati; C. W. Xiao; A. Prakash; T. Kohno; D. Song"	"2018"	"Robust Physical-World Attacks on Deep Learning Visual Classification"	""	"2018 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr)"	""	""	""	""	""	"1625-1634"	""	""	""	""	""	""	""	"Robust Physical-World Attacks on Deep Learning Visual Classification"	"Proc Cvpr Ieee"	"1063-6919"	"10.1109/Cvpr.2018.00175"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000457843601078"	""	""	""	"Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier."	"Bl9nz
Times Cited:363
Cited References Count:39
IEEE Conference on Computer Vision and Pattern Recognition"	""	"<Go to ISI>://WOS:000457843601078"	""	"Univ Michigan, Ann Arbor, MI 48109 USA
Univ Washington, Seattle, WA 98195 USA
Univ Calif Berkeley, Berkeley, CA 94720 USA
Samsung Res Amer, Mountain View, CA USA
SUNY Stony Brook, Stony Brook, NY 11794 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. X. Wang; F. Wu; Y. H. Long; L. Rimanic; C. Zhang; B. Li"	"2021"	"DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"2146-2168"	""	""	""	""	""	""	""	"DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation"	""	""	"10.1145/3460120.3484579"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478302011"	""	""	"differential privacy
generative models
gradient compression"	"Recent success of deep neural networks (DNNs) hinges on the availability of large-scale dataset; however, training on such dataset often poses privacy risks for sensitive training information. In this paper, we aim to explore the power of generative models and gradient sparsity, and propose a scalable privacy-preserving generative model DATALENS, which is able to generate synthetic data in a differentially private (DP) way given sensitive input data. Thus, it is possible to train models for different down-stream tasks with the generated data while protecting the private information. In particular, we leverage the generative adversarial networks (GAN) and PATE framework to train multiple discriminators as "teacher" models, allowing them to vote with their gradient vectors to guarantee privacy.
Comparing with the standard PATE privacy preserving framework which allows teachers to vote on one-dimensional predictions, voting on the high dimensional gradient vectors is challenging in terms of privacy preservation. As dimension reduction techniques are required, we need to navigate a delicate tradeoff space between (1) the improvement of privacy preservation and (2) the slowdown of SGD convergence. To tackle this, we propose a novel dimension compression and aggregation approach ToPAGG, which combines top-k dimension compression with a corresponding noise injection mechanism. We theoretically prove that the DATALENS framework guarantees differential privacy for its generated data, and provide a novel analysis on its convergence to illustrate such a tradeoff on privacy and convergence rate, which requires nontrivial analysis as it requires a joint analysis on gradient compression, coordinate-wise gradient clipping, and DP mechanism. To demonstrate the practical usage of DATALENS, we conduct extensive experiments on diverse datasets including MNIST, Fashion-MNIST, and high dimensional CelebA and Place365 datasets. We show that DATALENS significantly outperforms other baseline differentially private data generative models. Our code is publicly available at https://github.com/AI-secure/DataLens."	"Bs7xr
Times Cited:0
Cited References Count:63"	""	"<Go to ISI>://WOS:000768478302011"	""	"Univ Illinois, Champaign, IL 61820 USA
Swiss Fed Inst Technol, Zurich, Switzerland"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. T. Tran; V. H. Tran; N. B. Nguyen; T. K. Nguyen; N. M. Cheung"	"2021"	"On Data Augmentation for GAN Training"	""	"IEEE Trans Image Process"	""	""	"30"	""	""	"1882-1897"	""	""	""	"2021/01/12"	""	""	""	"On Data Augmentation for GAN Training"	""	"1941-0042 (Electronic)
1057-7149 (Linking)"	"10.1109/TIP.2021.3049346"	""	""	""	""	""	""	""	""	""	""	""	"33428571"	""	""	""	"Recent successes in Generative Adversarial Networks (GAN) have affirmed the importance of using more data in GAN training. Yet it is expensive to collect data in many domains such as medical applications. Data Augmentation (DA) has been applied in these applications. In this work, we first argue that the classical DA approach could mislead the generator to learn the distribution of the augmented data, which could be different from that of the original data. We then propose a principled framework, termed Data Augmentation Optimized for GAN (DAG), to enable the use of augmented data in GAN training to improve the learning of the original distribution. We provide theoretical analysis to show that using our proposed DAG aligns with the original GAN in minimizing the Jensen-Shannon (JS) divergence between the original distribution and model distribution. Importantly, the proposed DAG effectively leverages the augmented data to improve the learning of discriminator and generator. We conduct experiments to apply DAG to different GAN models: unconditional GAN, conditional GAN, self-supervised GAN and CycleGAN using datasets of natural images and medical images. The results show that DAG achieves consistent and considerable improvements across these models. Furthermore, when DAG is used in some GAN models, the system establishes state-of-the-art Frechet Inception Distance (FID) scores. Our code is available (https://github.com/tntrung/dag-gans)."	"Tran, Ngoc-Trung
Tran, Viet-Hung
Nguyen, Ngoc-Bao
Nguyen, Trung-Kien
Cheung, Ngai-Man
eng
IEEE Trans Image Process. 2021;30:1882-1897. doi: 10.1109/TIP.2021.3049346. Epub 2021 Jan 20."	""	"https://www.ncbi.nlm.nih.gov/pubmed/33428571"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"K. Yuan; D. Tang; X. J. Liao; X. F. Wang; X. Feng; Y. Chen; M. H. Sun; H. R. Lu; K. H. Zhang"	"2019"	"Stealthy Porn: Understanding Real-World Adversarial Images for Illicit Online Promotion"	""	"2019 Ieee Symposium on Security and Privacy (Sp 2019)"	""	""	""	""	""	"952-966"	""	""	""	""	""	""	""	"Stealthy Porn: Understanding Real-World Adversarial Images for Illicit Online Promotion"	"P Ieee S Secur Priv"	"1081-6011"	"10.1109/Sp.2019.00032"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000510006100057"	""	""	""	"Recent years have witnessed the rapid progress in deep learning (DL), which also brings their potential weaknesses to the spotlights of security and machine learning studies. With important discoveries made by adversarial learning research, surprisingly little attention, however, has been paid to the real-world adversarial techniques deployed by the cybercriminal to evade image-based detection. Unlike the adversarial examples that induce misclassification using nearly imperceivable perturbation, real-world adversarial images tend to be less optimal yet equally effective. As a first step to understand the threat, we report in the paper a study on adversarial promotional porn images (APPIs) that are extensively used in underground advertising. We show that the adversary today's strategically constructs the APPIs to evade explicit content detection while still preserving their sexual appeal, even though the distortions and noise introduced are clearly observable to humans.
To understand such real-world adversarial images and the underground business behind them, we develop a novel DL-based methodology called Malena, which focuses on the regions of an image where sexual content is least obfuscated and therefore visible to the target audience of a promotion. Using this technique, we have discovered over 4,000 APPIs from 4,042,690 images crawled from popular social media, and further brought to light the unique techniques they use to evade popular explicit content detectors (e.g., Google Cloud Vision API, Yahoo Open NSFW model), and the reason that these techniques work. Also studied are the ecosystem of such illicit promotions, including the obfuscated contacts advertised through those images, compromised accounts used to disseminate them, and large APPI campaigns involving thousands of images. Another interesting finding is the apparent attempt made by cybercriminals to steal others' images for their advertising. The study highlights the importance of the research on real-world adversarial learning and makes the first step towards mitigating the threats it poses."	"Bo3ed
Times Cited:11
Cited References Count:41
IEEE Symposium on Security and Privacy"	""	"<Go to ISI>://WOS:000510006100057"	""	"Indiana Univ, Bloomington, IN 47405 USA
Chinese Univ Hong Kong, Hong Kong, Peoples R China
Chinese Acad Sci, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"W. B. Jiang; H. W. Li; S. Liu; Y. Z. Ren; M. He"	"2019"	"A Flexible Poisoning Attack Against Machine Learning"	""	"Icc 2019 - 2019 Ieee International Conference on Communications (Icc)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"A Flexible Poisoning Attack Against Machine Learning"	"Ieee Icc"	"1550-3607"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000492038802061"	""	""	"poisoning attack
machine learning"	"Recent years have witnessed tremendous academic efforts and industry growth in machine learning. The security of machine learning has become increasingly prominent. Poisoning attack is one of the most relevant security threats to machine learning which focuses on polluting the training data that machine learning needs during the training process. Specifically, the attacker blends crafted poisoning samples into training data in order to make the learned model beneficial to him. To the best of our knowledge, existing researches about poisoning attack focused on either integrity attack or availability attack, which did not unify these two attacks together. Aside from that, from the attacker's perspective, attacker's strategy is not flexible enough. Finally, existing proposals only concentrated on increasing the test error of the learned model but ignored the importance of the concealment of attack. To overcome these issues, we firstly present a thorough adversarial model for poisoning attack in which attacker's strategy is defined from two aspects, i.e., the effect of attack and the concealment of attack. Then we unify integrity attack and availability attack together in similar formulations. Furthermore, in order to enhance flexibility, a tradeoff parameter is inserted into attacker's objective function which means the attacker can balance the attraction of effect against the requirement of concealment. Finally, as examples, extensive experiments are conducted on linear regression and logistic regression to demonstrate the effectiveness of attack."	"Bo0ma
Times Cited:2
Cited References Count:25
IEEE International Conference on Communications"	""	"<Go to ISI>://WOS:000492038802061"	""	"Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China
CETC Big Data Res Inst Co Ltd, Guiyang 550022, Guizhou, Peoples R China
Sci & Technol Commun Secur Lab, Chengdu 610041, Sichuan, Peoples R China
Fortinet Technol Canada ULC, Ottawa, ON, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"R. X. Chen; J. Y. Chen; H. B. Zheng; Q. Xuan; Z. Y. Ming; W. R. Jiang; C. Cui"	"2022"	"Salient feature extractor for adversarial defense on deep neural networks"	""	"Information Sciences"	""	""	"600"	""	""	"118-143"	""	""	""	""	"Jul"	""	""	"Salient feature extractor for adversarial defense on deep neural networks"	"Inform Sciences"	"0020-0255"	"10.1016/j.ins.2022.03.056"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000793056000007"	""	""	"adversarial attack
defense
generative adversarial network
salient feature"	"Recent years have witnessed unprecedented success achieved by deep learning models in the field of computer vision. However, their vulnerability towards carefully-crafted adversarial examples has also attracted the increasing attention of researchers. Motivated by the observation that adversarial examples are due to the non-robust feature learned from the original dataset by models, we propose the concepts of salient feature (SF) and trivial feature (TF). The former represents the class-related feature, while the latter is usually adopted to mislead the model. We extract these two features with coupled generative adversarial network model and put forward a novel detection and defense method named salient feature extractor (SFE) to defend against adversarial attacks. Concretely, detection is realized by separating and comparing the difference between SF and TF of the input. At the same time, correct labels are obtained by re-identifying SF. Extensive experiments are carried out on MNIST, CIFAR-10, and ImageNet datasets where SFE shows superior results in effectiveness and efficiency compared with state-of-the-art baselines. Furthermore, we provide an interpretable understanding of the defense and detection process. The code of SFE could be downloaded from ( https://github.com/haibinzheng/SFE). (c) 2022 Elsevier Inc. All rights reserved."	"1c3xl
Times Cited:0
Cited References Count:48"	""	"<Go to ISI>://WOS:000793056000007"	""	"Zhejiang Univ Technol, Coll Informat Engn, Hangzhou, Peoples R China
Zhejiang Univ Technol, Inst Cyberspace Secur, Hangzhou, Peoples R China
Zhejiang Univ City Coll, Hangzhou, Peoples R China
Hangzhou Dianzi Univ, Coll Comp Sci, Hangzhou, Peoples R China
Big Data & Cyber Secur Res Inst, Zhejiang Police Coll, Hangzhou, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Zhao; H. Zhu; R. G. Liang; Q. T. Shen; S. Z. Zhang; K. Chen"	"2019"	"Seeing isn't Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors"	""	"Proceedings of the 2019 Acm Sigsac Conference on Computer and Communications Security (Ccs'19)"	""	""	""	""	""	"1989-2004"	""	""	""	""	""	""	""	"Seeing isn't Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors"	""	""	"10.1145/3319535.3354259"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000509760700120"	""	""	"physical adversarial attack
object detectors
neural networks"	"Recently Adversarial Examples (AEs) that deceive deep learning models have been a topic of intense research interest. Compared with the AEs in the digital space, the physical adversarial attack is considered as a more severe threat to the applications like face recognition in authentication, objection detection in autonomous driving cars, etc. In particular, deceiving the object detectors practically, is more challenging since the relative position between the object and the detector may keep changing. Existing works attacking object detectors are still very limited in various scenarios, e.g., varying distance and angles, etc.
In this paper, we presented systematic solutions to build robust and practical AEs against real world object detectors. Particularly, for Hiding Attack (HA), we proposed the feature-interference reinforcement (FIR) method and the enhanced realistic constraints generation (ERG) to enhance robustness, and for Appearing Attack (AA), we proposed the nested-AE, which combines two AEs together to attack object detectors in both long and short distance. We also designed diverse styles of AEs to make AA more surreptitious. Evaluation results show that our AEs can attack the state-of-the-art real-time object detectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4% with varying distance from 1m to 25m and angles from -60 degrees to 60 degrees(1). Our AEs are also demonstrated to be highly transferable, capable of attacking another three state-of-the-art black-box models with high success rate."	"Bo2zy
Times Cited:26
Cited References Count:46"	""	"<Go to ISI>://WOS:000509760700120"	""	"Chinese Acad Sci, Inst Informat Engn, SKLOIS, Beijing, Peoples R China
Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China
Boston Univ, Dept Comp Sci, Metropolitan Coll, Boston, MA 02215 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Mahloujifar; D. I. Diochnos; M. Mahmoody"	"2020"	"Learning under p-tampering poisoning attacks"	""	"Annals of Mathematics and Artificial Intelligence"	""	""	"88"	""	"7"	"759-792"	""	""	""	""	"Jul"	""	""	"Learning under p-tampering poisoning attacks"	"Ann Math Artif Intel"	"1012-2443"	"10.1007/s10472-019-09675-1"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000499964200001"	""	""	"poisoning attacks
adversarial machine learning
p-tampering attacks
learnability"	"Recently, Mahloujifar and Mahmoody (Theory of Cryptography Conference'17) studied attacks against learning algorithms using a special case of Valiant's malicious noise, called p-tampering, in which the adversary gets to change any training example with independent probability p but is limited to only choose 'adversarial' examples with correct labels. They obtained p-tampering attacks that increase the error probability in the so called 'targeted' poisoning model in which the adversary's goal is to increase the loss of the trained hypothesis over a particular test example. At the heart of their attack was an efficient algorithm to bias the expected value of any bounded real-output function through p-tampering. In this work, we present new biasing attacks for increasing the expected value of bounded real-valued functions. Our improved biasing attacks, directly imply improved p-tampering attacks against learners in the targeted poisoning model. As a bonus, our attacks come with considerably simpler analysis. We also study the possibility of PAC learning under p-tampering attacks in the non-targeted (aka indiscriminate) setting where the adversary's goal is to increase the risk of the generated hypothesis (for a random test example). We show that PAC learning is possible under p-tampering poisoning attacks essentially whenever it is possible in the realizable setting without the attacks. We further show that PAC learning under 'no-mistake' adversarial noise is not possible, if the adversary could choose the (still limited to only p fraction of) tampered examples that she substitutes with adversarially chosen ones. Our formal model for such 'bounded-budget' tampering attackers is inspired by the notions of adaptive corruption in cryptography."	"Sp. Iss. SI
Mf9jp
Times Cited:2
Cited References Count:58"	""	"<Go to ISI>://WOS:000499964200001"	""	"Univ Virginia, Charlottesville, VA 22903 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. X. Zhang; Z. C. Ren; Z. H. Wang; P. J. Ren; Z. M. Chen; P. F. Hu; Y. Zhang"	"2021"	"Membership Inference Attacks Against Recommender Systems"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"864-879"	""	""	""	""	""	""	""	"Membership Inference Attacks Against Recommender Systems"	""	""	"10.1145/3460120.3484770"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478300054"	""	""	"membership inference attack
recommender system
membership leakage"	"Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems.
In this paper, we make the first attempt on quantifying the privacy leakage of reconunender systems through the lens of membership inference. In contrast with traditional membership inference against machine learning classifiers, our attack faces two main differences. First, our attack is on the user-level but not on the data sample-level. Second, the adversary can only observe the ordered recommended items from a recommender system instead of prediction results in the form of posterior probabilities. To address the above challenges, we propose a novel method by representing users from relevant items. Moreover, a shadow recommender is established to derive the labeled training data for training the attack model. Extensive experimental results show that our attack framework achieves a strong performance. In addition, we design a defense mechanism to effectively mitigate the membership inference threat of recommender systems.(1)"	"Bs7xr
Times Cited:0
Cited References Count:52"	""	"<Go to ISI>://WOS:000768478300054"	""	"Shandong Univ, Jinan, Peoples R China
CISPA Helmholtz Ctr Informat Secur, Saarbrucken, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. H. Fang; N. Z. Q. Gong; J. Liu"	"2020"	"Influence Function based Data Poisoning Attacks to Top-N Recommender Systems"	""	"Web Conference 2020: Proceedings of the World Wide Web Conference (Www 2020)"	""	""	""	""	""	"3019-3025"	""	""	""	""	""	""	""	"Influence Function based Data Poisoning Attacks to Top-N Recommender Systems"	""	""	"10.1145/3366423.3380072"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000626273303014"	""	""	"adversarial recommender systems
data poisoning attacks
adversarial machine learning"	"Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-N items that match the best with a user's preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods."	"Bq9oc
Times Cited:25
Cited References Count:43"	""	"<Go to ISI>://WOS:000626273303014"	""	"Iowa State Univ, Ames, IA 50011 USA
Duke Univ, Durham, NC 27706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Atzeni; F. Diaz; A. Marcelli; A. Sanchez; G. Squillero; A. Tonda"	"2018"	"Countering Android Malware: A Scalable Semi-Supervised Approach for Family-Signature Generation"	""	"Ieee Access"	""	""	"6"	""	""	"59540-59556"	""	""	""	""	""	""	""	"Countering Android Malware: A Scalable Semi-Supervised Approach for Family-Signature Generation"	"Ieee Access"	"2169-3536"	"10.1109/Access.2018.2874502"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000449552600001"	""	""	"semi-supervised learning
clustering
android
malware
automatic signature generation
classification
behavior"	"Reducing the effort required by humans in countering malware is of utmost practical value. We describe a scalable, semi-supervised framework to dig into massive data sets of Android applications and identify new malware families. Until 2010, the industrial standard for the detection of malicious applications has been mainly based on signatures; as each tiny alteration in malware makes them ineffective, new signatures are frequently created - a task that requires a considerable amount of time and resources from skilled experts. The framework we propose is able to automatically cluster applications in families and suggest formal rules for identifying them with 100% recall and quite high precision. The families are used either to safely extend experts' knowledge on new samples or to reduce the number of applications requiring thorough analyses. We demonstrated the effectiveness and the scalability of the approach running experiments on a database of 1.5 million Android applications. In 2018, the framework has been successfully deployed on Koodous, a collaborative anti-malware platform."	"Gz6nb
Times Cited:11
Cited References Count:62"	""	"<Go to ISI>://WOS:000449552600001"	""	"Politecn Torino, Dept Control & Comp Engn DAWN, I-10129 Turin, Italy
Hispasec Sistemas SL, Malaga 29001, Spain
INRA, UMR GMPA 782, F-78850 Thiverval Grignon, France"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. Sasaki; S. Hidano; T. Uchibayashi; T. Suganuma; M. Hiji; S. Kiyomoto"	"2019"	"On Embedding Backdoor in Malware Detectors Using Machine Learning"	""	"2019 17th International Conference on Privacy, Security and Trust (Pst)"	""	""	""	""	""	"300-304"	""	""	""	""	""	""	""	"On Embedding Backdoor in Malware Detectors Using Machine Learning"	"Ann Conf Priv Secur"	"1712-364x"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000561703600038"	""	""	"malware detection
poisoning attack
backdoor
machine learning"	"Researching for malware detection using machine learning is becoming active. However, conventional detection techniques do not consider the impact of attacks on machine learning, which has become complicated in recent years. In this research, we focus on data poisoning attack, which is one of the typical attacks on machine learning, and aim to clarify the influence of attacks on malware detection technology. Data poisoning attack is an attack method that intentionally manipulates the predicted result of a learned model by injecting poisoning data into training data, and by applying this, it is possible to embed a backdoor that induces mis-prediction of only specific input data. In this paper, we first propose an attack framework for backdoor embedding that prevents detection of only specific types of malware by data poisoning attack. Next, we will describe a method to generate poisoning data efficiently while avoiding attack detection by solving the optimization problem. Furthermore, we take malware detection technology using logistic regression and show the effectiveness of the our method through evaluation experiments using two datasets."	"Bp7dz
Times Cited:0
Cited References Count:16
Annual Conference on Privacy Security and Trust-PST"	""	"<Go to ISI>://WOS:000561703600038"	""	"Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi, Japan
KDDI Res Inc, Fujimino, Japan
Tohoku Univ, Cybersci Ctr, Sendai, Miyagi, Japan
Tohoku Univ, Grad Sch Econ & Management, Sendai, Miyagi, Japan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Wiebe; R. S. S. Kumar"	"2018"	"Hardening quantum machine learning against adversaries"	""	"New Journal of Physics"	""	""	"20"	""	""	""	""	""	""	""	"Dec 21"	""	""	"Hardening quantum machine learning against adversaries"	"New J Phys"	"1367-2630"	"ARTN 123019
10.1088/1367-2630/aae71a"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000454265100001"	""	""	"quantum computing
quantum machine learning
quantum algorithms
hamiltonian simulation"	"Security of machine learning has begun to become a serious issue for present day applications. An important question remaining is whether emerging quantum technologies will help or hinder the security of machine learning. Here we discuss a number of ways that quantum information can be used to help make quantum classifiers more secure or private. In particular, we demonstrate a form of robust principal component analysis that, under some circumstances, can provide an exponential speedup relative to robust methods used at present. To demonstrate this approach we introduce a linear combinations of unitaries Hamiltonian simulation method that we show functions when given an imprecise Hamiltonian oracle, which maybe of independent interest. We also introduce a new quantum approach for bagging and boosting that can use quantum superposition over the classifiers or splits of the training set to aggragate over many more models than would be possible classically. Finally, we provide a private form of k-means clustering that can be used to prevent an all powerful adversary from learning more than a small fraction of a bit from any user. These examples show the role that quantum technologies can play in the security of ML and vice versa. This illustrates that quantum computing can provide useful advantages to machine learning apart from speedups."	"Hf5ie
Times Cited:6
Cited References Count:61"	""	"<Go to ISI>://WOS:000454265100001"	""	"Microsoft Corp, One Microsoft Way, Redmond, WA 98052 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Yao; A. S. Rakin; D. L. Fan"	"2020"	"DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips"	""	"Proceedings of the 29th Usenix Security Symposium"	""	""	"abs/2003.13746"	""	""	"1463-1480"	""	""	""	""	""	""	""	"DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000668146200083"	""	""	"hardware"	"Security of machine learning is increasingly becoming a major concern due to the ubiquitous deployment of deep learning in many security-sensitive domains. Many prior studies have shown external attacks such as adversarial examples that tamper the integrity of DNNs using maliciously crafted inputs. However, the security implication of internal threats (i.e., hardware vulnerabilities) to DNN models has not yet been well understood.
In this paper, we demonstrate the first hardware-based attack on quantized deep neural networks-DeepHammer-that deterministically induces bit flips in model weights to compromise DNN inference by exploiting the rowhammer vulnerability. DeepHammer performs an aggressive bit search in the DNN model to identify the most vulnerable weight bits that are flippable under system constraints. To trigger deterministic bit flips across multiple pages within a reasonable amount of time, we develop novel system-level techniques that enable fast deployment of victim pages, memory-efficient rowhammering and precise flipping of targeted bits. DeepHammer can deliberately degrade the inference accuracy of the victim DNN system to a level that is only as good as random guess, thus completely depleting the intelligence of targeted DNN systems. We systematically demonstrate our attacks on real systems against 11 DNN architectures with 4 datasets corresponding to different application domains. Our evaluation shows that DeepHammer is able to successfully tamper DNN inference behavior at run-time within a few minutes. We further discuss several mitigation techniques from both algorithm and system levels to protect DNNs against such attacks. Our work highlights the need to incorporate security mechanisms in future machine learning systems to enhance the robustness of DNN against hardware-based deterministic fault injections."	"Br7kc
Times Cited:9
Cited References Count:69"	""	"<Go to ISI>://WOS:000668146200083"	""	"Univ Cent Florida, Orlando, FL 32816 USA
Arizona State Univ, Tempe, AZ 85287 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. T. Tran; V. H. Tran; N. B. Nguyen; L. X. Yang; N. M. Cheung"	"2019"	"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game"	""	"Advances in Neural Information Processing Systems 32 (Nips 2019)"	""	""	"32"	""	""	""	""	""	""	""	""	""	""	"Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000535866904084"	""	""	""	"Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet 32 x 32 and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: https //github.com/tntrung/msgan"	"Bp0qe
Times Cited:12
Cited References Count:54
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000535866904084"	""	"Singapore Univ Technol & Design SUTD, Singapore, Singapore"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Song; X. Yang; Z. Xu; I. King"	"2022"	"Graph-Based Semi-Supervised Learning: A Comprehensive Review"	""	"IEEE Trans Neural Netw Learn Syst"	""	""	"PP"	""	""	""	""	""	""	"2022/03/19"	"Mar 18"	""	""	"Graph-Based Semi-Supervised Learning: A Comprehensive Review"	""	"2162-2388 (Electronic)
2162-237X (Linking)"	"10.1109/TNNLS.2022.3155478"	""	""	""	""	""	""	""	""	""	""	""	"35302941"	""	""	""	"Semi-supervised learning (SSL) has tremendous value in practice due to the utilization of both labeled and unlabelled data. An essential class of SSL methods, referred to as graph-based semi-supervised learning (GSSL) methods in the literature, is to first represent each sample as a node in an affinity graph, and then, the label information of unlabeled samples can be inferred based on the structure of the constructed graph. GSSL methods have demonstrated their advantages in various domains due to their uniqueness of structure, the universality of applications, and their scalability to large-scale data. Focusing on GSSL methods only, this work aims to provide both researchers and practitioners with a solid and systematic understanding of relevant advances as well as the underlying connections among them. The concentration on one class of SSL makes this article distinct from recent surveys that cover a more general and broader picture of SSL methods yet often neglect the fundamental understanding of GSSL methods. In particular, a significant contribution of this article lies in a newly generalized taxonomy for GSSL under the unified framework, with the most up-to-date references and valuable resources such as codes, datasets, and applications. Furthermore, we present several potential research directions as future work with our insights into this rapidly growing field."	"Song, Zixing
Yang, Xiangli
Xu, Zenglin
King, Irwin
eng
IEEE Trans Neural Netw Learn Syst. 2022 Mar 18;PP. doi: 10.1109/TNNLS.2022.3155478."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35302941"	""	""	""	""	""	""	""	""	""	""
"Journal Article"	"N. Carlini"	"2021"	"Poisoning the Unlabeled Dataset of Semi-Supervised Learning"	""	"Proceedings of the 30th Usenix Security Symposium"	""	""	"abs/2105.01622"	""	""	"1577-1592"	""	""	""	""	""	""	""	"Poisoning the Unlabeled Dataset of Semi-Supervised Learning"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000722006801040"	""	""	""	"Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data.
We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods.
We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack."	"Bs4qp
Times Cited:0
Cited References Count:72"	""	"<Go to ISI>://WOS:000722006801040"	""	"Google, Mountain View, CA 94043 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. Kotinas; N. Fakotakis"	"2018"	"Text Analysis for Decision Making under Adversarial Environments"	""	"10th Hellenic Conference on Artificial Intelligence (Setn 2018)"	""	""	""	""	""	""	""	""	""	""	""	""	""	"Text Analysis for Decision Making under Adversarial Environments"	""	""	"10.1145/3200947.3201018"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000509978600039"	""	""	"adversarial learning
decision support
text analytics
sentiment analysis
social media"	"Sentiment analysis and other practices for text analytics on social media rely on publicly available and editable collections of data for training and evaluation. These data collections are subject to poisoning and data contamination attacks by adversaries having an interest in misleading the results of the performed analysis. We present the problem of adversarial text mining with a focus on decision making and we suggest cross-discipline, cross-application and cross-model strategies for more robust analyses. Our approach is practitioner-centric and is based on broadly-used interpretable models with applications in decision making."	"Bo3df
Times Cited:0
Cited References Count:32"	""	"<Go to ISI>://WOS:000509978600039"	""	"Univ Patras, Patras, Greece"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. T. Duy; L. K. Tien; N. H. Khoa; D. T. T. Hien; A. G. T. Nguyen; V. H. Pham"	"2021"	"DIGFuPAS: Deceive IDS with GAN and function-preserving on adversarial samples in SDN-enabled networks"	""	"Computers & Security"	""	""	"109"	""	""	"102367"	""	""	""	""	"Oct"	""	""	"DIGFuPAS: Deceive IDS with GAN and function-preserving on adversarial samples in SDN-enabled networks"	"Comput Secur"	"0167-4048"	"ARTN 102367
10.1016/j.cose.2021.102367"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000685459300022"	""	""	"gan
adversarial attacks
intrusion detection
ids
network anomaly detection
sdn
intrusion detection"	"Showing a great potential in various domains, machine learning techniques are more and more used in the task of malicious network traffic detection to significantly enhance the ability of intrusion detection system (IDS). When associating with Software-Defined Networks (SDN), the deployment of IDSs can leverage the centralized control plane in SDN to support for large-scale network monitoring. However, machine learning-based IDSs themselves can be attacked and tricked by adversarial examples with additional perturbation from the original ones. It is vital to provide supplementary unknown traffic to evaluate and improve the resilience of IDS against variants of cyberattacks. Thus, this work explores the method of generating adversarial attack samples by Generative Adversarial Model (GAN) to deceive IDS. We propose DIGFuPAS, a framework can create attack samples which can bypass machine learning-based IDSs in SDN with the black-box manner. In this framework, instead of Vanilla GAN, we use Wassertein GAN (WGAN) to improve the ability of GAN convergence training. In addition, the strategy of preserving functional features of attack traffic is applied to maintain the operational aspect of adversarial attacks. Through our implementation and experiments on NSL-KDD and CICIDS2018 dataset, the decreased detection rate of black-box IDSs on adversarial attacks demonstrates that our proposed framework can make IDSs in SDN-enabled networks misclassify on GAN-based synthetic attacks. Also, we utilize DIGFuPAS as a tool for evaluating and improving the robustness of IDS by repetitively retraining classifiers from crafted network traffic flow. (c) 2021 Elsevier Ltd. All rights reserved."	"Ua9ff
Times Cited:1
Cited References Count:59"	""	"<Go to ISI>://WOS:000685459300022"	""	"Univ Informat Technol, Informat Secur Lab, Ho Chi Minh City, Vietnam
Vietnam Natl Univ, Ho Chi Minh City, Vietnam"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. H. Choi; H. Zhang; J. H. Kim; C. J. Hsieh; J. S. Lee"	"2019"	"Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks"	""	"2019 Ieee/Cvf International Conference on Computer Vision (Iccv 2019)"	""	""	""	""	""	"303-311"	""	""	""	""	""	""	""	"Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks"	"Ieee I Conf Comp Vis"	"1550-5499"	"10.1109/Iccv.2019.00039"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000531438100031"	""	""	""	"Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many computer vision applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks."	"Bo9jw
Times Cited:9
Cited References Count:24
IEEE International Conference on Computer Vision"	""	"<Go to ISI>://WOS:000531438100031"	""	"Yonsei Univ, Sch Integrated Technol, Seoul, South Korea
Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"P. Kantartopoulos; N. Pitropakis; A. Mylonas; N. Kylilis"	"2020"	"Exploring Adversarial Attacks and Defences for Fake Twitter Account Detection"	""	"Technologies"	""	""	"8"	""	"4"	""	""	""	""	""	"Dec"	""	""	"Exploring Adversarial Attacks and Defences for Fake Twitter Account Detection"	"Technologies"	""	"ARTN 64
10.3390/technologies8040064"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000602267800001"	""	""	"adversarial attacks
poisoning
social media
machine learning
twitter"	"Social media has become very popular and important in people's lives, as personal ideas, beliefs and opinions are expressed and shared through them. Unfortunately, social networks, and specifically Twitter, suffer from massive existence and perpetual creation of fake users. Their goal is to deceive other users employing various methods, or even create a stream of fake news and opinions in order to influence an idea upon a specific subject, thus impairing the platform's integrity. As such, machine learning techniques have been widely used in social networks to address this type of threat by automatically identifying fake accounts. Nonetheless, threat actors update their arsenal and launch a range of sophisticated attacks to undermine this detection procedure, either during the training or test phase, rendering machine learning algorithms vulnerable to adversarial attacks. Our work examines the propagation of adversarial attacks in machine learning based detection for fake Twitter accounts, which is based on AdaBoost. Moreover, we propose and evaluate the use of k-NN as a countermeasure to remedy the effects of the adversarial attacks that we have implemented."	"Pk2fy
Times Cited:1
Cited References Count:33"	""	"<Go to ISI>://WOS:000602267800001"	""	"Edinburgh Napier Univ, Sch Comp, Edinburgh EH11 4DY, Midlothian, Scotland
Eight Bells LTD, CY-2002 Nicosia, Cyprus
Univ Hertfordshire, Dept Comp Sci, Hatfield AL10 9AB, Herts, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Malik; A. Akhunzada; I. Bibi; M. Imran; A. Musaddiq; S. W. Kim"	"2020"	"Hybrid Deep Learning: An Efficient Reconnaissance and Surveillance Detection Mechanism in SDN"	""	"Ieee Access"	""	""	"8"	""	""	"134695-134706"	""	""	""	""	""	""	""	"Hybrid Deep Learning: An Efficient Reconnaissance and Surveillance Detection Mechanism in SDN"	"Ieee Access"	"2169-3536"	"10.1109/Access.2020.3009849"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000554376400001"	""	""	"computer architecture
intrusion detection
feature extraction
machine learning
software
computer crime
anomaly detection
security
hybrid deep learning model
software defined networks
long short-term memory
convolutional neural network
intrusion detection
networks taxonomy
requirements
defense
secure"	"Software defined network (SDN) centralized control intelligence and network abstraction aims to facilitate applications, service deployment, programmability, innovation and ease in configuration management of the underlying networks. However, the centralized control intelligence and programmability is primarily a potential target for the evolving cyber threats and attacks to throw the entire network into chaos. The authors propose a control plane-based orchestration for varied sophisticated threats and attacks. The proposed mechanism comprises of a hybrid Cuda-enabled DL-driven architecture that utilizes the predictive power of Long short-term memory (LSTM) and Convolutional Neural Network (CNN) for an efficient and timely detection of multi-vector threats and attacks. A current state of the art dataset CICIDS2017 and standard performance evaluation metrics have been employed to thoroughly evaluate the proposed mechanism. We rigorously compared our proposed technique with our constructed hybrid DL-architectures and current benchmark algorithms. Our analysis shows that the proposed approach out-performs in terms of detection accuracy with a trivial trade-off speed efficiency. We also performed a 10-fold cross validation to explicitly show unbiased results."	"Ms6if
Times Cited:18
Cited References Count:38"	""	"<Go to ISI>://WOS:000554376400001"	""	"COMSATS Univ Islamabad, Comp Sci Dept, Islamabad 46000, Pakistan
Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark
Yeungnam Univ, Dept Informat & Commun Engn, Gyongsan 38541, South Korea"	""	""	""	""	""	""	""	"English"
"Journal Article"	"I. Hussain; J. S. Zeng; Xinhong; S. Q. Tan"	"2020"	"A Survey on Deep Convolutional Neural Networks for Image Steganography and Steganalysis"	""	"Ksii Transactions on Internet and Information Systems"	""	""	"14"	""	"3"	"1228-1248"	""	""	""	""	"Mar 31"	""	""	"A Survey on Deep Convolutional Neural Networks for Image Steganography and Steganalysis"	"Ksii T Internet Inf"	"1976-7277"	"10.3837/tiis.2020.03.017"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000523207200017"	""	""	"steganalysis
steganography
deep learning
convolutional neural network"	"Steganalysis & steganography have witnessed immense progress over the past few years by the advancement of deep convolutional neural networks (DCNN). In this paper, we analyzed current research states from the latest image steganography and steganalysis frameworks based on deep learning. Our objective is to provide for future researchers the work being done on deep learning-based image steganography & steganalysis and highlights the strengths and weakness of existing up-to-date techniques. The result of this study opens new approaches for upcoming research and may serve as source of hypothesis for further significant research on deep learning-based image steganography and steganalysis. Finally, technical challenges of current methods and several promising directions on deep learning steganography and steganalysis are suggested to illustrate how these challenges can be transferred into prolific future research avenues."	"Kz4ap
Times Cited:10
Cited References Count:82"	""	"<Go to ISI>://WOS:000523207200017"	""	"Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Liu; N. Shroff"	"2019"	"Data Poisoning Attacks on Stochastic Bandits"	""	"International Conference on Machine Learning, Vol 97"	""	""	"97"	""	""	""	""	""	""	""	""	""	""	"Data Poisoning Attacks on Stochastic Bandits"	"Pr Mach Learn Res"	"2640-3498"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000684034304020"	""	""	""	"Stochastic multi-armed bandits form a class of online learning problems that have important applications in online recommendation systems, adaptive medical treatment, and many others. Even though potential attacks against these learning algorithms may hijack their behavior, causing catastrophic loss in real-world applications, little is known about adversarial attacks on bandit algorithms. In this paper, we propose a framework of offline attacks on bandit algorithms and study convex optimization based attacks on several popular bandit algorithms. We show that the attacker can force the bandit algorithm to pull a target arm with high probability by a slight manipulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and propose an adaptive attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adaptive attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits."	"Bs0rl
Times Cited:5
Cited References Count:19
Proceedings of Machine Learning Research"	""	"<Go to ISI>://WOS:000684034304020"	""	"Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA
Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Y. Huang; Y. Lin; H. Y. Lee; L. S. Lee"	"2021"	"Defending Your Voice: Adversarial Attack on Voice Conversion"	""	"2021 Ieee Spoken Language Technology Workshop (Slt)"	""	""	""	""	""	"552-559"	""	""	""	""	""	""	""	"Defending Your Voice: Adversarial Attack on Voice Conversion"	"Ieee W Sp Lang Tech"	"2639-5479"	"10.1109/Slt48900.2021.9383529"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000663633300076"	""	""	"voice conversion
adversarial attack
speaker verification
speaker representation"	"Substantial improvements have been achieved in recent years in voice conversion, which converts the speaker characteristics of an utterance into those of another speaker without changing the linguistic content of the utterance. Nonetheless, the improved conversion technologies also led to concerns about privacy and authentication. It thus becomes highly desired to be able to prevent one's voice from being improperly utilized with such voice conversion technologies. This is why we report in this paper the first known attempt to perform adversarial attack on voice conversion. We introduce human imperceptible noise into the utterances of a speaker whose voice is to be defended. Given these adversarial examples, voice conversion models cannot convert other utterances so as to sound like being produced by the defended speaker. Preliminary experiments were conducted on two currently state-of-the-art zero-shot voice conversion models. Objective and subjective evaluation results in both white-box and black-box scenarios are reported. It was shown that the speaker characteristics of the converted utterances were made obviously different from those of the defended speaker, while the adversarial examples of the defended speaker are not distinguishable from the authentic utterances."	"Br6so
Times Cited:3
Cited References Count:39
IEEE Workshop on Spoken Language Technology"	""	"<Go to ISI>://WOS:000663633300076"	""	"Natl Taiwan Univ, Coll Elect Engn & Comp Sci, Taipei, Taiwan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. J. Liang; W. B. Guo; T. B. Luo; V. Honavar; G. Wang; X. Y. Xing"	"2021"	"FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data"	""	"28th Annual Network and Distributed System Security Symposium (Ndss 2021)"	""	""	""	""	""	""	""	""	""	""	""	""	""	"FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data"	""	""	"10.14722/ndss.2021.24403"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000680821200068"	""	""	""	"Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels. Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers. Without such labels, it is challenging to train and deploy targeted countermeasures.
In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels. We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarse-grained labels available for different attack types. The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the low-quality labels. We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarse-grained labels. We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering. Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness. Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts. With real-world A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds."	"Bs0bg
Times Cited:1
Cited References Count:84"	""	"<Go to ISI>://WOS:000680821200068"	""	"Penn State Univ, University Pk, PA 16802 USA
Robinhood, San Jose, CA USA
Univ Illinois, Urbana, IL USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Huang; Q. X. Zhang; J. B. Liu; R. T. Hou; X. M. Wang; Y. Li"	"2020"	"Adversarial attacks on deep-learning-based SAR image target recognition"	""	"Journal of Network and Computer Applications"	""	""	"162"	""	""	"102632"	""	""	""	""	"Jul 15"	""	""	"Adversarial attacks on deep-learning-based SAR image target recognition"	"J Netw Comput Appl"	"1084-8045"	"ARTN 102632
10.1016/j.jnca.2020.102632"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000536139100001"	""	""	"sar
deep learning
convolutional neural network
adversarial example"	"Synthetic aperture radar (SAR) image target recognition has consistently been a research hotspot in the field of radar image interpretation. Compared with traditional target recognition algorithms, SAR target recognition algorithms based on deep learning offer end-to-end feature learning, which can effectively improve the target recognition rate, making them an important method for radar target recognition. However, recent research shows that optical image recognition methods based on deep learning are vulnerable to adversarial examples. In SAR image target recognition, whether adversarial examples exist for deep learning algorithms is still an open question. This paper uses three mainstream algorithms to generate adversarial examples to attack three classical deep learning algorithms for SAR image target recognition. The experiments involve publicly real SAR images for white-box and black-box attacks. The results show that SAR target recognition algorithms based on deep learning are potentially vulnerable to adversarial examples."	"Ls1gb
Times Cited:22
Cited References Count:34"	""	"<Go to ISI>://WOS:000536139100001"	""	"Guangzhou Univ, Sch Comp Sci, Guangzhou 510006, Peoples R China
Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Zhang; C. Yan; B. A. Malin"	"2022"	"Membership inference attacks against synthetic health data"	""	"J Biomed Inform"	""	""	"125"	""	""	"103977"	""	""	""	"2021/12/18"	"Jan"	""	""	"Membership inference attacks against synthetic health data"	""	"1532-0480 (Electronic)
1532-0464 (Linking)"	"10.1016/j.jbi.2021.103977"	""	""	""	""	"PMC8766950"	""	""	""	""	""	""	"34920126"	""	""	"Confidentiality
Disclosure
Genomics
Humans
Machine Learning
*Population Health
*Contrastive representation learning
*Electronic health record
*Membership inference
*Synthetic data"	"Synthetic data generation has emerged as a promising method to protect patient privacy while sharing individual-level health data. Intuitively, sharing synthetic data should reduce disclosure risks because no explicit linkage is retained between the synthetic records and the real data upon which it is based. However, the risks associated with synthetic data are still evolving, and what seems protected today may not be tomorrow. In this paper, we show that membership inference attacks, whereby an adversary infers if the data from certain target individuals (known to the adversary a priori) were relied upon by the synthetic data generation process, can be substantially enhanced through state-of-the-art machine learning frameworks, which calls into question the protective nature of existing synthetic data generators. Specifically, we formulate the membership inference problem from the perspective of the data holder, who aims to perform a disclosure risk assessment prior to sharing any health data. To support such an assessment, we introduce a framework for effective membership inference against synthetic health data without specific assumptions about the generative model or a well-defined data structure, leveraging the principles of contrastive representation learning. To illustrate the potential for such an attack, we conducted experiments against synthesis approaches using two datasets derived from several health data resources (Vanderbilt University Medical Center, the All of Us Research Program) to determine the upper bound of risk brought by an adversary who invokes an optimal strategy. The results indicate that partially synthetic data are vulnerable to membership inference at a very high rate. By contrast, fully synthetic data are only marginally susceptible and, in most cases, could be deemed sufficiently protected from membership inference."	"Zhang, Ziqi
Yan, Chao
Malin, Bradley A
eng
U2C OD023196/OD/NIH HHS/
UL1 TR002243/TR/NCATS NIH HHS/
Research Support, N.I.H., Extramural
J Biomed Inform. 2022 Jan;125:103977. doi: 10.1016/j.jbi.2021.103977. Epub 2021 Dec 14."	""	"https://www.ncbi.nlm.nih.gov/pubmed/34920126"	""	"Vanderbilt University, 2525 West End Avenue, Nashville, TN 37240, United States. Electronic address: ziqi.zhang@vanderbilt.edu.
Vanderbilt University, 2525 West End Avenue, Nashville, TN 37240, United States.
Vanderbilt University, 2525 West End Avenue, Nashville, TN 37240, United States; Vanderbilt University Medical Center, 2525 West End Avenue, Nashville, TN 37240, United States."	""	""	""	""	""	""	""	""
"Journal Article"	"D. Roy; C. Morse; M. A. McGrath; J. He; A. Arora"	"2017"	"Cross-Environmentally Robust Intruder Discrimination in Radar Motes"	""	"2017 Ieee 14th International Conference on Mobile Ad Hoc and Sensor Systems (Mass)"	""	""	""	""	""	"426-434"	""	""	""	""	""	""	""	"Cross-Environmentally Robust Intruder Discrimination in Radar Motes"	"Ieee Int Conf Mob"	"2155-6806"	"10.1109/Mass.2017.54"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000427360900054"	""	""	"feature-selection"	"Target discrimination in wireless sensor networks remains challenging when sensors have structured electronic noise and deployment settings have variable in-situ clutter. Data driven learning of discrimination functions is especially hard when deployment sites are remote or hazardous, necessitating reliance on surrogate environments for data collection. The challenge is exacerbated if sensors are resource constrained. We present an intruder discrimination system that addresses these challenges in the context of a battery powered radar mote. The system robustly rejects clutter moving in-situ to trigger discrimination of humans versus other targets only when said targets displace across the scene. Its learning uses a new, generic extension to feature selection methods that leverages cross-environmental robustness instead of a random or bounded model of feature noise. We experimentally validate that our scheme improves the cross-environmental performance medians and dispersions of extant methods by up to 70% and 100% respectively. It achieves optimal performance with very few features given a modest number of diverse training environments, allowing for efficient mote-scale implementation. A mote mesh network has been deployed to detect poachers while rejecting cattle and other non-targets at a rhino reserve in South Africa."	"Bj7jq
Times Cited:4
Cited References Count:25
IEEE International Conference on Mobile Ad-hoc and Sensor Systems"	""	"<Go to ISI>://WOS:000427360900054"	""	"Ohio State Univ, Columbus, OH 43210 USA
Samraksh Co, Dublin, OH USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"T. Huang; Y. F. Chen; B. J. Yao; B. F. Yang; X. M. Wang; Y. Li"	"2020"	"Adversarial attacks on deep-learning-based radar range profile target recognition"	""	"Information Sciences"	""	""	"531"	""	""	"159-176"	""	""	""	""	"Aug"	""	""	"Adversarial attacks on deep-learning-based radar range profile target recognition"	"Inform Sciences"	"0020-0255"	"10.1016/j.ins.2020.03.066"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000537628800010"	""	""	"adversarial attacks
deep neural networks
radar images
target recognition"	"Target recognition based on a high-resolution range profile (HRRP) has always been a research hotspot in the radar signal interpretation field. Deep learning has been an important method for HRRP target recognition. However, recent research has shown that optical image target recognition methods based on deep learning are vulnerable to adversarial samples. Whether HRRP target recognition methods based on deep learning can be attacked remains an open question. In this paper, four methods of generating adversarial perturbations are proposed. Algorithm 1 generates the nontargeted fine-grained perturbation based on the binary search method. Algorithm 2 generates the targeted fine-grained perturbation based on the multiple-iteration method. Algorithm 3 generates the nontargeted universal adversarial perturbation (UAP) based on aggregating some fine-grained perturbations. Algorithm 4 generates the targeted universal perturbation based on scaling one fine-grained perturbation. These perturbations are used to generate adversarial samples to attack HRRP target recognition methods based on deep learning under white-box and black-box attacks. The experiments are conducted with actual radar data and show that the HRRP adversarial samples have certain aggressiveness. Therefore, HRRP target recognition methods based on deep learning have potential security risks. (c) 2020 Elsevier Inc. All rights reserved."	"Lu2zd
Times Cited:19
Cited References Count:34"	""	"<Go to ISI>://WOS:000537628800010"	""	"Guangzhou Univ, Sch Comp Sci, Guangzhou 510006, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Arjoune; S. Faruque"	"2020"	"Artificial Intelligence for 5G Wireless Systems: Opportunities, Challenges, and Future Research Direction"	""	"2020 10th Annual Computing and Communication Workshop and Conference (Ccwc)"	""	""	""	""	""	"1023-1028"	""	""	""	""	""	""	""	"Artificial Intelligence for 5G Wireless Systems: Opportunities, Challenges, and Future Research Direction"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000668567200164"	""	""	"5g wireless communication
deep learning
energy efficiency
channel coding
scheduling
cybersecurity
channel estimation
massive mimo
deep
networks"	"The advent of the wireless communications systems augurs new cutting-edge technologies, including self-driving vehicles, unmanned aerial systems, autonomous robots, the Internet-of-Things, and virtual reality. These technologies require high data rates, ultra-low latency, and high reliability, all of which are promised by the fifth generation of wireless communication systems (5G). Many research groups state that 5G cannot meet its demands without artificial intelligence (A.I.) integration as 5G wireless networks are expected to generate unprecedented traffic giving wireless research designers access to big data that can help in predicting the demands and adjust cell designs to meet the users' requirements. Subsequently, many researchers applied A.I. in many aspects of 5G wireless communication design including radio resource allocation, network management, cybersecurity. In this paper, we provide an in-depth review of A.I. for 5G wireless communication systems. in this respect, the aim of this paper is to survey A.I. in 5G wireless communication and networking by discussing many case studies, discuss the challenges, and shed new light on future research directions for leveraging A.I. in 5G wireless communications."	"Br7ni
Times Cited:7
Cited References Count:49"	""	"<Go to ISI>://WOS:000668567200164"	""	"Univ North Dakota, Sch Elect Engn & Comp Sci, Grand Forks, ND 58202 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Li; W. M. Zhang; C. Qin; K. J. Chen; W. B. Zhou; N. H. Yu"	"2021"	"Adversarial batch image steganography against CNN-based pooled steganalysis"	""	"Signal Processing"	""	""	"181"	""	""	"107920"	""	""	""	""	"Apr"	""	""	"Adversarial batch image steganography against CNN-based pooled steganalysis"	"Signal Process"	"0165-1684"	"ARTN 107920
10.1016/j.sigpro.2020.107920"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000611843900010"	""	""	"batch steganography
adversarial attack
pooled steganalysis
deep learning
adaptive steganography
social networks
strategy"	"The application of adversarial embedding in single image steganography exhibits its advantage in resisting convolutional neural network (CNN)-based steganalysis. As an important technique to move the steganography from the laboratory to the real world, batch steganography is developed based on the single image steganography, which uses a series of images as carriers. Furthermore, existing pooled steganalysis also applied CNN architecture for feature extraction, which aims to detect batch steganography. Therefore, it is reasonable and meaningful to introduce adversarial embedding in batch steganography to resist pooled steganalysis. However, as far as we know, there is no work about adversarial batch steganography. Adversarial batch image steganography should be able to resist pooled steganalysis which takes a group of images as a unit, therefore the loss function of the single image steganalyzer can not be directly used for adversarial embedding. In addition, adversarial embedding should be combined with batch strategy. In this paper, we propose a general framework of adversarial embedding for batch steganography, in which a new loss function is designed and the batch strategy is combined with adversarial embedding. By this framework, we can adapt most adversarial embedding algorithms for single image steganography to batch steganography. To verify the efficiency of the proposed framework, we design an algorithm called ADVersarial Image Merging Steganography (ADV-IMS) based on ADVersarial EMBedding (ADV-EMB), and carry out a series corresponding experiments. Experimental results show the proposed method significantly improves the security performance of batch steganography against pooled steganalysis and keeps a high-security level against single image steganalysis. (C) 2020 Elsevier B.V. All rights reserved."	"Py1xn
Times Cited:4
Cited References Count:42"	""	"<Go to ISI>://WOS:000611843900010"	""	"Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"V. Duddu"	"2018"	"A Survey of Adversarial Machine Learning in Cyber Warfare"	""	"Defence Science Journal"	""	""	"68"	""	"4"	"356-366"	""	""	""	""	"Jul"	""	""	"A Survey of Adversarial Machine Learning in Cyber Warfare"	"Defence Sci J"	"0011-748x"	"10.14429/dsj.68.12371"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000444418100004"	""	""	"adversarial machine learning
adversary modelling
cyber attacks
security
privacy
security
attacks"	"The changing nature of warfare has seen a paradigm shift from the conventional to asymmetric, contactless warfare such as information and cyber warfare. Excessive dependence on information and communication technologies, cloud infrastructures, big data analytics, data-mining and automation in decision making poses grave threats to business and economy in adversarial environments. Adversarial machine learning is a fast growing area of research which studies the design of Machine Learning algorithms that are robust in adversarial environments. This paper presents a comprehensive survey of this emerging area and the various techniques of adversary modelling. We explore the threat models for Machine Learning systems and describe the various techniques to attack and defend them. We present privacy issues in these models and describe a cyber-warfare test-bed to test the effectiveness of the various attack-defence strategies and conclude with some open problems in this area of research."	"Gt3ql
Times Cited:17
Cited References Count:80"	""	"<Go to ISI>://WOS:000444418100004"	""	"Indraprastha Inst Informat Technol, Delhi 110020, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. S. Zhang; B. Liu; T. Q. Zhu; A. D. Zhou; W. L. Zhou"	"2022"	"Visual privacy attacks and defenses in deep learning: a survey"	""	"Artificial Intelligence Review"	""	""	""	""	""	""	""	""	""	""	"Jan 31"	""	""	"Visual privacy attacks and defenses in deep learning: a survey"	"Artif Intell Rev"	"0269-2821"	"10.1007/s10462-021-10123-y"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000750286100002"	""	""	"visual privacy
attack and defense
deep learning
privacy preservation"	"The concerns on visual privacy have been increasingly raised along with the dramatic growth in image and video capture and sharing. Meanwhile, with the recent breakthrough in deep learning technologies, visual data can now be easily gathered and processed to infer sensitive information. Therefore, visual privacy in the context of deep learning is now an important and challenging topic. However, there has been no systematic study on this topic to date. In this survey, we discuss algorithms of visual privacy attacks and the corresponding defense mechanisms in deep learning. We analyze the privacy issues in both visual data and visual deep learning systems. We show that deep learning can be used as a powerful privacy attack tool as well as preservation techniques with great potential. We also point out the possible direction and suggestions for future work. By thoroughly investigating the relationship of visual privacy and deep learning, this article sheds insights on incorporating privacy requirements in the deep learning era."	"Yr9cj
Times Cited:1
Cited References Count:254"	""	"<Go to ISI>://WOS:000750286100002"	""	"Univ Technol Sydney, Ctr Cyber Secur & Privacy, Sch Comp Sci, Sydney, NSW, Australia
City Univ Macau, Macau, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Gandhimathi; G. Murugaboopathi"	"2020"	"A Novel Hybrid Intrusion Detection Using Flow-Based Anomaly Detection and Cross-Layer Features in Wireless Sensor Network"	""	"Automatic Control and Computer Sciences"	""	""	"54"	""	"1"	"62-69"	""	""	""	""	"Jan"	""	""	"A Novel Hybrid Intrusion Detection Using Flow-Based Anomaly Detection and Cross-Layer Features in Wireless Sensor Network"	"Autom Control Comput"	"0146-4116"	"10.3103/S0146411620010046"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000521741400007"	""	""	"cross-layer
multi-layer detection
attacks
flow-based anomaly detection
detection accuracy
detection system
attacks"	"The emerging technology of Wireless sensor network is expected to provide Intrusion detection technique for all kind of attacks. This article models and analyzes intrusion detection using flow-based IDS and cross-layered approach. The Critical application needs to replace cryptographic technique with flow-based anomaly detection for secure communication and single layer detection method with multi-layer detection method for efficient attack detection. The proposed detection system has a two-phase approach. In the first phase, flow-based anomaly detection method is used to detect potential anomalies in the network traffic. During second phase cross-layer features are correlated to narrow down the possible attacks. Simulation results clearly show that the proposed detection technique has an excellent performance in terms of detection accuracy, the energy consumption, and the false positive rate will reduce than the layer-based approach and packet-based approach."	"Kx2wc
Times Cited:1
Cited References Count:13"	""	"<Go to ISI>://WOS:000521741400007"	""	"Kalasalingam Acad Res & Educ, Dept Comp Sci & Engn, Krishnankoil, Tamil Nadu, India"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Cullina; A. N. Bhagoji; P. Mittal"	"2018"	"PAC-learning in the presence of evasion adversaries"	""	"Advances in Neural Information Processing Systems 31 (Nips 2018)"	""	""	"31"	""	""	""	""	""	""	""	""	""	""	"PAC-learning in the presence of evasion adversaries"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000461823300022"	""	""	"robustness"	"The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of an evasion adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the adversarial VC-dimension. We then show that sample complexity upper bounds from the Fundamental Theorem of Statistical learning can be extended to the case of evasion adversaries, where the sample complexity is controlled by the adversarial VC-dimension. We then explicitly derive the adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimension. Finally, we prove that the adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right."	"Bm3bf
Times Cited:4
Cited References Count:83
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000461823300022"	""	"Princeton Univ, Princeton, NJ 08544 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Liu; C. Peng; Y. Tian; S. Long; F. Tian; Z. Wu"	"2022"	"GDP vs. LDP: A Survey from the Perspective of Information-Theoretic Channel"	""	"Entropy (Basel)"	""	""	"24"	""	"3"	""	""	""	""	"2022/03/26"	"Mar 19"	""	""	"GDP vs. LDP: A Survey from the Perspective of Information-Theoretic Channel"	""	"1099-4300 (Electronic)
1099-4300 (Linking)"	"10.3390/e24030430"	""	""	""	""	"PMC8953244"	""	""	""	""	""	""	"35327940"	""	""	"GDP vs. LDP
Renyi divergence
expected distortion
information-theoretic channel
mutual information"	"The existing work has conducted in-depth research and analysis on global differential privacy (GDP) and local differential privacy (LDP) based on information theory. However, the data privacy preserving community does not systematically review and analyze GDP and LDP based on the information-theoretic channel model. To this end, we systematically reviewed GDP and LDP from the perspective of the information-theoretic channel in this survey. First, we presented the privacy threat model under information-theoretic channel. Second, we described and compared the information-theoretic channel models of GDP and LDP. Third, we summarized and analyzed definitions, privacy-utility metrics, properties, and mechanisms of GDP and LDP under their channel models. Finally, we discussed the open problems of GDP and LDP based on different types of information-theoretic channel models according to the above systematic review. Our main contribution provides a systematic survey of channel models, definitions, privacy-utility metrics, properties, and mechanisms for GDP and LDP from the perspective of information-theoretic channel and surveys the differential privacy synthetic data generation application using generative adversarial network and federated learning, respectively. Our work is helpful for systematically understanding the privacy threat model, definitions, privacy-utility metrics, properties, and mechanisms of GDP and LDP from the perspective of information-theoretic channel and promotes in-depth research and analysis of GDP and LDP based on different types of information-theoretic channel models."	"Liu, Hai
Peng, Changgen
Tian, Youliang
Long, Shigong
Tian, Feng
Wu, Zhenqiang
eng
Grant 62002081, Grant 62062020, Grant U1836205, and Grant 61602290/National Natural Science Foundation of China
Grant 2019M663907XB/Project Funded by China Postdoctoral Science Foundation
Grant 20183001/Major Scientific and Technological Special Project of Guizhou Province
Grant 2018BDKFJJ004/Foundation of Guizhou Provincial Key Laboratory of Public Big Data
Review
Switzerland
Entropy (Basel). 2022 Mar 19;24(3). pii: e24030430. doi: 10.3390/e24030430."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35327940"	""	"Guizhou Big Data Academy, Guizhou University, Guiyang 550025, China.
College of Computer Science and Technology, Guizhou University, Guiyang 550025, China.
State Key Laboratory of Public Big Data, Guizhou University, Guiyang 550025, China.
School of Computer Science, Shaanxi Normal University, Xi'an 710119, China."	""	""	""	""	""	""	""	""
"Journal Article"	"S. Dilmaghani; M. R. Brust; G. Danoy; N. Cassagnes; J. Pecero; P. C. Bouvry"	"2019"	"Privacy and Security of Big Data in AI Systems: A Research and Standards Perspective"	""	"2019 Ieee International Conference on Big Data (Big Data)"	""	""	""	""	""	"5737-5743"	""	""	""	""	""	""	""	"Privacy and Security of Big Data in AI Systems: A Research and Standards Perspective"	"Ieee Int Conf Big Da"	"2639-1589"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000554828705113"	""	""	"attacks"	"The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness."	"Bp4wk
Times Cited:6
Cited References Count:74
IEEE International Conference on Big Data"	""	"<Go to ISI>://WOS:000554828705113"	""	"Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, Luxembourg, Luxembourg
Univ Luxembourg, Fac Sci Technol & Commun FSTC, Luxembourg, Luxembourg
Agence Normalisat & Econ Connaissance ANEC GIE, Luxembourg, Luxembourg"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Oneto; N. Navarin; B. Biggio; F. Errica; A. Micheli; F. Scarselli; M. Bianchini; L. Demetrio; P. Bongini; A. Tacchella; A. Sperduti"	"2022"	"Towards learning trustworthily, automatically, and with guarantees on graphs: An overview"	""	"Neurocomputing"	""	""	"493"	""	""	"217-243"	""	""	""	""	"Jul 7"	""	""	"Towards learning trustworthily, automatically, and with guarantees on graphs: An overview"	"Neurocomputing"	"0925-2312"	"10.1016/j.neucom.2022.04.072"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000796514400002"	""	""	"learning on graphs
graph neural networks
kernels for graphs
trustworthy machine learning
fairness
privacy
robustness
explainability
learning automatically
learning with guarantees
neural-network
decision-making
classification
kernels
fair
approximation
explanation
classifiers
viewpoint
parameter"	"The increasing digitization and datification of all aspects of people's daily life, and the consequent growth in the use of personal data, are increasingly challenging the current development and adoption of Machine Learning (ML). First, the sheer complexity and amount of data available in these applications strongly demands for ML algorithms that can be trained directly on complex structures, which can be naturally described by graphs. In fact, graphs inherently capture information about entities, their attri-butes, and relationships between them. Directly applying ML to graphs relieves domain experts and data scientists from the challenging and time-consuming problem of designing a suitable vector-based data representation used by classical ML techniques. Second, ML algorithms should not only be designed to achieve high technical and functional standards; as the automated decisions provided by these algo-rithms can have a relevant impact on people's lives, their behavior has to be aligned with the values and principles of individuals and society. This demands for designing automated algorithms that we, as humans, can trust, fulfilling the requirements of fairness, robustness, privacy, and explainability. Third, designing effective ML algorithms requires skills and expertise developed at different levels. This substantially hinders the democratization and widespread availability of such technology for society at large, which in turn demands for improving the level of automatization and systematization of their design process, while also providing guarantees on their performance. For this reason, this paper provides an overview of the current works focused towards learning trustworthily, automatically, and with guar-antees on graphs.(c) 2022 Elsevier B.V. All rights reserved."	"1h4la
Times Cited:0
Cited References Count:231"	""	"<Go to ISI>://WOS:000796514400002"	""	"Univ Genoa, Via Opera Pia 11A, I-16145 Genoa, Italy
Univ Padua, Via Trieste 63, I-35121 Padua, Italy
Univ Cagliari, Piazza Armi, I-09123 Cagliari, Italy
Univ Pisa, Largo Bruno Pontecorvo 3, I-56127 Pisa, Italy
Univ Siena, Via Roma 56, I-53100 Siena, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Pitropakis; E. Panaousis; T. Giannetsos; E. Anastasiadis; G. Loukas"	"2019"	"A taxonomy and survey of attacks against machine learning"	""	"Computer Science Review"	""	""	"34"	""	""	""	""	""	""	""	"Nov"	""	""	"A taxonomy and survey of attacks against machine learning"	"Comput Sci Rev"	"1574-0137"	"ARTN 100199
10.1016/j.cosrev.2019.100199"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000497715600006"	""	""	"machine learning
attacks
taxonomy
survey
security
recognition
classifiers"	"The majority of machine learning methodologies operate with the assumption that their environment is benign. However, this assumption does not always hold, as it is often advantageous to adversaries to maliciously modify the training (poisoning attacks) or test data (evasion attacks). Such attacks can be catastrophic given the growth and the penetration of machine learning applications in society. Therefore, there is a need to secure machine learning enabling the safe adoption of it in adversarial cases, such as spam filtering, malware detection, and biometric recognition. This paper presents a taxonomy and survey of attacks against systems that use machine learning. It organizes the body of knowledge in adversarial machine learning so as to identify the aspects where researchers from different fields can contribute to. The taxonomy identifies attacks which share key characteristics and as such can potentially be addressed by the same defence approaches. Thus, the proposed taxonomy makes it easier to understand the existing attack landscape towards developing defence mechanisms, which are not investigated in this survey. The taxonomy is also leveraged to identify open problems that can lead to new research areas within the field of adversarial machine learning. (C) 2019 Elsevier Inc. All rights reserved."	"Jo6vp
Times Cited:48
Cited References Count:97"	""	"<Go to ISI>://WOS:000497715600006"	""	"Edinburgh Napier Univ, Edinburgh, Midlothian, Scotland
Univ Surrey, Guildford, Surrey, England
Tech Univ Denmark, Lyngby, Denmark
Imperial Coll London, London, England
Univ Greenwich, London, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. N. Lin; V. Sekar; G. Fanti"	"2021"	"On the Privacy Properties of GAN-generated Samples"	""	"24th International Conference on Artificial Intelligence and Statistics (Aistats)"	""	""	"130"	""	""	""	""	""	""	""	""	""	""	"On the Privacy Properties of GAN-generated Samples"	"Pr Mach Learn Res"	"2640-3498"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000659893801083"	""	""	""	"The privacy implications of generative adversarial networks (GANs) are a topic of great interest, leading to several recent algorithms for training GANs with privacy guarantees. By drawing connections to the generalization properties of GANs, we prove that under some assumptions, GAN-generated samples inherently satisfy some (weak) privacy guarantees. First, we show that if a GAN is trained on m samples and used to generate n samples, the generated samples are (epsilon, delta)-differentially-private for (epsilon, delta) pairs where delta scales as O(n/m). We show that under some special conditions, this upper bound is tight. Next, we study the robustness of GAN-generated samples to membership inference attacks. We model membership inference as a hypothesis test in which the adversary must determine whether a given sample was drawn from the training dataset or from the underlying data distribution. We show that this adversary can achieve an area under the ROC curve that scales no better than O(m(-1/4))."	"Br6dc
Times Cited:0
Cited References Count:48
Proceedings of Machine Learning Research"	""	"<Go to ISI>://WOS:000659893801083"	""	"Carnegie Mellon Univ, Pittsburgh, PA 15213 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Y. Sun; J. J. Liu; J. D. Wang; Y. R. Cao; N. Kato"	"2020"	"When Machine Learning Meets Privacy in 6G: A Survey"	""	"Ieee Communications Surveys and Tutorials"	""	""	"22"	""	"4"	"2694-2724"	""	""	""	""	""	""	""	"When Machine Learning Meets Privacy in 6G: A Survey"	"Ieee Commun Surv Tut"	""	"10.1109/Comst.2020.3011561"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000593999900019"	""	""	"privacy
data privacy
big data
mimo communication
tutorials
machine learning
privacy
machine learning
6g
violation
protection
communication
double-edged sword
support vector machine
internet-of-things
traffic classification
heterogeneous internet
vehicles challenges
intrusion detection
mobile
edge
security
network"	"The rapid-developing Artificial Intelligence (AI) technology, fast-growing network traffic, and emerging intelligent applications (e.g., autonomous driving, virtual reality, etc.) urgently require a new, faster, more reliable and flexible network form. At this time, researchers in both industry and academia have turned their attention to the sixth generation (6G) communication networks. In the 6G vision, various intelligent application scenarios that utilize Machine Learning (ML) technology (the most important branch of AI) will bring rich heterogeneous connections, as well as massive information storage and operations. When ML meets 6G, new opportunities will emerge along with numerous privacy challenges. On one hand, a secure ML structure, or the correct application of ML, can protect privacy in 6G. On the other hand, ML may be attacked or abused, resulting in privacy violation. It is worth noting that the alliance between 6G and ML may also be a double-edged sword in many cases, rather than absolutely infringe or protect privacy. Therefore, based on lots of existing meaningful works, this paper aims to provide a comprehensive survey of ML and privacy in 6G, with a view to further promoting the development of 6G and privacy protection technologies."	"Oy1fx
Times Cited:37
Cited References Count:199"	""	"<Go to ISI>://WOS:000593999900019"	""	"Xidian Univ, State Key Lab Integrated Serv Networks, Sch Cyber Engn, Xian 710071, Peoples R China
Northwestern Polytech Univ, Natl Engn Lab Integrated Aero Space Ground Ocean, Sch Cybersecur, Xian 710072, Peoples R China
Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Chen; Z. K. Zhang; T. H. Wang; M. Backes; M. Humbert; Y. Zhang"	"2021"	"When Machine Unlearning Jeopardizes Privacy"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"896-911"	""	""	""	""	""	""	""	"When Machine Unlearning Jeopardizes Privacy"	""	""	"10.1145/3460120.3484756"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478300056"	""	""	"machine unlearning
membership inference
machine learning security and privacy"	"The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning.(1)"	"Bs7xr
Times Cited:2
Cited References Count:72"	""	"<Go to ISI>://WOS:000768478300056"	""	"CISPA Helmholtz Ctr Informat Secur, Saarbrucken, Germany
Carnegie Mellon Univ, Pittsburgh, PA 15213 USA
Univ Virginia, Charlottesville, VA 22903 USA
Univ Lausanne, Lausanne, Switzerland
Purdue Univ, W Lafayette, IN 47907 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Liu; G. Ditzler"	"2020"	"Detecting Adversarial Audio via Activation Quantization Error"	""	"2020 International Joint Conference on Neural Networks (Ijcnn)"	""	""	""	""	""	"1-7"	""	""	""	""	""	""	""	"Detecting Adversarial Audio via Activation Quantization Error"	"Ieee Ijcnn"	"2161-4393"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000626021403021"	""	""	""	"The robustness and vulnerability of Deep Neural Networks (DNN) are quickly becoming a critical area of interest since these models are in widespread use across real-world applications (i.e., image and audio analysis, recommendation system, natural language analysis, etc.). A DNN's vulnerability is exploited by an adversary to generate data to attack the model; however, the majority of adversarial data generators have focused on image domains with far fewer work on audio domains. More recently, audio analysis models were shown to be vulnerable to adversarial audio examples (e.g., speech command classification, automatic speech recognition, etc.). Thus, one urgent open problem is to detect adversarial audio reliably. In this contribution, we incorporate a separate and yet related DNN technique to detect adversarial audio, namely model quantization. Then we propose an algorithm to detect adversarial audio by using a DNN's quantization error. Specifically, we demonstrate that adversarial audio typically exhibits a larger activation quantization error than benign audio. The quantization error is measured using character error rates. We use the difference in errors to discriminate adversarial audio. Experiments with three the-state-of-the-art audio attack algorithms against the DeepSpeech model show our detection algorithm achieved high accuracy on the Mozilla dataset."	"Bq9mm
Times Cited:0
Cited References Count:26
IEEE International Joint Conference on Neural Networks (IJCNN)"	""	"<Go to ISI>://WOS:000626021403021"	""	"Univ Arizona, Dept Elect Engn, Tucson, AZ 85721 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"G. Pitolli; G. Laurenza; L. Aniello; L. Querzoni; R. Baldoni"	"2021"	"MalFamAware: automatic family identification and malware classification through online clustering"	""	"International Journal of Information Security"	""	""	"20"	""	"3"	"371-386"	""	""	""	""	"Jun"	""	""	"MalFamAware: automatic family identification and malware classification through online clustering"	"Int J Inf Secur"	"1615-5262"	"10.1007/s10207-020-00509-4"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000540671400001"	""	""	"malware analysis
malware family identification
incremental clustering
birch"	"The skyrocketing growth rate of new malware brings novel challenges to protect computers and networks. Discerning truly novel malware from variants of known samples is a way to keep pace with this trend. This can be done by grouping known malware in families by similarity and classifying new samples into those families. As malware and their families evolve over time, approaches based on classifiers trained on a fixed ground truth are not suitable. Other techniques use clustering to identify families, but they need to periodically re-cluster the whole set of samples, which does not scale well. A promising approach is based on incremental clustering, where periodically only yet unknown samples are clustered to identify new families, and classifiers are retrained accordingly. However, the latter solutions usually are not able to immediately react and identify new malware families. In this paper, we propose MalFamAware, a novel approach to malware family identification based on an online clustering algorithm, namely BIRCH, which efficiently updates clusters as new samples are fed without requiring to re-scan the entire dataset. MalFamAwareis able to both classify new malware in existing families and identify new families at runtime. We present experimental evaluations where MalFamAware outperforms both total re-clustering and incremental clustering solutions in terms of accuracy and time. We also compare our solution with classifiers retrained over time, obtaining better accuracy, in particular when samples belong to yet unknown families."	"Sj9wp
Times Cited:3
Cited References Count:45"	""	"<Go to ISI>://WOS:000540671400001"	""	"Sapienza Univ Rome, Res Ctr Cyber Intelligence & Informat Secur, Rome, Italy
Univ Southampton, Cyber Secur Res Grp, Southampton, Hants, England"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. R. Manoj; M. Sadeghi; E. G. Larsson"	"2022"	"Downlink Power Allocation in Massive MIMO via Deep Learning: Adversarial Attacks and Training"	""	"Ieee Transactions on Cognitive Communications and Networking"	""	""	"8"	""	"2"	"707-719"	""	""	""	""	"Jun"	""	""	"Downlink Power Allocation in Massive MIMO via Deep Learning: Adversarial Attacks and Training"	"Ieee T Cogn Commun"	"2332-7731"	"10.1109/Tccn.2022.3147203"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000808086800023"	""	""	"adversarial attacks
adversarial training
black-box attack
deep neural networks
massive mimo
regression
white-box attack
networks"	"The successful emergence of deep learning (DL) in wireless system applications has raised concerns about new security-related challenges. One such security challenge is adversarial attacks. Although there has been much work demonstrating the susceptibility of DL-based classification tasks to adversarial attacks, regression-based problems in the context of a wireless system have not been studied so far from an attack perspective. The aim of this paper is twofold: (i) we consider a regression problem in a wireless setting and show that adversarial attacks can break the DL-based approach and (ii) we analyze the effectiveness of adversarial training as a defensive technique in adversarial settings and show that the robustness of DL-based wireless system against attacks improves significantly. Specifically, the wireless application considered in this paper is the DL-based power allocation in the downlink of a multicell massive multi-input-multi-output system, where the goal of the attack is to yield an infeasible solution by the DL model. We extend the gradient-based adversarial attacks: fast gradient sign method (FGSM), momentum iterative FGSM, and projected gradient descent method to analyze the susceptibility of the considered wireless application with and without adversarial training. We analyze the deep neural network (DNN) models performance against these attacks, where the adversarial perturbations are crafted using both the white-box and black-box attacks."	"1y4bn
Times Cited:0
Cited References Count:48"	""	"<Go to ISI>://WOS:000808086800023"	""	"Linkoping Univ, Dept Elect Engn, Div Commun Syst, S-58183 Linkoping, Sweden
Indian Inst Technol Guwahati, Dept Elect & Elect Engn, Gauhati 781039, India
Linkoping Univ, Div Commun Syst, S-58183 Linkoping, Sweden
Linkoping Univ, Dept Elect Engn ISY, S-58183 Linkoping, Sweden"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Katzir; Y. Elovici"	"2018"	"Quantifying the resilience of machine learning classifiers used for cyber security"	""	"Expert Systems with Applications"	""	""	"92"	""	""	"419-429"	""	""	""	""	"Feb"	""	""	"Quantifying the resilience of machine learning classifiers used for cyber security"	"Expert Syst Appl"	"0957-4174"	"10.1016/j.eswa.2017.09.053"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000414107100032"	""	""	"adversarial learning
classifier resilience
cyber security"	"The use of machine learning algorithms for cyber security purposes gives rise to questions of adversarial resilience, namely: Can we quantify the effort required of an adversary to manipulate a system that is based on machine learning techniques? Can the adversarial resilience of such systems be formally modeled and evaluated? Can we quantify this resilience such that different systems can be compared using empiric metrics?
Past works have demonstrated how an adversary can manipulate a system based on machine learning techniques by changing some of its inputs. However, comparatively little work has emphasized the creation of a formal method for measuring and comparing the adversarial resilience of different machine learning models to these changes.
In this work we study the adversarial resilience of detection systems based on supervised machine learning models. We provide a formal definition for adversarial resilience while focusing on multisensory fusion systems. We define the model robustness (MRB) score, a metric for evaluating the relative resilience of different models, and suggest two novel feature selection algorithms for constructing adversary aware classifiers. The first algorithm selects only features that cannot realistically be modified by the adversary, while the second algorithm allows control over the resilience versus accuracy tradeoff. Finally, we evaluate our approach with a real-life use case of dynamic malware classification using an extensive, up-to-date corpus of benign and malware executables. We demonstrate the potential of using adversary aware feature selection for building more resilient classifiers and provide empirical evidence supporting the inherent resilience of ensemble algorithms compared to single model algorithms. (C) 2017 Elsevier Ltd. All rights reserved."	"Fl3fa
Times Cited:24
Cited References Count:40"	""	"<Go to ISI>://WOS:000414107100032"	""	"Ben Gurion Univ Negev, Ben Gurion Univ, Dept Software & Informat Syst Engn, Deutsch Telekom Labs, IL-8410501 Beer Sheva, Israel"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Cao; C. Si; Q. Sun; Y. Liu; S. Li; P. Gope"	"2022"	"ABCAttack: A Gradient-Free Optimization Black-Box Attack for Fooling Deep Image Classifiers"	""	"Entropy (Basel)"	""	""	"24"	""	"3"	""	""	""	""	"2022/03/26"	"Mar 15"	""	""	"ABCAttack: A Gradient-Free Optimization Black-Box Attack for Fooling Deep Image Classifiers"	""	"1099-4300 (Electronic)
1099-4300 (Linking)"	"10.3390/e24030412"	""	""	""	""	"PMC8953161"	""	""	""	""	""	""	"35327923"	""	""	"adversarial examples
black-box attack
deep neural networks
image classification
information security"	"The vulnerability of deep neural network (DNN)-based systems makes them susceptible to adversarial perturbation and may cause classification task failure. In this work, we propose an adversarial attack model using the Artificial Bee Colony (ABC) algorithm to generate adversarial samples without the need for a further gradient evaluation and training of the substitute model, which can further improve the chance of task failure caused by adversarial perturbation. In untargeted attacks, the proposed method obtained 100%, 98.6%, and 90.00% success rates on the MNIST, CIFAR-10 and ImageNet datasets, respectively. The experimental results show that the proposed ABCAttack can not only obtain a high attack success rate with fewer queries in the black-box setting, but also break some existing defenses to a large extent, and is not limited by model structure or size, which provides further research directions for deep learning evasion attacks and defenses."	"Cao, Han
Si, Chengxiang
Sun, Qindong
Liu, Yanxiao
Li, Shancang
Gope, Prosanta
eng
U20B2050/National Natural Science Foundation of China
2019-38/Youth Innovation Team of Shaanxi Universities
Switzerland
Entropy (Basel). 2022 Mar 15;24(3). pii: e24030412. doi: 10.3390/e24030412."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35327923"	""	"Key Laboratory of Network Computing and Security, Xi'an University of Technology, Xi'an 710048, China.
National Computer Network Emergency Response Technical Team/Coordination Center of China (CNCERT/CC), Beijing 100029, China.
School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an 710049, China.
The Department of Computer Science and Creative Technology, University of the West of England, Bristol BS16 1QY, UK.
Department of Computer Science, University of Sheffield, Sheffield S10 2TN, UK."	""	""	""	""	""	""	""	""
"Journal Article"	"H. J. Zhao; Q. Tian; L. Pan; Y. Lin"	"2020"	"The technology of adversarial attacks in signal recognition"	""	"Physical Communication"	""	""	"43"	""	""	"101199"	""	""	""	""	"Dec"	""	""	"The technology of adversarial attacks in signal recognition"	"Phys Commun-Amst"	"1874-4907"	"ARTN 101199
10.1016/j.phycom.2020.101199"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000595135300001"	""	""	"adversarial attack
signal recognition
deep learning
wireless security
automatic modulation classification
deep
identification"	"The wide application of contour stellar images has helped researchers transform signal classification problems into image classification problems to solve signal recognition based on deep learning. However, deep neural networks (DNN) are quite vulnerable to adversarial examples, thus simply evaluating the adversarial attack performance on the signal sequence recognition model cannot meet the current security requirements. From the perspective of an attacker, this study converts individual signals into stellar contour images, and then generates adversarial examples to evaluate the adversarial attack impacts. The results show that whether the current input sample is a signal sequence or a converted image, the DNN is vulnerable to the threat of adversarial examples. In the selected methods, whether it is under different perturbations or signal-to-noise ratio (SNRs), the momentum iteration method has the best performance among them, and under the perturbation of 0.01, the attack performance is more than 10% higher than the fast gradient sign method. Also, to measure the invisibility of adversarial examples, the contour stellar images before and after the attack were compared to maintain a balance between the attack success rate and the attack concealment. (c) 2020 Elsevier B.V. All rights reserved."	"Oz7xw
Times Cited:3
Cited References Count:36"	""	"<Go to ISI>://WOS:000595135300001"	""	"Harbin Engn Univ, Coll Informat & Commun Engn, Harbin, Peoples R China
Harbin Engn Univ, Coll Comp Sci & Technol, Harbin, Peoples R China
Harbin Qianfan Technol Co LTD, Harbin, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Bahramali; M. Nasr; A. Houmansadr; D. Goeckel; D. Towsley"	"2021"	"Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems"	""	"Ccs '21: Proceedings of the 2021 Acm Sigsac Conference on Computer and Communications Security"	""	""	""	""	""	"126-140"	""	""	""	""	""	""	""	"Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems"	""	""	"10.1145/3460120.3484777"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000768478300011"	""	""	"wireless communication systems
adversarial examples
universal perturbations
deep neural networks
channel estimation
deep"	"There is significant enthusiasm for the employment of Deep Neural Networks (DNNs) for important tasks in major wireless communication systems: channel estimation and decoding in orthogonal frequency division multiplexing (OFDM) systems, end-to-end autoencoder system design, radio signal classification, and signal authentication. Unfortunately, DNNs can be susceptible to adversarial examples, potentially making such wireless systems fragile and vulnerable to attack. In this work, by designing robust adversarial examples that meet key criteria, we perform a comprehensive study of the threats facing DNN-based wireless systems.
We model the problem of adversarial wireless perturbations as an optimization problem that incorporates domain constraints specific to different wireless systems. This allows us to generate wireless adversarial perturbations that can be applied to wireless signals on-the-fly (i.e., with no need to know the target signals a priori), are undetectable from natural wireless noise, and are robust against removal. We show that even in the presence of significant defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communication systems are vulnerable to adversarial attacks and call into question the employment of DNNs for a number of tasks in robust wireless communication."	"Bs7xr
Times Cited:0
Cited References Count:48"	""	"<Go to ISI>://WOS:000768478300011"	""	"Univ Massachusetts, Amherst, MA 01003 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"M. Mobini; G. Kaddoum; M. Herceg"	"2022"	"Design of a SIMO Deep Learning-Based Chaos Shift Keying (DLCSK) Communication System"	""	"Sensors (Basel)"	""	""	"22"	""	"1"	""	""	""	""	"2022/01/12"	"Jan 3"	""	""	"Design of a SIMO Deep Learning-Based Chaos Shift Keying (DLCSK) Communication System"	""	"1424-8220 (Electronic)
1424-8220 (Linking)"	"10.3390/s22010333"	""	""	""	""	"PMC8749677"	""	""	""	""	""	""	"35009877"	""	""	"Lstm
chaos shift keying
deep learning
multi-antenna"	"This paper brings forward a Deep Learning (DL)-based Chaos Shift Keying (DLCSK) demodulation scheme to promote the capabilities of existing chaos-based wireless communication systems. In coherent Chaos Shift Keying (CSK) schemes, we need synchronization of chaotic sequences, which is still practically impossible in a disturbing environment. Moreover, the conventional Differential Chaos Shift Keying (DCSK) scheme has a drawback, that for each bit, half of the bit duration is spent sending non-information bearing reference samples. To deal with this drawback, a Long Short-Term Memory (LSTM)-based receiver is trained offline, using chaotic maps through a finite number of channel realizations, and then used for classifying online modulated signals. We presented that the proposed receiver can learn different chaotic maps and estimate channels implicitly, and then retrieves the transmitted messages without any need for chaos synchronization or reference signal transmissions. Simulation results for both the AWGN and Rayleigh fading channels show a remarkable BER performance improvement compared to the conventional DCSK scheme. The proposed DLCSK system will provide opportunities for a new class of receivers by leveraging the advantages of DL, such as effective serial and parallel connectivity. A Single Input Multiple Output (SIMO) architecture of the DLCSK receiver with excellent reliability is introduced to show its capabilities. The SIMO DLCSK benefits from a DL-based channel estimation approach, which makes this architecture simpler and more efficient for applications where channel estimation is problematic, such as massive MIMO, mmWave, and cloud-based communication systems."	"Mobini, Majid
Kaddoum, Georges
Herceg, Marijan
eng
Switzerland
Sensors (Basel). 2022 Jan 3;22(1). pii: s22010333. doi: 10.3390/s22010333."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35009877"	""	"Department of Electrical and Computer Engineering, Babol Noshirvani University of Technology, Babol 47148-71167, Iran.
Departement de Genie Electrique, University of Quebec, Ecole de Technologie Superieure, Montreal, QC H3C1K3, Canada.
Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, Josip Juraj Strossmayer University of Osijek, 31000 Osijek, Croatia."	""	""	""	""	""	""	""	""
"Journal Article"	"X. T. Zheng; B. Wang; L. Xie"	"2019"	"Synthetic Dynamic PMU Data Generation: A Generative Adversarial Network Approach"	""	"2019 International Conference on Smart Grid Synchronized Measurements and Analytics (Sgsma)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Synthetic Dynamic PMU Data Generation: A Generative Adversarial Network Approach"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000494746200039"	""	""	"synthetic pmu data
generative adversarial nets (gan)"	"This paper concerns with the production of synthetic phasor measurement unit (PMU) data for research and education purposes. Due to the confidentiality of real PMU data and no public access to the real power systems infrastructure information, the lack of credible realistic data becomes a growing concern. Instead of constructing synthetic power grids and then producing synthetic PMU measurement data by time simulations, we propose a model-free approach to directly generate synthetic PMU data. we train the generative adversarial network (GAN) with real PMU data, which can be used to generate synthetic PMU data capturing the system dynamic behaviors. To validate the sequential generation by GAN to mimic PMU data, we theoretically analyze GAN's capacity of learning system dynamics. Further by evaluating the synthetic PMU data by a proposed quantitative method, we verify GAN's potential to synthesize realistic samples and meanwhile realize that GAN model in this paper still has room to improve. Moreover it is the first time that such generative model is applied to synthesize PMU data."	"Bo1bu
Times Cited:2
Cited References Count:17"	""	"<Go to ISI>://WOS:000494746200039"	""	"Texas A&M Univ, Dept Elect Engn, College Stn, TX 77843 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Z. Pei; K. Ren; Y. Q. Yang; C. Liu; T. Qin; D. S. Li"	"2021"	"Towards Generating Real-World Time Series Data"	""	"2021 21st Ieee International Conference on Data Mining (Icdm 2021)"	""	""	""	""	""	"469-478"	""	""	""	""	""	""	""	"Towards Generating Real-World Time Series Data"	"Ieee Data Mining"	"1550-4786"	"10.1109/Icdm51629.2021.00058"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000780454100048"	""	""	"time series
data generation
missing values"	"Time series data generation has drawn increasing attention in recent years. Several generative adversarial network (GAN) based methods have been proposed to tackle the problem usually with the assumption that the targeted time series data are well-formatted and complete. However, real-world time series (RTS) data are far away from this utopia, e.g., long sequences with variable lengths and informative missing data raise intractable challenges for designing powerful generation algorithms. In this paper, we propose a novel generative framework for RTS data - RTSGAN to tackle the aforementioned challenges. RTSGAN first learns an encoder-decoder module which provides a mapping between a time series instance and a fixed-dimension latent vector and then learns a generation module to generate vectors in the same latent space. By combining the generator and the decoder, RTSGAN is able to generate RTS which respect the original feature distributions and the temporal dynamics. To generate time series with missing values, we further equip RTSGAN with an observation embedding layer and a decide-and-generate decoder to better utilize the informative missing patterns. Experiments on the four RTS datasets show that the proposed framework outperforms the previous generation methods in terms of synthetic data utility for downstream classification and prediction tasks. Our code is available at https://seqml.github.io/rtsgan."	"Bs9gc
Times Cited:0
Cited References Count:48
IEEE International Conference on Data Mining"	""	"<Go to ISI>://WOS:000780454100048"	""	"Univ Illinois, Urbana, IL USA
Microsoft Res Asia, Shanghai, Peoples R China
Microsoft Res Asia, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Demontis; M. Melis; B. Biggio; D. Maiorca; D. Arp; K. Rieck; I. Corona; G. Giacinto; F. Roli"	"2019"	"Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection"	""	"Ieee Transactions on Dependable and Secure Computing"	""	""	"16"	""	"4"	"711-724"	""	""	""	""	"Jul-Aug"	""	""	"Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection"	"Ieee T Depend Secure"	"1545-5971"	"10.1109/Tdsc.2017.2700270"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000475350000012"	""	""	"android malware detection
static analysis
secure machine learning
computer security
classifiers
robust"	"To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks."	"Ii7cn
Times Cited:69
Cited References Count:50"	""	"<Go to ISI>://WOS:000475350000012"	""	"Univ Cagliari, Dept Elect & Elect Engn, Piazza Armi, I-09123 Cagliari, Italy
Tech Univ Carolo Wilhelmina Braunschweig, Inst Syst Secur, Rebenring 56, D-38106 Braunschweig, Germany"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Y. Fang; J. F. Wang; B. Y. Li; S. Q. Wu; Y. J. Zhou; H. Y. Huang"	"2019"	"Evading Anti-Malware Engines With Deep Reinforcement Learning"	""	"Ieee Access"	""	""	"7"	""	""	"48867-48879"	""	""	""	""	""	""	""	"Evading Anti-Malware Engines With Deep Reinforcement Learning"	"Ieee Access"	"2169-3536"	"10.1109/Access.2019.2908033"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000466214500001"	""	""	"anti-malware engines evasion
deep machine learning
malware detection
reinforcement learning"	"To reduce the risks of malicious software, malware detection methods using machine learning have received tremendous attention in recent years. Most of the conventional methods are based on supervised learning, which relies on static features with definite labels. However, recent studies have shown the models based on supervised learning are vulnerable to deliberate attacks. This work tends to expose and demonstrate the weakness in these models. A DQEAF framework using reinforcement learning to evade anti-malware engines is presented. DQEAF trains an AI agent through a neural network by constantly interacting with malware samples. Actions are a set of reasonable modifications, which do not damage samples' structure and functions. The agent selects the optimal sequence of actions to modify the malware samples, thus they can bypass the detection engines. The training process depends on the characteristics of the raw binary stream features of samples. The experiments show that the proposed method has a success rate of 75%. The efficacy of the proposed DQEAF has also been evaluated by other families of malicious software, which shows good robustness."	"Hv8es
Times Cited:22
Cited References Count:39"	""	"<Go to ISI>://WOS:000466214500001"	""	"Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Tong; M. Z. Zhang; C. Lang; Z. G. Zheng"	"2021"	"An Image Privacy Protection Algorithm Based on Adversarial Perturbation Generative Networks"	""	"Acm Transactions on Multimedia Computing Communications and Applications"	""	""	"17"	""	"2"	"1 - 14"	""	""	""	""	"Jun"	""	""	"An Image Privacy Protection Algorithm Based on Adversarial Perturbation Generative Networks"	"Acm T Multim Comput"	"1551-6857"	"Artn 43
10.1145/3381088"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000661037000002"	""	""	"social network
neural networks
privacy
adversarial perturbation generative network"	"Today, users of social platforms upload a large number of photos. These photos contain personal private information, including user identity information, which is easily gleaned by intelligent detection algorithms. To thwart this, in this work, we propose an intelligent algorithm to prevent deep neural network (DNN) detectors from detecting private information, especially human faces, while minimizing the impact on the visual quality of the image. More specifically, we design an image privacy protection algorithm by training and generating a corresponding adversarial sample for each image to defend DNN detectors. In addition, we propose an improved model based on the previous model by training an adversarial perturbation generative network to generate perturbation instead of training for each image. We evaluate and compare our proposed algorithm with other methods on wider face dataset and others by three indicators: Mean average precision, Averaged distortion, and Time spent. The results show that our method significantly interferes with DNN detectors while causing weak impact to the visual quality of images, and our improved model does speed up the generation of adversarial perturbations."	"Sr4uu
Times Cited:0
Cited References Count:29"	""	"<Go to ISI>://WOS:000661037000002"	""	"Beihang Univ, 37 Xueyuan Rd, Beijing 100191, Peoples R China
Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Gupta; I. Johri; K. Srinivasan; Y. C. Hu; S. M. Qaisar; K. Y. Huang"	"2022"	"A Systematic Review on Machine Learning and Deep Learning Models for Electronic Information Security in Mobile Networks"	""	"Sensors (Basel)"	""	""	"22"	""	"5"	""	""	""	""	"2022/03/11"	"Mar 4"	""	""	"A Systematic Review on Machine Learning and Deep Learning Models for Electronic Information Security in Mobile Networks"	""	"1424-8220 (Electronic)
1424-8220 (Linking)"	"10.3390/s22052017"	""	""	""	""	"PMC8915055"	""	""	""	""	""	""	"35271163"	""	""	"Artificial Intelligence
*Computer Communication Networks
*Deep Learning
Electronics
Machine Learning
cyber security
cyber-attacks
deep learning
information security
network
threats
vulnerabilities"	"Today's advancements in wireless communication technologies have resulted in a tremendous volume of data being generated. Most of our information is part of a widespread network that connects various devices across the globe. The capabilities of electronic devices are also increasing day by day, which leads to more generation and sharing of information. Similarly, as mobile network topologies become more diverse and complicated, the incidence of security breaches has increased. It has hampered the uptake of smart mobile apps and services, which has been accentuated by the large variety of platforms that provide data, storage, computation, and application services to end-users. It becomes necessary in such scenarios to protect data and check its use and misuse. According to the research, an artificial intelligence-based security model should assure the secrecy, integrity, and authenticity of the system, its equipment, and the protocols that control the network, independent of its generation, in order to deal with such a complicated network. The open difficulties that mobile networks still face, such as unauthorised network scanning, fraud links, and so on, have been thoroughly examined. Numerous ML and DL techniques that can be utilised to create a secure environment, as well as various cyber security threats, are discussed. We address the necessity to develop new approaches to provide high security of electronic data in mobile networks because the possibilities for increasing mobile network security are inexhaustible."	"Gupta, Chaitanya
Johri, Ishita
Srinivasan, Kathiravan
Hu, Yuh-Chung
Qaisar, Saeed Mian
Huang, Kuo-Yi
eng
Review
Systematic Review
Switzerland
Sensors (Basel). 2022 Mar 4;22(5). pii: s22052017. doi: 10.3390/s22052017."	""	"https://www.ncbi.nlm.nih.gov/pubmed/35271163"	""	"School of Computer Science and Engineering, Vellore Institute of Technology, Vellore 632014, India.
School of Information Technology and Engineering, Vellore Institute of Technology, Vellore 632014, India.
Department of Mechanical and Electromechanical Engineering, National ILan University, Yilan 26047, Taiwan.
Electrical and Computer Engineering Department, Effat University, Jeddah 22332, Saudi Arabia.
Department of Bio-Industrial Mechatronic Engineering, National Chung Hsing University, Taichung 402, Taiwan."	""	""	""	""	""	""	""	""
"Journal Article"	"D. Dasgupta; Z. Akhtar; S. Sen"	"2022"	"Machine learning in cybersecurity: a comprehensive survey"	""	"Journal of Defense Modeling and Simulation-Applications Methodology Technology-Jdms"	""	""	"19"	""	"1"	"57-106"	""	""	""	""	"Jan"	""	""	"Machine learning in cybersecurity: a comprehensive survey"	"J Def Model Simul-Ap"	"1548-5129"	"Artn 1548512920951275
10.1177/1548512920951275"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000571646600001"	""	""	"cybersecurity
machine learning
intrusion detection
deep neural network
adversarial examples
adversarial learning
defensive techniques
intrusion detection system
false data injection
anomaly detection
game-theory
attacks
perspectives
challenges
threats"	"Today's world is highly network interconnected owing to the pervasiveness of small personal devices (e.g., smartphones) as well as large computing devices or services (e.g., cloud computing or online banking), and thereby each passing minute millions of data bytes are being generated, processed, exchanged, shared, and utilized to yield outcomes in specific applications. Thus, securing the data, machines (devices), and user's privacy in cyberspace has become an utmost concern for individuals, business organizations, and national governments. In recent years, machine learning (ML) has been widely employed in cybersecurity, for example, intrusion or malware detection and biometric-based user authentication. However, ML algorithms are vulnerable to attacks both in the training and testing phases, which usually leads to remarkable performance decreases and security breaches. Comparatively, limited studies have been conducted to understand the essence and degree of the vulnerabilities of ML techniques against security threats and their defensive mechanisms. It is imperative to systematize recent works related to cybersecurity using ML to seek the attention of researchers, scientists, and engineers. Therefore, in this paper, we provide a comprehensive survey of the works that have been carried out most recently (from 2013 to 2018) on ML in cybersecurity, describing the basics of cyber-attacks and corresponding defenses, the basics of the most commonly used ML algorithms, and proposed ML and data mining schemes for cybersecurity in terms of features, dimensionality reduction, and classification/detection techniques. In this context, this article also provides an overview of adversarial ML, including the security characteristics of deep learning methods. Finally, open issues and challenges in cybersecurity are highlighted and potential future research directions are discussed."	"Sp. Iss. SI
Zd6sy
Times Cited:11
Cited References Count:255"	""	"<Go to ISI>://WOS:000571646600001"	""	"Univ Memphis, Ctr Informat Assurance CfIA, Memphis, TN 38152 USA
State Univ New York SUNY Polytech Inst, Dept Network & Comp Secur, Utica, NY USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"H. Wen; Y. Wu; C. M. Yang; H. C. Duan; S. Yu"	"2020"	"A Unified Federated Learning Framework for Wireless Communications: towards Privacy, Efficiency, and Security"	""	"Ieee Infocom 2020 - Ieee Conference on Computer Communications Workshops (Infocom Wkshps)"	""	""	""	""	""	"653-658"	""	""	""	""	""	""	""	"A Unified Federated Learning Framework for Wireless Communications: towards Privacy, Efficiency, and Security"	"Ieee Conf Comput"	"2159-4228"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000593830400109"	""	""	"federated learning
data augmentation
distillation
conditional variational autoencoder
communication efficient
privacy preserving
byzantine robustness"	"Training high-quality machine learning models on distributed systems is a critical issue to achieve edge intelligence in wireless communications. Conventional data-driven machine learning approaches are infeasible due to non-IID data caused by privacy issues and the limited communication resources in wireless networks. Besides, considering the complex user identities, the training process also faces the challenges of Byzantine devices, which can inject poisoning information into models. In this paper, we propose a two-step federated learning framework, robust federated augmentation and distillation (RFA-RFD), to enable privacy-preserving, communication-efficient, and Byzantine tolerant on-device machine learning in wireless communications. RFA is a method to tackle the problem of non-IID local data, which firstly trains local data generators on edge devices, then trains a global generator in the cloud server according to the IID dataset generated by the uploaded local generators, and finally, devices rectify non-IID dataset by downloading the global generator. After obtaining IID local data in edge devices, RFD is implemented to improve the performance of local models, in which devices only share the local information of models' outputs to reduce communication overhead. By employing a detection and discard mechanism in both RFA and RFD, our framework achieves robustness to the influence of Byzantine devices. Experiments show the effectiveness of RFA-RFD on preserving privacy, correcting non-IID data, reducing communication overhead, and resisting Byzantine devices, without much loss of accuracy compared with existing state-of-the-art methods."	"Bq4qe
Times Cited:5
Cited References Count:19
IEEE Conference on Computer Communications Workshops"	""	"<Go to ISI>://WOS:000593830400109"	""	"Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China
Univ Elect Sci & Technol China, Natl Key Lab Commun, Chengdu, Peoples R China
Univ Technol Sydney, Sch Comp Sci, Sydney, NSW, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Wu; J. R. He"	"2021"	"Indirect Invisible Poisoning Attacks on Domain Adaptation"	""	"Kdd '21: Proceedings of the 27th Acm Sigkdd Conference on Knowledge Discovery & Data Mining"	""	""	""	""	""	"1852-1862"	""	""	""	""	""	""	""	"Indirect Invisible Poisoning Attacks on Domain Adaptation"	""	""	"10.1145/3447548.3467214"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000749556801087"	""	""	"domain adaptation
domain discrepancy
poisoning attack"	"Unsupervised domain adaptation has been successfully applied across multiple high-impact applications, since it improves the generalization performance of a learning algorithm when the source and target domains are related. However, the adversarial vulnerability of domain adaptation models has largely been neglected. Most existing unsupervised domain adaptation algorithms might be easily fooled by an adversary, resulting in deteriorated prediction performance on the target domain, when transferring the knowledge from a maliciously manipulated source domain.
To demonstrate the adversarial vulnerability of existing domain adaptation techniques, in this paper, we propose a generic data poisoning attack framework named I2Attack for domain adaptation with the following properties: (1) perceptibly unnoticeable: all the poisoned inputs are natural-looking; (2) adversarially indirect: only source examples are maliciously manipulated; (3) algorithmically invisible: both source classification error and marginal domain discrepancy between source and target domains will not increase. Specifically, it aims to degrade the overall prediction performance on the target domain by maximizing the label-informed domain discrepancy over both input feature space and class-label space between source and target domains. Within this framework, a family of practical poisoning attacks are presented to fool the existing domain adaptation algorithms associated with different discrepancy measures. Extensive experiments on various domain adaptation benchmarks confirm the effectiveness and computational efficiency of our proposed I2Attack framework."	"Bs6lu
Times Cited:0
Cited References Count:40"	""	"<Go to ISI>://WOS:000749556801087"	""	"Univ Illinois, Champaign, IL 61820 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Liang; M. Q. Su; W. You; W. C. Shi; G. Yang"	"2016"	"Cracking Classifiers for Evasion: A Case Study on the Google's Phishing Pages Filter"	""	"Proceedings of the 25th International Conference on World Wide Web (Www'16)"	""	""	""	""	""	"345-356"	""	""	""	""	""	""	""	"Cracking Classifiers for Evasion: A Case Study on the Google's Phishing Pages Filter"	""	""	"10.1145/2872427.2883060"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000461467400030"	""	""	"phishing detection
machine learning
classifiers
cracking
collision attacks
evasion attacks
security
systems"	"Various classifiers based on the machine learning techniques have been widely used in security applications. Meanwhile, they also became an attack target of adversaries. Many existing studies have paid much attention to the evasion attacks on the online classifiers and discussed defensive methods. However, the security of the classifiers deployed in the client environment has not got the attention it deserves. Besides, earlier studies only concentrated on the experimental classifiers developed for research purposes only. The security of widely-used commercial classifiers still remains unclear. In this paper, we use the Google's phishing pages filter (GPPF), a classifier deployed in the Chrome browser which owns over one billion users, as a case to investigate the security challenges for the client-side classifiers. We present a new attack methodology targeting on client-side classifiers, called classifiers cracking. With the methodology, we successfully cracked the classification model of GPPF and extracted sufficient knowledge can be exploited for evasion attacks, including the classification algorithm, scoring rules and features, etc. Most importantly, we completely reverse engineered 84.8% scoring rules, covering most of high-weighted rules. Based on the cracked information, we performed two kinds of evasion attacks to GPPF, using 100 real phishing pages for the evaluation purpose. The experiments show that all the phishing pages (100%) can be easily manipulated to bypass the detection of GPPF. Our study demonstrates that the existing client-side classifiers are very vulnerable to classifiers cracking attacks."	"Bm2tn
Times Cited:21
Cited References Count:54"	""	"<Go to ISI>://WOS:000461467400030"	""	"Renmin Univ China, MOE, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China
Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Biggio; G. Fumera; F. Roli"	"2014"	"Pattern Recognition Systems under Attack: Design Issues and Research Challenges"	""	"International Journal of Pattern Recognition and Artificial Intelligence"	""	""	"28"	""	"7"	""	""	""	""	""	"Nov"	""	""	"Pattern Recognition Systems under Attack: Design Issues and Research Challenges"	"Int J Pattern Recogn"	"0218-0014"	"Artn 1460002
10.1142/S0218001414600027"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000343996900002"	""	""	"secure pattern recognition
adversarial learning
robust classification
multiple classifier systems
security evaluation
robustness"	"We analyze the problem of designing pattern recognition systems in adversarial settings, under an engineering viewpoint, motivated by their increasing exploitation in security-sensitive applications like spam and malware detection, despite their vulnerability to potential attacks has not yet been deeply understood. We first review previous work and report examples of how a complex system may be evaded either by leveraging on trivial vulnerabilities of its untrained components, e.g. parsing errors in the pre-processing steps, or by exploiting more subtle vulnerabilities of learning algorithms. We then discuss the need of exploiting both reactive and proactive security paradigms complementarily to improve the security by design. Our ultimate goal is to provide some useful guidelines for improving the security of pattern recognition in adversarial settings, and to suggest related open issues to foster research in this area."	"Sp. Iss. SI
As0wp
Times Cited:50
Cited References Count:74"	""	"<Go to ISI>://WOS:000343996900002"	""	"Univ Cagliari, Dept Elect & Elect Engn, I-09123 Cagliari, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Kim; Y. E. Sagduyu; T. Erpek; K. Davaslioglu; S. Ulukus"	"2021"	"Channel Effects on Surrogate Models of Adversarial Attacks against Wireless Signal Classifiers"	""	"Ieee International Conference on Communications (Icc 2021)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Channel Effects on Surrogate Models of Adversarial Attacks against Wireless Signal Classifiers"	"Ieee Icc"	"1550-3607"	"10.1109/Icc42927.2021.9500374"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000719386000130"	""	""	""	"We consider a wireless communication system that consists of a background emitter, a transmitter, and an adversary. The transmitter is equipped with a deep neural network (DNN) classifier for detecting the ongoing transmissions from the background emitter and transmits a signal if the spectrum is idle. Concurrently, the adversary trains its own DNN classifier as the surrogate model by observing the spectrum to detect the ongoing transmissions of the background emitter and generate adversarial attacks to fool the transmitter into misclassifying the channel as idle. This surrogate model may differ from the transmitter's classifier significantly because the adversary and the transmitter experience different channels from the background emitter and therefore their classifiers are trained with different distributions of inputs. This system model may represent a setting where the background emitter is a primary user, the transmitter is a secondary user, and the adversary is trying to fool the secondary user to transmit even though the channel is occupied by the primary user. We consider different topologies to investigate how different surrogate models that are trained by the adversary (depending on the differences in channel effects experienced by the adversary) affect the performance of the adversarial attack. The simulation results show that the surrogate models that are trained with different distributions of channel-induced inputs severely limit the attack performance and indicate that the transferability of adversarial attacks is neither readily available nor straightforward to achieve since surrogate models for wireless applications may significantly differ from the target model depending on channel effects."	"Bs4hx
Times Cited:0
Cited References Count:29
IEEE International Conference on Communications"	""	"<Go to ISI>://WOS:000719386000130"	""	"Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA
Intelligent Automat Inc, Rockville, MD 20855 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Kim; Y. E. Sagduyu; K. Davaslioglu; T. Erpek; S. Ulukus"	"2020"	"Over-the-Air Adversarial Attacks on Deep Learning Based Modulation Classifier over Wireless Channels"	""	"2020 54th Annual Conference on Information Sciences and Systems (Ciss)"	""	""	""	""	""	"330-335"	""	""	""	""	""	""	""	"Over-the-Air Adversarial Attacks on Deep Learning Based Modulation Classifier over Wireless Channels"	""	""	"10.1109/Ciss48834.2020.1570617416"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000712170800058"	""	""	""	"We consider a wireless communication system that consists of a transmitter, a receiver, and an adversary. The transmitter transmits signals with different modulation types, while the receiver classifies its received signals to modulation types using a deep learning-based classifier. In the meantime, the adversary makes over-the-air transmissions that are received as superimposed with the transmitter's signals to fool the classifier at the receiver into making errors. While this evasion attack has received growing interest recently, the channel effects from the adversary to the receiver have been ignored so far such that the previous attack mechanisms cannot be applied under realistic channel effects. In this paper, we present how to launch a realistic evasion attack by considering channels from the adversary to the receiver. Our results show that modulation classification is vulnerable to an adversarial attack over a wireless channel that is modeled as Rayleigh fading with path loss and shadowing. We present various adversarial attacks with respect to availability of information about channel, transmitter input, and classifier architecture. First, we present two types of adversarial attacks, namely a targeted attack (with minimum power) and non-targeted attack that aims to change the classification to a target label or to any other label other than the true label, respectively. Both are white-box attacks that are transmitter input-specific and use channel information. Then we introduce an algorithm to generate adversarial attacks using limited channel information where the adversary only knows the channel distribution. Finally, we present a black-box universal adversarial perturbation (UAP) attack where the adversary has limited knowledge about both channel and transmitter input. By accounting for different levels of information availability, we show the vulnerability of modulation classifier to over-the-air adversarial attacks."	"Bs3kn
Times Cited:20
Cited References Count:20"	""	"<Go to ISI>://WOS:000712170800058"	""	"Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA
Intelligent Automat Inc, Rockville, MD 20855 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Kim; Y. E. Sagduyu; T. Erpek; K. Davaslioglu; S. Ulukus"	"2020"	"Adversarial Attacks with Multiple Antennas Against Deep Learning-Based Modulation Classifiers"	""	"2020 Ieee Globecom Workshops (Gc Wkshps)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"Adversarial Attacks with Multiple Antennas Against Deep Learning-Based Modulation Classifiers"	"Ieee Globe Work"	"2166-0069"	"10.1109/GCWkshps50303.2020.9367473"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000662202100082"	""	""	""	"We consider a wireless communication system, where a transmitter sends signals to a receiver with different modulation types while the receiver classifies the modulation types of the received signals using its deep learning-based classifier. Concurrently, an adversary transmits adversarial perturbations using its multiple antennas to fool the classifier into misclassifying the received signals. From the adversarial machine learning perspective, we show how to utilize multiple antennas at the adversary to improve the adversarial (evasion) attack performance. Two main points are considered while exploiting the multiple antennas at the adversary, namely the power allocation among antennas and the utilization of channel diversity. First, we show that multiple independent adversaries, each with a single antenna cannot improve the attack performance compared to a single adversary with multiple antennas using the same total power. Then, we consider various ways to allocate power among multiple antennas at a single adversary such as allocating power to only one antenna, and proportional or inversely proportional to the channel gain. By utilizing channel diversity, we introduce an attack to transmit the adversarial perturbation through the channel with the largest channel gain at the symbol level. We show that this attack reduces the classifier accuracy significantly compared to other attacks under different channel conditions in terms of channel variance and channel correlation across antennas. Also, we show that the attack success improves significantly as the number of antennas increases at the adversary that can better utilize channel diversity to craft adversarial attacks."	"Br6nk
Times Cited:2
Cited References Count:25
IEEE Globecom Workshops"	""	"<Go to ISI>://WOS:000662202100082"	""	"Univ Maryland, College Pk, MD 20742 USA
Intelligent Automat Inc, Rockville, MD USA
Virginia Tech, Hume Ctr, Arlington, VA USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. G. Chachlakis; P. P. Markopoulos"	"2021"	"Novel Algorithms for Lp-Quasi-Norm Principal-Component Analysis"	""	"28th European Signal Processing Conference (Eusipco 2020)"	""	""	""	""	""	"1045-1049"	""	""	""	""	""	""	""	"Novel Algorithms for Lp-Quasi-Norm Principal-Component Analysis"	"Eur Signal Pr Conf"	"2076-1465"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000632622300210"	""	""	"principal-component analysis
pca
l1-pca
lp-norm
quasi-norm
lp-quasi-norm
outliers
robustness"	"We consider outlier-resistant Lp-quasi-norm (p <= 1) Principal-Component Analysis (Lp-PCA) of a D-by-N matrix. It was recently shown that Lp-PCA ( p <= 1) admits an exact solution by means of combinatorial optimization with computational cost exponential in N. To date, apart from the exact solution to Lp-PCA (p <= 1), there exists no converging algorithm of lower cost that approximates its exact solution. In this work, we (i) propose a novel and converging algorithm that approximates the exact solution to Lp-PCA with significantly lower computational cost than that of the exact solver, (ii) conduct formal complexity and convergence analyses, and (iii) propose a multi-component solver based on subspace-deflation. Numerical studies on matrix reconstruction and medical-data classification illustrate the outlier resistance of Lp-PCA."	"Br1kb
Times Cited:0
Cited References Count:41
European Signal Processing Conference"	""	"<Go to ISI>://WOS:000632622300210"	""	"Rochester Inst Technol, Dept Elect & Microelect Engn, Rochester, NY 14623 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. Kim; Y. E. Sagduyu; K. Davaslioglu; T. Erpek; S. Ulukus"	"2020"	"How to Make 5G Communications "Invisible": Adversarial Machine Learning for Wireless Privacy"	""	"2020 54th Asilomar Conference on Signals, Systems, and Computers"	""	""	""	""	""	"763-767"	""	""	""	""	""	""	""	"How to Make 5G Communications "Invisible": Adversarial Machine Learning for Wireless Privacy"	"Conf Rec Asilomar C"	"1058-6393"	"10.1109/Ieeeconf51394.2020.9443500"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000681731800148"	""	""	"physical-layer
information
networks
security"	"We consider the problem of hiding wireless communications from an eavesdropper that employs a deep learning (DL) classifier to detect whether any transmission of interest is present or not. There exists one transmitter that transmits to its receiver in the presence of an eavesdropper, while a cooperative jammer (CJ) transmits carefully crafted adversarial perturbations over the air to fool the eavesdropper into classifying the received superposition of signals as noise. The CJ puts an upper bound on the strength of perturbation signal to limit its impact on the bit error rate (BER) at the receiver. We show that this adversarial perturbation causes the eavesdropper to misclassify the received signals as noise with high probability while increasing the BER only slightly. On the other hand, the CJ cannot fool the eavesdropper by simply transmitting Gaussian noise as in conventional jamming and instead needs to craft perturbation signals built by adversarial machine learning to enable covert communications. Our results show that signals with different modulation types and eventually 5G communications can be effectively hidden from an eavesdropper even if it is equipped with a DL classifier to detect transmissions."	"Bs0gn
Times Cited:2
Cited References Count:28
Conference Record of the Asilomar Conference on Signals Systems and Computers"	""	"<Go to ISI>://WOS:000681731800148"	""	"Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA
Intelligent Automat Inc, Rockville, MD 20855 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Carlini; D. Wagner"	"2018"	"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text"	""	"2018 Ieee Symposium on Security and Privacy Workshops (Spw 2018)"	""	""	""	""	""	"1-7"	""	""	""	""	""	""	""	"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text"	""	""	"10.1109/Spw.2018.00009"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000674762400001"	""	""	""	"We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100% success rate. The feasibility of this attack introduce a new domain to study adversarial examples."	"Br8zo
Times Cited:215
Cited References Count:40"	""	"<Go to ISI>://WOS:000674762400001"	""	"Univ Calif Berkeley, Berkeley, CA 94720 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"U. T. Tantipongpipat; C. Waites; D. Boob; A. A. Siva; R. Cummings"	"2021"	"Differentially private synthetic mixed-type data generation for unsupervised learning"	""	"Intelligent Decision Technologies-Netherlands"	""	""	"15"	""	"4"	"779-807"	""	""	""	""	""	""	""	"Differentially private synthetic mixed-type data generation for unsupervised learning"	"Intell Decis Technol"	"1872-4981"	"10.3233/Idt-210195"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000749999000019"	""	""	"differential privacy
synthetic data generation
generative adversarial networks
mixed-type data"	"We introduce the DP-auto-GAN framework for synthetic data generation, which combines the low dimensional representation of autoencoders with the flexibility of Generative Adversarial Networks (GANs). This framework can be used to take in raw sensitive data and privately train a model for generating synthetic data that will satisfy similar statistical properties as the original data. This learned model can generate an arbitrary amount of synthetic data, which can then be freely shared due to the post-processing guarantee of differential privacy. Our framework is applicable to unlabeled mixed-type data, that may include binary, categorical, and real-valued data. We implement this framework on both binary data (MIMIC-III) and mixed-type data (ADULT), and compare its performance with existing private algorithms on metrics in unsupervised settings. We also introduce a new quantitative metric able to detect diversity, or lack thereof, of synthetic data."	"Yr4yx
Times Cited:0
Cited References Count:54"	""	"<Go to ISI>://WOS:000749999000019"	""	"Twitter, San Francisco, CA USA
Stanford Univ, Stanford, CA 94305 USA
Southern Methodist Univ, Dallas, TX USA
Amazon, Seattle, WA USA
Columbia Univ, 500 W 120th St, New York, NY 10027 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"F. Bergadano"	"2019"	"Keyed learning: An adversarial learning framework-formalization, challenges, and anomaly detection applications"	""	"Etri Journal"	""	""	"41"	""	"5"	"608-618"	""	""	""	""	"Oct"	""	""	"Keyed learning: An adversarial learning framework-formalization, challenges, and anomaly detection applications"	"Etri J"	"1225-6463"	"10.4218/etrij.2019-0140"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000490485800006"	""	""	"adversarial learning
anomaly detection
keyed learning
classification"	"We propose a general framework for keyed learning, where a secret key is used as an additional input of an adversarial learning system. We also define models and formal challenges for an adversary who knows the learning algorithm and its input data but has no access to the key value. This adversarial learning framework is subsequently applied to a more specific context of anomaly detection, where the secret key finds additional practical uses and guides the entire learning and alarm-generating procedure."	"Je1wf
Times Cited:1
Cited References Count:41"	""	"<Go to ISI>://WOS:000490485800006"	""	"Univ Torino, Dipartimento Informat, Turin, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"S. O. Arik; T. Pfister"	"2020"	"ProtoAttend: Attention-Based Prototypical Learning"	""	"Journal of Machine Learning Research"	""	""	"21"	""	""	"210:1-210:35"	""	""	""	""	""	""	""	"ProtoAttend: Attention-Based Prototypical Learning"	"J Mach Learn Res"	"1532-4435"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000590012200001"	""	""	"sample-based interpretability
confidence
attention
explainable deep learning
prototypical"	"We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. Protoattend yields superior results in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a sample-based interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All these can be achieved with minimal additional test time and a practically viable training time computational cost."	"Os2qo
Times Cited:2
Cited References Count:45"	""	"<Go to ISI>://WOS:000590012200001"	""	"Google Cloud AI, Sunnyvale, CA 94043 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"B. W. Xi"	"2020"	"Adversarial machine learning for cybersecurity and computer vision: Current developments and challenges"	""	"Wiley Interdisciplinary Reviews-Computational Statistics"	""	""	"12"	""	"5"	""	""	""	""	""	"Sep"	""	""	"Adversarial machine learning for cybersecurity and computer vision: Current developments and challenges"	"Wires Comput Stat"	"1939-0068"	"ARTN e1511
10.1002/wics.1511"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000527035200001"	""	""	"adversarial machine learning
cybersecurity
deep learning
evasion attack
poisoning attack
intrusion detection
attacks
security
patterns
privacy
models
noise"	"We provide a comprehensive overview of adversarial machine learning focusing on two application domains, that is, cybersecurity and computer vision. Research in adversarial machine learning addresses a significant threat to the wide application of machine learning techniques-they are vulnerable to carefully crafted attacks from malicious adversaries. For example, deep neural networks fail to correctly classify adversarial images, which are generated by adding imperceptible perturbations to clean images. We first discuss three main categories of attacks against machine learning techniques-poisoning attacks, evasion attacks, and privacy attacks. Then the corresponding defense approaches are introduced along with the weakness and limitations of the existing defense approaches. We notice adversarial samples in cybersecurity and computer vision are fundamentally different. While adversarial samples in cybersecurity often have different properties/distributions compared with training data, adversarial images in computer vision are created with minor input perturbations. This further complicates the development of robust learning techniques, because a robust learning technique must withstand different types of attacks.
This article is categorized under:
Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification
Statistical Learning and Exploratory Methods of the Data Sciences > Deep Learning
Statistical and Graphical Methods of Data Analysis > Robust Methods"	"Nd2qa
Times Cited:1
Cited References Count:125"	""	"<Go to ISI>://WOS:000527035200001"	""	"Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Z. Ma; X. Z. Zhang; W. Sun; X. J. Zhu"	"2019"	"Policy Poisoning in Batch Reinforcement Learning and Control"	""	"Advances in Neural Information Processing Systems 32 (Nips 2019)"	""	""	"32"	""	""	""	""	""	""	""	""	""	""	"Policy Poisoning in Batch Reinforcement Learning and Control"	"Adv Neur In"	"1049-5258"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000535866906025"	""	""	""	"We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set, and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens, and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks, and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed, and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks."	"Bp0qe
Times Cited:5
Cited References Count:22
Advances in Neural Information Processing Systems"	""	"<Go to ISI>://WOS:000535866906025"	""	"Univ Wisconsin, Madison, WI 53706 USA
Microsoft Res New York, New York, NY USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Rakhsha; G. Radanovic; R. Devidze; X. J. Zhu; A. Singla"	"2021"	"Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks"	""	"Journal of Machine Learning Research"	""	""	"22"	""	""	"210:1-210:45"	""	""	""	""	""	""	""	"Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks"	"J Mach Learn Res"	"1532-4435"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000706876200001"	""	""	"training-time adversarial attacks
reinforcement learning
policy teaching
environment poisoning
security threat"	"We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice."	"Wg3dk
Times Cited:0
Cited References Count:77"	""	"<Go to ISI>://WOS:000706876200001"	""	"Max Planck Inst Software Syst MPI SWS, D-66123 Saarbrucken, Germany
Univ Wisconsin, Madison, WI 53706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"A. Gupta; M. Sellathurai"	"2021"	"End-to-End Learning-Based Framework for Amplify-and-Forward Relay Networks"	""	"Ieee Access"	""	""	"9"	""	""	"81660-81677"	""	""	""	""	""	""	""	"End-to-End Learning-Based Framework for Amplify-and-Forward Relay Networks"	"Ieee Access"	"2169-3536"	"10.1109/Access.2021.3085901"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000673964000001"	""	""	"relay networks (telecommunication)
artificial neural networks
knowledge engineering
modulation
decoding
signal to noise ratio
convergence
amplify-and-forward
autoencoder
feed-forward neural networks
learning
relay networks
resource-allocation
i/q imbalance
autoencoder
transmission
performance
design
system"	"We study end-to-end learning-based frameworks for amplify-and-forward (AF) relay networks, with and without the channel state information (CSI) knowledge. The designed framework resembles an autoencoder (AE) where all the components of the neural network (NN)-based source and destination nodes are optimized together in an end-to-end manner, and the signal transmission takes place with an AF relay node. Unlike the literature that employs an NN-based relay node with full CSI knowledge, we consider a conventional relay node that only amplifies the received signal using CSI gains. Without the CSI knowledge, we employ power normalization-based amplification that normalizes the transmission power of each block of symbols. We propose and compare symbol-wise and bit-wise AE frameworks by minimizing categorical and binary cross-entropy loss that maximizes the symbol-wise and bit-wise mutual information (MI), respectively. We determine the estimated MI and examine the convergence of both AE frameworks with signal-to-noise ratio (SNR). For both these AE frameworks, we design coded modulation and differential coded modulation, depending upon the availability of CSI at the destination node, that obtains symbols in 2n-dimensions, where n is the block length. To explain the properties of the 2n-dimensional designs, we utilize various metrics like minimum Euclidean distance, normalized second-order and fourth-order moments, and constellation figures of merit. We show that both these AE frameworks obtain similar spherical coded-modulation designs in 2n-dimensions, and bit-wise AE that inherently obtains the optimal bit-labeling outperforms symbol-wise AE (with faster convergence under low SNR) and the conventional AF relay network with a considerable SNR margin."	"Tk1yz
Times Cited:2
Cited References Count:40"	""	"<Go to ISI>://WOS:000673964000001"	""	"Heriot Watt Univ, Dept Engn & Phys Sci EPS, Edinburgh EH14 4AS, Midlothian, Scotland"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. G. Chachlakis; P. P. Markopoulos"	"2019"	"COMBINATORIAL SEARCH FOR THE Lp-NORM PRINCIPAL COMPONENT OF A MATRIX"	""	"Conference Record of the 2019 Fifty-Third Asilomar Conference on Signals, Systems & Computers"	""	""	""	""	""	"1611-1615"	""	""	""	""	""	""	""	"COMBINATORIAL SEARCH FOR THE Lp-NORM PRINCIPAL COMPONENT OF A MATRIX"	"Conf Rec Asilomar C"	"1058-6393"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000544249200308"	""	""	"principal-component analysis
pca
l1-pca
lp-norm
lp-pca
outliers
robustness"	"We study Lp-norm Principal-Component Analysis (Lp-PCA) of a matrix. For p = 2 (standard PCA), the problem can be solved with standard Singular-Value Decomposition (SVD). For p = 1 (L1-PCA), the problem was recently solved exactly and approximately with efficient iterative algorithms. For general values of p, the exact solution to Lp-PCA remains to date unknown. In this work, for the first time in the literature, we prove that, for p <= 1, Lp-PCA can be solved exactly through combinatorial optimization and present the first exact solver. Our experimental studies on medical data demonstrate the significant robustness of Lp-PCA, p <= 1, against outliers."	"Bp2pr
Times Cited:1
Cited References Count:37
Conference Record of the Asilomar Conference on Signals Systems and Computers"	""	"<Go to ISI>://WOS:000544249200308"	""	"Rochester Inst Technol, Dept Elect & Microelect Engn, Rochester, NY 14623 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Severini; R. N. Mysore; V. Sekar; S. Banerjee; M. K. Reiter"	"2021"	"The Netivus Manifesto: Making Collaborative Network Management Easier for the Rest of Us"	""	"Acm Sigcomm Computer Communication Review"	""	""	"51"	""	"2"	"10-17"	""	""	""	""	"Apr"	""	""	"The Netivus Manifesto: Making Collaborative Network Management Easier for the Rest of Us"	"Acm Sigcomm Comp Com"	"0146-4833"	"10.1145/3464994.3464997"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000650998200002"	""	""	"collaborative debugging
manageability"	"We study operational issues faced by Small and Medium Enterprise (SME) network owners and find that SME network management practices have stagnated over the past decade, despite many recent advances in network management. Many of these advances target hyperscalers and ISPs and cannot be directly applied to SME networks that are operated with vastly different constraints. In our work, we outline these constraints and explain how they impact challenges around debugging, namely: representing, reproducing, and remediating network problems. This article takes a fresh look at these challenges in the light of SME practices around collaborative debugging and presents a roadmap aimed to help resolve SME operational issues quickly."	"Sc9ql
Times Cited:0
Cited References Count:24"	""	"<Go to ISI>://WOS:000650998200002"	""	"Carnegie Mellon Univ, Pittsburgh, PA 15213 USA
VMware Res, Palo Alto, CA USA
Duke Univ, Durham, NC 27706 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Sitawarin; D. Wagner"	"2020"	"Minimum-Norm Adversarial Examples on KNN and KNN-Based Models"	""	"2020 Ieee Symposium on Security and Privacy Workshops (Spw 2020)"	""	""	""	""	""	"34-40"	""	""	""	""	""	""	""	"Minimum-Norm Adversarial Examples on KNN and KNN-Based Models"	""	""	"10.1109/Spw50608.2020.00023"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000674766800005"	""	""	""	"We study the robustness against adversarial examples of kNN classifiers and classifiers that combine kNN with neural networks. The main difficulty lies in the fact that finding an optimal attack on kNN is intractable for typical datasets. In this work, we propose a gradient-based attack on kNN and kNN-based defenses, inspired by the previous work by Sitawarin & Wagner [1]. We demonstrate that our attack outperforms their method on all of the models we tested with only a minimal increase in the computation time. The attack also beats the state-of-the-art attack [2] on kNN when k > 1 using less than 1% of its running time. We hope that this attack can be used as a new baseline for evaluating the robustness of kNN and its variants."	"Br8zr
Times Cited:0
Cited References Count:23"	""	"<Go to ISI>://WOS:000674766800005"	""	"Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. Usynin; A. Ziller; M. Makowski; R. Braren; D. Rueckert; B. Glocker; G. Kaissis; J. Passerat-Palmbach"	"2021"	"Adversarial interference and its mitigations in privacy-preserving collaborative machine learning"	""	"Nature Machine Intelligence"	""	""	"3"	""	"9"	"749-758"	""	""	""	""	"Sep"	""	""	"Adversarial interference and its mitigations in privacy-preserving collaborative machine learning"	"Nat Mach Intell"	""	"10.1038/s42256-021-00390-3"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000696824400004"	""	""	"attacks"	"When the training data for machine learning are highly personal or sensitive, collaborative approaches can help a collective of stakeholders to train a model together without having to share any data. But there are still risks to the privacy of the data. This Perspective provides an overview of potential attacks on collaborative machine learning and how these threats could be addressed.
Despite the rapid increase of data available to train machine-learning algorithms in many domains, several applications suffer from a paucity of representative and diverse data. The medical and financial sectors are, for example, constrained by legal, ethical, regulatory and privacy concerns preventing data sharing between institutions. Collaborative learning systems, such as federated learning, are designed to circumvent such restrictions and provide a privacy-preserving alternative by eschewing data sharing and relying instead on the distributed remote execution of algorithms. However, such systems are susceptible to malicious adversarial interference attempting to undermine their utility or divulge confidential information. Here we present an overview and analysis of current adversarial attacks and their mitigations in the context of collaborative machine learning. We discuss the applicability of attack vectors to specific learning contexts and attempt to formulate a generic foundation for adversarial influence and mitigation mechanisms. We moreover show that a number of context-specific learning conditions are exploited in similar fashion across all settings. Lastly, we provide a focused perspective on open challenges and promising areas of future research in the field."	"Ur5yj
Times Cited:0
Cited References Count:108"	""	"<Go to ISI>://WOS:000696824400004"	""	"Imperial Coll London, Dept Comp, London, England
Tech Univ Munich, Dept Diagnost & Intervent Radiol, Munich, Germany
OpenMined, Oxford, England
Tech Univ Munich, Inst Artificial Intelligence Med & Healthcare, Munich, Germany
ConsenSys Hlth, New York, NY USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"N. Bouacida; P. Mohapatra"	"2021"	"Vulnerabilities in Federated Learning"	""	"Ieee Access"	""	""	"9"	""	""	"63229-63249"	""	""	""	""	""	""	""	"Vulnerabilities in Federated Learning"	"Ieee Access"	"2169-3536"	"10.1109/Access.2021.3075203"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000645858600001"	""	""	"training
security
data models
computational modeling
servers
privacy
training data
attacks
defenses
federated learning
security threats
vulnerabilities
privacy
security
system
attacks"	"With more regulations tackling the protection of users' privacy-sensitive data in recent years, access to such data has become increasingly restricted. A new decentralized training paradigm, known as Federated Learning (FL), enables multiple clients located at different geographical locations to learn a machine learning model collaboratively without sharing their data. While FL has recently emerged as a promising solution to preserve users' privacy, this new paradigm's potential security implications may hinder its widespread adoption. The existing FL protocols exhibit new unique vulnerabilities that adversaries can exploit to compromise the trained model. FL is often preferred in learning environments where security and privacy are the key concerns. Therefore, it is crucial to raise awareness of the consequences resulting from the new threats to FL systems. To date, the security of traditional machine learning systems has been widely examined. However, many open challenges and complex questions are still surrounding FL security. In this paper, we bridge the gap in FL literature by providing a comprehensive survey of the unique security vulnerabilities exposed by the FL ecosystem. We highlight the vulnerabilities sources, key attacks on FL, defenses, as well as their unique challenges, and discuss promising future research directions towards more robust FL."	"Rv5fm
Times Cited:7
Cited References Count:134"	""	"<Go to ISI>://WOS:000645858600001"	""	"Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"X. Yuan; P. He; Q. Zhu; X. Li"	"2019"	"Adversarial Examples: Attacks and Defenses for Deep Learning"	""	"IEEE Trans Neural Netw Learn Syst"	""	""	"30"	""	"9"	"2805-2824"	""	""	""	"2019/01/15"	"Sep"	""	""	"Adversarial Examples: Attacks and Defenses for Deep Learning"	""	"2162-2388 (Electronic)
2162-237X (Linking)"	"10.1109/TNNLS.2018.2886017"	""	""	""	""	""	""	""	""	""	""	""	"30640631"	""	""	""	"With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed."	"Yuan, Xiaoyong
He, Pan
Zhu, Qile
Li, Xiaolin
eng
Research Support, U.S. Gov't, Non-P.H.S.
IEEE Trans Neural Netw Learn Syst. 2019 Sep;30(9):2805-2824. doi: 10.1109/TNNLS.2018.2886017. Epub 2019 Jan 14."	""	"https://www.ncbi.nlm.nih.gov/pubmed/30640631"	"internal-pdf://2144196829/Yuan-2019-Adversarial Examples_ Attacks and De.pdf"	""	""	""	""	""	""	""	""	""
"Journal Article"	"M. M. Zhang; L. M. Hu; C. Shi; X. Wang"	"2020"	"Adversarial Label-Flipping Attack and Defense for Graph Neural Networks"	""	"20th Ieee International Conference on Data Mining (Icdm 2020)"	""	""	""	""	""	"791-800"	""	""	""	""	""	""	""	"Adversarial Label-Flipping Attack and Defense for Graph Neural Networks"	"Ieee Data Mining"	"1550-4786"	"10.1109/Icdm50108.2020.00088"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000630177700078"	""	""	"adversarial label-flipping attacks
adversarial robustness
graph neural networks"	"With the great popularity of Graph Neural Networks (GNNs), the robustness of GNNs to adversarial attacks has received increasing attention. However, existing works neglect adversarial label-flipping attacks, where the attacker can manipulate an unnoticeable fraction of training labels. Exploring the robustness of GNNs to label-flipping attacks is highly critical, especially when labels are collected from external sources and false labels are easy to inject (e.g., recommendation systems). In this work, we introduce the first study of adversarial label-flipping attacks on GNNs. We propose an effective attack model LafAK based on approximated closed form of GNNs and continuous surrogate of non-differentiable objective, efficiently generating attacks via gradient-based optimizers. Furthermore, we show that one key reason for the vulnerability of GNNs to label-flipping attack is overfitting to flipped nodes. Based on this observation, we propose a defense framework which introduces a community-preserving self-supervised task as regularization to avoid overfitting. We demonstrate the effectiveness of our proposed attack model to GNNs on four real-world datasets. The effectiveness of our defense framework is also well validated by the substantial improvements of defense based GNN and its variants under label-flipping attacks."	"Br0ql
Times Cited:1
Cited References Count:41
IEEE International Conference on Data Mining"	""	"<Go to ISI>://WOS:000630177700078"	""	"Beijing Univ Posts & Telecommun, Dept Comp Sci, Beijing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Q. Liu; J. J. Guo; C. K. Wen; S. Jin"	"2020"	"Adversarial attack on DL-based massive MIMO CSI feedback"	""	"Journal of Communications and Networks"	""	""	"22"	""	"3"	"230-235"	""	""	""	""	"Jun"	""	""	"Adversarial attack on DL-based massive MIMO CSI feedback"	"J Commun Netw-S Kor"	"1229-2370"	"10.1109/Jcn.2020.000016"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000551471500007"	""	""	"adversarial attack
csi feedback
deep learning
wireless security"	"With the increasing application of deep learning (DL) algorithms in wireless communications, the physical layer faces new challenges caused by adversarial attack. Such attack has significantly affected the neural network in computer vision. We choose DL-based channel state information (CSI) to show the effect of adversarial attack on DL-based communication system. We present a practical method to craft white-box adversarial attack on DL-based CSI feedback process. Our simulation results show the destructive effect adversarial attack causes on DL-based CSI feedback by analyzing the performance of normalized mean square error. We also launch a jamming attack for comparison and find that the jamming attack could be prevented with certain precautions. As DL algorithm becomes the trend in developing wireless communication, this work raises concerns regarding the security in the use of DL-based algorithms."	"Sp. Iss. SI
Mo4at
Times Cited:6
Cited References Count:15"	""	"<Go to ISI>://WOS:000551471500007"	""	"Southeast Univ, Sch Cyber Sci & Engn, Nanjing 210096, Peoples R China
Southeast Univ, Natl Mobile Commun Res Lab, Nanjing 210096, Peoples R China
Natl Sun Yat Sen Univ, Inst Commun Engn, Kaohsiung 804, Taiwan"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. Zhou; A. R. Nelakurthi; J. R. He"	"2018"	"Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners"	""	"Kdd'18: Proceedings of the 24th Acm Sigkdd International Conference on Knowledge Discovery & Data Mining"	""	""	""	""	""	"2817-2826"	""	""	""	""	""	""	""	"Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000455346400289"	""	""	"crowd teaching
exponentially decayed memory
human learner"	"With the increasing demand for large amount of labeled data, crowdsourcing has been used in many large-scale data mining applications. However, most existing works in crowdsourcing mainly focus on label inference and incentive design. In this paper, we address a different problem of adaptive crowd teaching, which is a sub-area of machine teaching in the context of crowdsourcing. Compared with machines, human beings are extremely good at learning a specific target concept (e.g., classifying the images into given categories) and they can also easily transfer the learned concepts into similar learning tasks. Therefore, a more effective way of utilizing crowdsourcing is by supervising the crowd to label in the form of teaching. In order to perform the teaching and expertise estimation simultaneously, we propose an adaptive teaching framework named JEDI to construct the personalized optimal teaching set for the crowdsourcing workers. In JEDI teaching, the teacher assumes that each learner has an exponentially decayed memory. Furthermore, it ensures comprehensiveness in the learning process by carefully balancing teaching diversity and learner's accurate learning in terms of teaching usefulness. Finally, we validate the effectiveness and efficacy of JEDI teaching in comparison with the state-of-the-art techniques on multiple data sets with both synthetic learners and real crowdsourcing workers."	"Bl7qe
Times Cited:17
Cited References Count:29"	""	"<Go to ISI>://WOS:000455346400289"	""	"Arizona State Univ, Tempe, AZ 85287 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. Dunn; N. Moustafa; B. Turnbull"	"2020"	"Robustness Evaluations of Sustainable Machine Learning Models against Data Poisoning Attacks in the Internet of Things"	""	"Sustainability"	""	""	"12"	""	"16"	""	""	""	""	""	"Aug"	""	""	"Robustness Evaluations of Sustainable Machine Learning Models against Data Poisoning Attacks in the Internet of Things"	"Sustainability-Basel"	""	"ARTN 6434
10.3390/su12166434"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000579583300001"	""	""	"adversarial machine learning
sustainable machine learning
data poisoning
deep learning
internet of things
iot
security"	"With the increasing popularity of the Internet of Things (IoT) platforms, the cyber security of these platforms is a highly active area of research. One key technology underpinning smart IoT systems is machine learning, which classifies and predicts events from large-scale data in IoT networks. Machine learning is susceptible to cyber attacks, particularly data poisoning attacks that inject false data when training machine learning models. Data poisoning attacks degrade the performances of machine learning models. It is an ongoing research challenge to develop trustworthy machine learning models resilient and sustainable against data poisoning attacks in IoT networks. We studied the effects of data poisoning attacks on machine learning models, including the gradient boosting machine, random forest, naive Bayes, and feed-forward deep learning, to determine the levels to which the models should be trusted and said to be reliable in real-world IoT settings. In the training phase, a label modification function is developed to manipulate legitimate input classes. The function is employed at data poisoning rates of 5%, 10%, 20%, and 30% that allow the comparison of the poisoned models and display their performance degradations. The machine learning models have been evaluated using the ToN_IoT and UNSW NB-15 datasets, as they include a wide variety of recent legitimate and attack vectors. The experimental results revealed that the models' performances will be degraded, in terms of accuracy and detection rates, if the number of the trained normal observations is not significantly larger than the poisoned data. At the rate of data poisoning of 30% or greater on input data, machine learning performances are significantly degraded."	"Od0zn
Times Cited:8
Cited References Count:41"	""	"<Go to ISI>://WOS:000579583300001"	""	"Univ New South Wales, Sch Engn & Informat Technol, Canberra, ACT 2612, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"W. B. Jiang; H. W. Li; S. Liu; X. Z. Luo; R. X. Lu"	"2020"	"Poisoning and Evasion Attacks Against Deep Learning Algorithms in Autonomous Vehicles"	""	"Ieee Transactions on Vehicular Technology"	""	""	"69"	""	"4"	"4439-4449"	""	""	""	""	"Apr"	""	""	"Poisoning and Evasion Attacks Against Deep Learning Algorithms in Autonomous Vehicles"	"Ieee T Veh Technol"	"0018-9545"	"10.1109/Tvt.2020.2977378"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000530284400075"	""	""	"autonomous vehicles
traffic sign recognition
deep learning
evasion attack
poisoning attack
authentication scheme
enabling efficient
security
intelligent
challenges
network
search"	"With the ongoing development and improvement of deep learning technology, autonomous vehicles (AVs) have made tremendous progress in recent years. Despite its great potential, AV supported by deep learning technology still faces numerous security threats, which prevent AV from being putting into large-scale practice. Aiming at this challenging situation, in this paper, we would like to exploit two attacks against deep learning algorithms in traffic sign recognition system by leveraging particle swarm optimization. Specifically, we first exploit the PAPSO (poisoning attack with particle swarm optimization) which focuses on the training process of the deep learning algorithms in the traffic sign recognition system, i.e., the attacker injects crafted samples into the training dataset, causing a reduction in classification accuracy of the traffic sign recognition system. Then, we also explore the EAPSO (evasion attack with particle swarm optimization) which on the other hand focuses on the interference process of the deep learning algorithms, i.e., the attacker adds some hardly perceptible perturbations to the targeted test sample, leading to a misclassification on it. Extensive experiments are conducted to shed light on the effectiveness of our attacks, and some corresponding defense strategies are also presented."	"Lj6ph
Times Cited:15
Cited References Count:54"	""	"<Go to ISI>://WOS:000530284400075"	""	"Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China
Peng Cheng Lab, Shenzhen 518000, Peoples R China
Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China
Univ New Brunswick, Fac Comp Sci, Fredericton, NB E3B 5A3, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Z. Abaid; M. A. Kaafar; S. Jha"	"2017"	"Quantifying the Impact of Adversarial Evasion Attacks on Machine Learning Based Android Malware Classifiers"	""	"2017 Ieee 16th International Symposium on Network Computing and Applications (Nca)"	""	""	""	""	""	"375-384"	""	""	""	""	""	""	""	"Quantifying the Impact of Adversarial Evasion Attacks on Machine Learning Based Android Malware Classifiers"	""	""	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000426971900057"	""	""	""	"With the proliferation of Android-based devices, malicious apps have increasingly found their way to user devices. Many solutions for Android malware detection rely on machine learning; although effective, these are vulnerable to attacks from adversaries who wish to subvert these algorithms and allow malicious apps to evade detection. In this work, we present a statistical analysis of the impact of adversarial evasion attacks on various linear and non-linear classifiers, using a recently proposed Android malware classifier as a case study. We systematically explore the complete space of possible attacks varying in the adversary's knowledge about the classifier; our results show that it is possible to subvert linear classifiers (Support Vector Machines and Logistic Regression) by perturbing only a few features of malicious apps, with more knowledgeable adversaries degrading the classifier's detection rate from 100% to 0% and a completely blind adversary able to lower it to 12%. We show non-linear classifiers (Random Forest and Neural Network) to be more resilient to these attacks. We conclude our study with recommendations for designing classifiers to be more robust to the attacks presented in our work."	"Bj6wz
Times Cited:22
Cited References Count:22"	""	"<Go to ISI>://WOS:000426971900057"	""	"Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia
Macquarie Univ, Comp Dept, N Ryde, NSW, Australia
CSIRO, Data61, Canberra, ACT, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"C. L. Miao; Q. Li; H. P. Xiao; W. J. Jiang; M. D. Huai; L. Su"	"2018"	"Towards Data Poisoning Attacks in Crowd Sensing Systems"	""	"Proceedings of the 2018 the Nineteenth International Symposium on Mobile Ad Hoc Networking and Computing (Mobihoc '18)"	""	""	""	""	""	"111-120"	""	""	""	""	""	""	""	"Towards Data Poisoning Attacks in Crowd Sensing Systems"	""	""	"10.1145/3209582.3209594"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000471038600012"	""	""	"data poisoning
crowd sensing
truth discovery"	"With the proliferation of sensor-rich mobile devices, crowd sensing has emerged as a new paradigm of collecting information from the physical world. However, the sensory data provided by the participating workers are usually not reliable. In order to identify truthful values from the crowd sensing data, the topic of truth discovery, whose goal is to estimate each worker's reliability and infer the underlying truths through weighted data aggregation, is widely studied. Since truth discovery incorporates workers' reliability into the aggregation procedure, it shows robustness to the data poisoning attacks, which are usually conducted by the malicious workers who aim to degrade the effectiveness of the crowd sensing systems through providing malicious sensory data. However, truth discovery is not perfect in all cases. In this paper, we study how to effectively conduct two types of data poisoning attacks, i.e., the availability attack and the target attack, against a crowd sensing system empowered with the truth discovery mechanism. We develop an optimal attack framework in which the attacker can not only maximize his attack utility but also disguise the introduced malicious workers as normal ones such that they cannot be detected easily. The desirable performance of the proposed framework is verified through extensive experiments conducted on a real-world crowd sensing system."	"Bm9jp
Times Cited:28
Cited References Count:45"	""	"<Go to ISI>://WOS:000471038600012"	""	"SUNY Buffalo, Buffalo, NY 14260 USA
Univ Illinois, Champaign, IL USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"K. Ren; T. H. Zheng; Z. Qin; X. Liu"	"2020"	"Adversarial Attacks and Defenses in Deep Learning"	""	"Engineering"	""	""	"6"	""	"3"	"346-360"	""	""	""	""	"Mar"	""	""	"Adversarial Attacks and Defenses in Deep Learning"	"Engineering-Prc"	"2095-8099"	"10.1016/j.eng.2019.12.012"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000531607900018"	""	""	"machine learning
deep neural network
adversarial example
adversarial attack
adversarial defense
robustness"	"With the rapid developments of artificial intelligence (Al) and deep learning (DL) techniques, it is critical to ensure the security and robustness of the deployed algorithms. Recently, the security vulnerability of DL algorithms to adversarial samples has been widely recognized. The fabricated samples can lead to various misbehaviors of the DL models while being perceived as benign by humans. Successful implementations of adversarial attacks in real physical-world scenarios further demonstrate their practicality. Hence, adversarial attack and defense techniques have attracted increasing attention from both machine learning and security communities and have become a hot research topic in recent years. In this paper, we first introduce the theoretical foundations, algorithms, and applications of adversarial attack techniques. We then describe a few research efforts on the defense techniques, which cover the broad frontier in the field. Several open problems and challenges are subsequently discussed, which we hope will provoke further research efforts in this critical area. (C) 2020 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company."	"Ll5oq
Times Cited:94
Cited References Count:115"	""	"<Go to ISI>://WOS:000531607900018"	""	"Zhejiang Univ, Inst Cyberspace Res, Hangzhou 310027, Peoples R China
Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China
Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 2E8, Canada
McGill Univ, Sch Comp Sci, Montreal, PQ H3A 0E9, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"Y. C. Zhao; J. L. Chen; J. L. Zhang; Z. L. Yang; H. W. Tu; H. Han; K. Zhu; B. Chen"	"2021"	"User-Level Membership Inference for Federated Learning in Wireless Network Environment"	""	"Wireless Communications & Mobile Computing"	""	""	"2021"	""	""	""	""	""	""	""	"Oct 20"	""	""	"User-Level Membership Inference for Federated Learning in Wireless Network Environment"	"Wirel Commun Mob Com"	"1530-8669"	"Artn 5534270
10.1155/2021/5534270"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000715790100002"	""	""	""	"With the rise of privacy concerns in traditional centralized machine learning services, federated learning, which incorporates multiple participants to train a global model across their localized training data, has lately received significant attention in both industry and academia. Bringing federated learning into a wireless network scenario is a great move. The combination of them inspires tremendous power and spawns a number of promising applications. Recent researches reveal the inherent vulnerabilities of the various learning modes for the membership inference attacks that the adversary could infer whether a given data record belongs to the model's training set. Although the state-of-the-art techniques could successfully deduce the membership information from the centralized machine learning models, it is still challenging to infer the member data at a more confined level, the user level. It is exciting that the common wireless monitor technique in the wireless network environment just provides a good ground for fine-grained membership inference. In this paper, we novelly propose and define a concept of user-level inference attack in federated learning. Specifically, we first give a comprehensive analysis of active and targeted membership inference attacks in the context of federated learning. Then, by considering a more complicated scenario that the adversary can only passively observe the updating models from different iterations, we incorporate the generative adversarial networks into our method, which can enrich the training set for the final membership inference model. In the end, we comprehensively research and implement inferences launched by adversaries of different roles, which makes the attack scenario complete and realistic. The extensive experimental results demonstrate the effectiveness of our proposed attacking approach in the case of single label and multilabel.</p>"	"Wt3ti
Times Cited:0
Cited References Count:40"	""	"<Go to ISI>://WOS:000715790100002"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China
Yangzhou Univ, Sch Informat Engn, Yangzhou, Jiangsu, Peoples R China
La Trobe Univ, Comp Sci & Informat Technol, Bundoora, Vic, Australia"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. L. Chen; J. L. Zhang; Y. C. Zhao; H. Han; K. Zhu; B. Chen"	"2020"	"Beyond Model-Level Membership Privacy Leakage: an Adversarial Approach in Federated Learning"	""	"2020 29th International Conference on Computer Communications and Networks (Icccn 2020)"	""	""	""	""	""	"1-9"	""	""	""	""	""	""	""	"Beyond Model-Level Membership Privacy Leakage: an Adversarial Approach in Federated Learning"	"Ieee Ic Comp Com Net"	"1095-2055"	""	""	""	""	""	""	""	""	""	""	""	""	"WOS:000627816700125"	""	""	"federated learning
membership inference
generative adversarial networks
user-level"	"With the rise of privacy concerns in traditional centralized machine learning services, the federated learning, which incorporates multiple participants to train a global model across their localized training data, has lately received significant attention in both industry and academia. However, recent researches reveal the inherent vulnerabilities of the federated learning for the membership inference attacks that the adversary could infer whether a given data record belongs to the model's training set. Although the state-of-the-art techniques could successfully deduce the membership information from the centralized machine learning models, it is still challenging to infer the membership to a more confined level, user-level. In this paper, We propose a novel user-level inference attack mechanism in federated learning. Specifically, we first give a comprehensive analysis of active and targeted membership inference attacks in the context of the federated learning. Then, by considering a more complicated scenario that the adversary can only passively observe the updating models from different iterations, we incorporate the generative adversarial networks into our method, which can enrich the training set for the final membership inference model. The extensive experimental results demonstrate the effectiveness of our proposed attacking approach in the case of single-label and multi-label."	"Br0by
Times Cited:1
Cited References Count:29
IEEE International Conference on Computer Communications and Networks"	""	"<Go to ISI>://WOS:000627816700125"	""	"Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. L. Wen; B. Z. H. Zhao; M. H. Xue; A. Oprea; H. F. Qian"	"2021"	"With Great Dispersion Comes Greater Resilience: Efficient Poisoning Attacks and Defenses for Linear Regression Models"	""	"Ieee Transactions on Information Forensics and Security"	""	""	"16"	""	""	"3709-3723"	""	""	""	""	""	""	""	"With Great Dispersion Comes Greater Resilience: Efficient Poisoning Attacks and Defenses for Linear Regression Models"	"Ieee T Inf Foren Sec"	"1556-6013"	"10.1109/Tifs.2021.3087332"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000678332000005"	""	""	"training
linear regression
data models
predictive models
numerical models
machine learning
time complexity
data poisoning attacks and defenses
linear regression models
complexity"	"With the rise of third parties in the machine learning pipeline, the service provider in "Machine Learning as a Service" (MLaaS), or external data contributors in online learning, or the retraining of existing models, the need to ensure the security of the resulting machine learning models has become an increasingly important topic. The security community has demonstrated that without transparency of the data and the resulting model, there exist many potential security risks, with new risks constantly being discovered. In this paper, we focus on one of these security risks - poisoning attacks. Specifically, we analyze how attackers may interfere with the results of regression learning by poisoning the training datasets. To this end, we analyze and develop a new poisoning attack algorithm. Our attack, termed Nopt, in contrast with previous poisoning attack algorithms, can produce larger errors with the same proportion of poisoning data-points. Furthermore, we also significantly improve the state-of-the-art defense algorithm, termed TRIM, proposed by Jagielsk et al. (IEEE S&P 2018), by incorporating the concept of probability estimation of clean data-points into the algorithm. Our new defense algorithm, termed Proda, demonstrates an increased effectiveness in reducing errors arising from the poisoning dataset through optimizing ensemble models. We highlight that the time complexity of TRIM had not been estimated; however, we deduce from their work that TRIM can take exponential time complexity in the worst-case scenario, in excess of Proda's logarithmic time. The performance of both our proposed attack and defense algorithms is extensively evaluated on four real-world datasets of housing prices, loans, health care, and bike sharing services. We hope that our work will inspire future research to develop more robust learning algorithms immune to poisoning attacks."	"Tq5pl
Times Cited:1
Cited References Count:46"	""	"<Go to ISI>://WOS:000678332000005"	""	"East China Normal Univ, Sch Comp Sci Technol, Shanghai 200062, Peoples R China
East China Normal Univ, Software Engn Inst, Shanghai 200062, Peoples R China
Univ New South Wales, CSIRO Data61, Sydney, NSW 2052, Australia
Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia
Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA"	""	""	""	""	""	""	""	"English"
"Journal Article"	"L. Pajola; L. Pasa; M. Conti"	"2019"	"Threat is in the Air: Machine Learning for Wireless Network Applications"	""	"Proceedings of the 2019 Acm Workshop on Wireless Security and Machine Learning (Wiseml '19)"	""	""	""	""	""	"16-21"	""	""	""	""	""	""	""	"Threat is in the Air: Machine Learning for Wireless Network Applications"	""	""	"10.1145/3324921.3328783"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000480399800004"	""	""	"wireless network applications
machine learning
adversarial machine learning
security
security"	"With the spread of wireless application, huge amount of data is generated every day. Thanks to its elasticity, machine learning is becoming a fundamental brick in this field, and many of applications are developed with the use of it and the several techniques that it offers. However, machine learning suffers on different problems and people that use it often are not aware of the possible threats. Often, an adversary tries to exploit these vulnerabilities in order to obtain benefits; because of this, adversarial machine learning is becoming wide studied in the scientific community. In this paper, we show state-of-the-art adversarial techniques and possible countermeasures, with the aim of warning people regarding sensible argument related to the machine learning."	"Bn3no
Times Cited:1
Cited References Count:58"	""	"<Go to ISI>://WOS:000480399800004"	""	"Univ Padua, Padua, Italy
Ist Italiano Tecnol, Ferrara, Italy"	""	""	""	""	""	""	""	"English"
"Journal Article"	"J. Z. Dai; C. S. Chen; Y. F. Li"	"2019"	"A Backdoor Attack Against LSTM-Based Text Classification Systems"	""	"Ieee Access"	""	""	"7"	""	""	"138872-138878"	""	""	""	""	""	""	""	"A Backdoor Attack Against LSTM-Based Text Classification Systems"	"Ieee Access"	"2169-3536"	"10.1109/Access.2019.2941376"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000498721000001"	""	""	"backdoor attacks
lstm
poisoning data
text classification"	"With the widespread use of deep learning system in many applications, the adversary has strong incentive to explore vulnerabilities of deep neural networks and manipulate them. Backdoor attacks against deep neural networks have been reported to be a new type of threat. In this attack, the adversary will inject backdoors into the model and then cause the misbehavior of the model through inputs including backdoor triggers. Existed research mainly focuses on backdoor attacks in image classification based on CNN, little attention has been paid to the backdoor attacks in RNN. In this paper, we implement a backdoor attack against LSTM-based text classification by data poisoning. After the backdoor is injected, the model will misclassify any text samples that contains a specific trigger sentence into the target category determined by the adversary. The backdoor attack is stealthy and the backdoor injected in the model has little impact on the performance of the model. We consider the backdoor attack in black-box setting, where the adversary has no knowledge of model structures or training algorithms except for a small amount of training data. We verify the attack through sentiment analysis experiment on the dataset of IMDB movie reviews. The experimental results indicate that our attack can achieve around 96% success rate with 1% poisoning rate."	"Jq1nx
Times Cited:20
Cited References Count:26"	""	"<Go to ISI>://WOS:000498721000001"	""	"Shanghai Univ, Sch Comp Engn & Technol, Shanghai 200444, Peoples R China"	""	""	""	""	""	""	""	"English"
"Journal Article"	"W. S. Fan; H. W. Li; W. B. Jiang; G. W. Xu; R. X. Lu"	"2020"	"A Practical Black-Box Attack Against Autonomous Speech Recognition Model"	""	"2020 Ieee Global Communications Conference (Globecom)"	""	""	""	""	""	"1-6"	""	""	""	""	""	""	""	"A Practical Black-Box Attack Against Autonomous Speech Recognition Model"	"Ieee Glob Comm Conf"	"2334-0983"	"10.1109/Globecom42002.2020.9348184"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000668970505012"	""	""	"machine learning
automatic speech recognition
differential evolution
black-box attack"	"With the wild applications of machine learning (ML) technology, automatic speech recognition (ASR) has made great progress in recent years. Despite its great potential, there are various evasion attacks of ML-based ASR, which could affect the security of applications built upon ASR. Up to now, most studies focus on white-box attacks in ASR, and there is almost no attention paid to black-box attacks where attackers can only query the target model to get output labels rather than probability vectors in audio domain. In this paper, we propose an evasion attack against ASR in the above-mentioned situation, which is more feasible in realistic scenarios. Specifically, we first train a substitute model by using data augmentation, which ensures that we have enough samples to train with a small number of times to query the target model. Then, based on the substitute model, we apply Differential Evolution (DE) algorithm to craft adversarial examples and implement black-box attack against ASR models from the Speech Commands dataset. Extensive experiments are conducted, and the results illustrate that our approach achieves untargeted attacks with over 70% success rate while still maintaining the authenticity of the original data well."	"Br7qi
Times Cited:0
Cited References Count:20
IEEE Global Communications Conference"	""	"<Go to ISI>://WOS:000668970505012"	""	"Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China
Cyberspace Secur Res Ctr, Peng Cheng Lab, Shenzhen 518000, Peoples R China
Univ New Brunswick, Fac Comp Sci, Fredericton, NB, Canada"	""	""	""	""	""	""	""	"English"
"Journal Article"	"D. J. Miller; Z. Xiang; G. Kesidis"	"2020"	"Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks"	""	"Proceedings of the Ieee"	""	""	"108"	""	"3"	"402-433"	""	""	""	""	"Mar"	""	""	"Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks"	"P Ieee"	"0018-9219"	"10.1109/Jproc.2020.2970615"	""	""	""	""	""	""	""	""	""	""	""	"WOS:000526550300002"	""	""	"training data
neural networks
reverse engineering
machine learning
robustness
training data
feature extraction
social networking (online)
informatics
adversarial machine learning
anomaly detection (ad)
backdoor
black box
data poisoning (dp)
deep neural networks (dnns)
membership inference attack
reverse engineering (re)
robust classification
targeted attacks
test-time-evasion (tte)
transferability
white box"	"With wide deployment of machine learning (ML)-based systems for a variety of applications including medical, military, automotive, genomic, multimedia, and social networking, there is great potential for damage from adversarial learning (AL) attacks. In this article, we provide a contemporary survey of AL, focused particularly on defenses against attacks on deep neural network classifiers. After introducing relevant terminology and the goals and range of possible knowledge of both attackers and defenders, we survey recent work on test-time evasion (TTE), data poisoning (DP), backdoor DP, and reverse engineering (RE) attacks and particularly defenses against the same. In so doing, we distinguish robust classification from anomaly detection (AD), unsupervised from supervised, and statistical hypothesis-based defenses from ones that do not have an explicit null (no attack) hypothesis. We also consider several scenarios for detecting backdoors. We provide a technical assessment for reviewed works, including identifying any issues/limitations, required hyperparameters, needed computational complexity, as well as the performance measures evaluated and the obtained quality. We then delve deeper, providing novel insights that challenge conventional AL wisdom and that target unresolved issues, including: robust classification versus AD as a defense strategy; the belief that attack success increases with attack strength, which ignores susceptibility to AD; small perturbations for TTE attacks: a fallacy or a requirement; validity of the universal assumption that a TTE attacker knows the ground-truth class for the example to be attacked; black, gray, or white-box attacks as the standard for defense evaluation; and susceptibility of query-based RE to an AD defense. We also discuss attacks on the privacy of training data. We then present benchmark comparisons of several defenses against TTE, RE, and backdoor DP attacks on images. The article concludes with a discussion of continuing research directions, including the supreme challenge of detecting attacks whose goal is not to alter classification decisions, but rather simply to embed, without detection, "fake news" or other false content."	"Le2jv
Times Cited:39
Cited References Count:100"	""	"<Go to ISI>://WOS:000526550300002"	""	"Penn State Univ, Sch Elect Engn & Comp Sci, University Pk, PA 16802 USA"	""	""	""	""	""	""	""	"English"
